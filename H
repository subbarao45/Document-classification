import asyncio
import uuid
import os
from openai import AsyncAzureOpenAI  # Use Azure-specific OpenAI SDK
import cohere
from httpx import Timeout

def make_id():
    return str(uuid.uuid4())

class EngineWrapper:
    def __init__(
        self,
        engine,  # Azure uses "engine" instead of "model"
        api_key=None,
        base_url=None,
        mode="api",  # Can be 'api', 'cohere', or 'aphrodite'
        quantization="gptq",
    ):
        """
        Initialize the Engine Wrapper for Azure OpenAI.
        """
        self.mode = mode
        self.engine = engine  # Deployment name in Azure
        self.api_key = api_key or os.getenv("AZURE_OPENAI_API_KEY", "your-api-key")
        self.base_url = base_url or os.getenv("AZURE_OPENAI_BASE_URL", "https://ssgpt-predev.openai.azure.com/")
        self.api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2023-03-15-preview")

        if mode == "cohere":
            self.client = cohere.AsyncClient(api_key=self.api_key)
        elif mode == "api":
            self.client = AsyncAzureOpenAI(
                timeout=Timeout(timeout=5000.0, connect=10.0),
                api_key=self.api_key,
                azure_endpoint=self.base_url,
                api_version=self.api_version
            )

    async def submit_completion(self, prompt, sampling_params):
        """
        Submit request and wait for the full completion response.
        """
        sampling_params.setdefault("temperature", 1)
        sampling_params.setdefault("top_p", 1)
        sampling_params.setdefault("max_tokens", 3000)
        sampling_params.setdefault("stop", [])
        sampling_params.setdefault("n_predict", sampling_params["max_tokens"])
        use_min_p = "min_p" in sampling_params

        if self.mode == "api":
            timed_out = False
            completion = ""

            try:
                if use_min_p:
                    stream = await self.client.completions.create(
                        engine=self.engine,  # Azure uses "engine" instead of "model"
                        prompt=prompt,
                        temperature=sampling_params["temperature"],
                        top_p=sampling_params["top_p"],
                        stop=sampling_params["stop"],
                        max_tokens=sampling_params["max_tokens"],
                        extra_body={"min_p": sampling_params["min_p"]},
                        stream=True,
                        timeout=360,
                    )
                else:
                    stream = await self.client.completions.create(
                        engine=self.engine,
                        prompt=prompt,
                        temperature=sampling_params["temperature"],
                        top_p=sampling_params["top_p"],
                        stop=sampling_params["stop"],
                        max_tokens=sampling_params["max_tokens"],
                        stream=True,
                        timeout=360,
                    )

                async for chunk in stream:
                    try:
                        if chunk.choices and chunk.choices[0].delta.content:
                            completion += chunk.choices[0].delta.content
                    except:
                        timed_out = True

            except Exception as e:
                print("Request Timed Out:", e)
                timed_out = True

            return prompt + completion, timed_out

        elif self.mode == "cohere":
            raise Exception("Cohere is not compatible with completion mode!")

    async def submit_chat(self, messages, sampling_params):
        """
        Submit chat request and stream back the response.
        """
        sampling_params.setdefault("temperature", 1)
        sampling_params.setdefault("top_p", 1)
        sampling_params.setdefault("max_tokens", 3000)
        sampling_params.setdefault("stop", [])
        use_min_p = "min_p" in sampling_params

        if self.mode == "api":
            completion = ""
            timed_out = False

            try:
                if use_min_p:
                    stream = await self.client.chat.completions.create(
                        engine=self.engine,
                        messages=messages,
                        temperature=sampling_params["temperature"],
                        top_p=sampling_params["top_p"],
                        stop=sampling_params["stop"],
                        max_tokens=sampling_params["max_tokens"],
                        extra_body={"min_p": sampling_params["min_p"]},
                        stream=True,
                    )
                else:
                    stream = await self.client.chat.completions.create(
                        engine=self.engine,
                        messages=messages,
                        temperature=sampling_params["temperature"],
                        top_p=sampling_params["top_p"],
                        stop=sampling_params["stop"],
                        max_tokens=sampling_params["max_tokens"],
                        stream=True,
                    )

                async for chunk in stream:
                    try:
                        if chunk.choices and chunk.choices[0].delta.content:
                            completion += chunk.choices[0].delta.content
                    except Exception as e:
                        print("Error during generation:", e)
                        timed_out = True

            except Exception as e:
                print("Azure OpenAI Request Timed Out:", e)
                timed_out = True

            return completion, timed_out

        elif self.mode == "cohere":
            raise Exception("Cohere is not compatible with chat mode!")
