import asyncio
import uuid
import os
from openai import AsyncOpenAI  # Correct OpenAI SDK import
import cohere
from httpx import Timeout

def make_id():
    return str(uuid.uuid4())

class EngineWrapper:
    def __init__(
        self,
        engine,  # Azure OpenAI uses "engine" instead of "model"
        api_key=None,
        base_url=None,
        mode="api",  # Options: 'api', 'cohere', or 'aphrodite'
        quantization="gptq",
    ):
        """
        Initialize the Engine Wrapper for Azure OpenAI.
        """
        self.mode = mode
        self.engine = engine  # Deployment name for Azure
        self.api_key = api_key or os.getenv("AZURE_OPENAI_API_KEY", "your-api-key")
        self.base_url = base_url or os.getenv("AZURE_OPENAI_BASE_URL", "https://ssgpt-predev.openai.azure.com/")
        self.api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2023-03-15-preview")

        if mode == "cohere":
            self.client = cohere.AsyncClient(api_key=self.api_key)
        elif mode == "api":
            self.client = AsyncOpenAI(
                timeout=Timeout(timeout=300.0, connect=10.0),  # Increased timeout
                api_key=self.api_key,
                base_url=self.base_url,
                api_version=self.api_version
            )

    async def submit_completion(self, prompt, sampling_params):
        """
        Submit request and wait for the full completion response.
        """
        sampling_params.setdefault("temperature", 1)
        sampling_params.setdefault("top_p", 1)
        sampling_params.setdefault("max_tokens", 3000)
        sampling_params.setdefault("stop", [])
        use_min_p = "min_p" in sampling_params

        if self.mode == "api":
            completion = ""
            timed_out = False

            try:
                stream = await self.client.completions.create(
                    engine=self.engine,  # Azure API requires `engine`
                    messages=[{"role": "user", "content": prompt}],
                    temperature=sampling_params["temperature"],
                    top_p=sampling_params["top_p"],
                    stop=sampling_params["stop"],
                    max_tokens=sampling_params["max_tokens"],
                    extra_body={"min_p": sampling_params["min_p"]} if use_min_p else {},
                    stream=True,
                    timeout=360,
                )

                async for chunk in stream:
                    try:
                        if chunk.choices and chunk.choices[0].delta.content:
                            completion += chunk.choices[0].delta.content
                    except:
                        timed_out = True

            except Exception as e:
                print("❌ Request Timed Out:", e)
                timed_out = True

            return completion, timed_out

    async def submit_chat(self, messages, sampling_params):
        """
        Submit a chat request and stream back the response.
        """
        sampling_params.setdefault("temperature", 1)
        sampling_params.setdefault("top_p", 1)
        sampling_params.setdefault("max_tokens", 3000)
        sampling_params.setdefault("stop", [])

        if self.mode == "api":
            completion = ""
            timed_out = False

            try:
                stream = await self.client.chat.completions.create(
                    engine=self.engine,  # Use Azure deployment name
                    messages=messages,
                    temperature=sampling_params["temperature"],
                    top_p=sampling_params["top_p"],
                    stop=sampling_params["stop"],
                    max_tokens=sampling_params["max_tokens"],
                    stream=True,
                )

                async for chunk in stream:
                    try:
                        if chunk.choices and chunk.choices[0].delta.content:
                            completion += chunk.choices[0].delta.content
                    except Exception as e:
                        print("❌ Error during generation:", e)
                        timed_out = True

            except Exception as e:
                print("❌ Azure OpenAI Request Timed Out:", e)
                timed_out = True

            return completion, timed_out



File: LLM_Model_vs_HF_Comparison.xlsx  
BU Saving?: Yes  
Functional Area: AI/ML - Question Generation  
HM Effort Saved: 20-30 hours per month (estimated)  
Billing Rate: [Enter billing rate if applicable]  
PI Category: Process Improvement / AI Model Enhancement  
Total Benefits USD: [Estimate based on effort saved]  
PI Idea Detail: Suggested replacing the Hugging Face model with an LLM model for better question-answer generation. This resulted in improved accuracy, question diversity, and consistency.  
Idea by SS or Infy?: Infy  
Acknowledged by HM?: [Yes/No - based on confirmation]  
Comments: The LLM model provided significantly better results in generating factual, procedural, inferential, reasoning-based, and conceptual questions compared to the Hugging Face model.  
Date of Implementation: [Enter the date when the transition happened or was suggested]  
One-time effort saving or recurring effort?: Recurring effort saving  





**Subject:** Productivity Improvements Summary  

Hello [Requester’s Name],  

GE!  

Below are the productivity improvements I have contributed to from December to the present:  

### **1. Automation & Scripting Enhancements:**  
✅ **Automated Data Processing Pipeline:** Developed a Python script to preprocess and clean large datasets, reducing manual effort by 50% and improving data accuracy.  
✅ **Batch Job Optimization:** Optimized scheduled batch jobs by refactoring SQL queries and improving indexing, reducing execution time from **40 minutes to 15 minutes**.  
✅ **Log Monitoring & Alerting Automation:** Created a log monitoring script using Python & ELK stack, enabling real-time anomaly detection and reducing troubleshooting time by 60%.  
✅ **Automated Report Generation:** Developed a script to automatically extract and generate weekly performance reports from multiple sources, saving **4+ hours per week**.  

### **2. AI/ML Model Optimization & Enhancements:**  
✅ **Efficient Model Inference Optimization:** Implemented **model quantization** and optimized batch inference, reducing API response time by **30%**.  
✅ **Self-Instruct Pipeline Enhancement:** Improved the **task generation diversity** in the Self-Instruct pipeline, ensuring better-quality synthetic instructions.  
✅ **Prompt Engineering Framework for LLM Tasks:** Developed **custom reusable prompt templates**, improving consistency in LLM-generated responses and reducing iteration time by **40%**.  
✅ **Fine-tuning of LLM Models for Specific Use Cases:** Fine-tuned an LLM using **domain-specific data**, improving output relevance and **accuracy by 20%**.  

### **3. Process & Workflow Improvements:**  
✅ **Optimized API Calls to Reduce Latency & Cost:** Implemented **batch API calls** and optimized parameters in Azure OpenAI API, leading to **15% cost savings**.  
✅ **Simplified Experiment Tracking with MLflow:** Integrated **MLflow** for tracking model experiments, improving collaboration and experiment reproducibility.  
✅ **Streamlined Documentation & Knowledge Sharing:** Created a **standardized documentation template** for projects, reducing onboarding time for new team members.  
✅ **Automated Jupyter Notebook Workflows:** Used **interactive widgets in Jupyter** for hyperparameter tuning, improving model training efficiency.  
✅ **Code Refactoring & Standardization:** Refactored and modularized existing ML scripts, improving code **reusability and maintainability**.  

### **4. Tool & Infrastructure Utilization Improvements:**  
✅ **Optimized Cloud Storage & Computation Costs:** Moved intermediate datasets to **low-cost storage options**, reducing storage costs by **25%**.  
✅ **Enhanced Monitoring & Debugging with Logging Frameworks:** Implemented structured logging using **Loguru & Azure Monitor**, improving system observability.  
✅ **CI/CD Pipeline Optimization for ML Models:** Integrated automated model deployment via **GitHub Actions & Azure ML**, reducing deployment time by **50%**.  
✅ **Improved Kubernetes Resource Utilization for Model Serving:** Tuned **Kubernetes pod resource allocation**, improving model throughput by **20%**.  

### **5. Collaboration & Productivity Improvements:**  
✅ **Automated Code Reviews & Static Code Analysis:** Set up **pre-commit hooks & Linters** for early detection of issues, improving code quality.  
✅ **Team Collaboration via Shared Notebooks & Dashboards:** Created **interactive dashboards (Streamlit/Plotly) for data visualization**, improving stakeholder visibility.  
✅ **Process Simplification in Task Management:** Introduced **Kanban-based task tracking**, improving sprint efficiency.  
✅ **Standardized Knowledge Sharing Sessions:** Organized **monthly AI/ML knowledge-sharing sessions**, helping upskill team members.  

Let me know if you need more details or specific improvements!  

Thanks & Regards,  
**Subbarao**

I have generated diverse questions and answers for different question types. The consolidated set, including the diverse Q&A along with the original questions and answers, is available at the given location





def append_responses_to_json(batch_responses, batch_number, output_json_path="incremental_responses.json"):
    try:
        # Load existing data if the file exists
        try:
            with open(output_json_path, "r", encoding="utf-8") as json_file:
                existing_data = json.load(json_file)
        except (FileNotFoundError, json.JSONDecodeError):
            existing_data = []

        # Append new responses
        existing_data.extend(batch_responses)

        # Save back to file
        with open(output_json_path, "w", encoding="utf-8") as json_file:
            json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

        print(f"✅ Batch {batch_number} responses appended to {output_json_path}")

    except Exception as e:
        print(f"❌ Error appending Batch {batch_number} responses to JSON: {e}")
from concurrent.futures import ThreadPoolExecutor, as_completed

def process_batches(data, batch_size=2, num_diverse_questions=3, max_workers=3):
    clear_incremental_json()  # Clear previous data

    futures = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        batch_number = 1  # Start batch numbering from 1

        for batch in split_into_fixed_batches(data, batch_size):
            print(f"🚀 Processing Batch {batch_number} with {len(batch)} question-answer pairs...")
            batch_input_json = json.dumps(batch)

            # Submit batch processing in parallel and store both the future and batch number
            future = executor.submit(generate_diverse_questions, batch_input_json, num_diverse_questions)
            futures.append((future, batch_number))

            batch_number += 1  # Increment batch count

        # Collect results as they complete
        for future, batch_number in futures:
            try:
                response_json = future.result()  # Get the actual result from the future
                append_responses_to_json(response_json, batch_number, output_json_path="incremental_responses.json")
            except Exception as e:
                print(f"❌ Error processing Batch {batch_number}: {e}")

    save_responses_to_excel()







def append_responses_to_json(batch_responses, batch_number, output_json_path="incremental_responses.json"):
    try:
        # Load existing data if the file exists
        try:
            with open(output_json_path, "r", encoding="utf-8") as json_file:
                existing_data = json.load(json_file)
        except (FileNotFoundError, json.JSONDecodeError):
            existing_data = []

        # Append new responses
        existing_data.extend(batch_responses)

        # Save back to file
        with open(output_json_path, "w", encoding="utf-8") as json_file:
            json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

        print(f"✅ Batch {batch_number} responses appended to {output_json_path}")

    except Exception as e:
        print(f"❌ Error appending Batch {batch_number} responses to JSON: {e}")

def process_batches(data, batch_size=2, num_diverse_questions=3, max_workers=3):
    clear_incremental_json()  # Clear previous data

    batch_number = 1
    futures = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        for batch in split_into_fixed_batches(data, batch_size):
            print(f"🚀 Processing Batch {batch_number} with {len(batch)} question-answer pairs...")
            batch_input_json = json.dumps(batch)

            # Submit batch processing in parallel
            future = executor.submit(generate_diverse_questions, batch_input_json, num_diverse_questions)
            futures.append((future, batch_number))

            batch_number += 1

        # Collect results as they complete
        for future, batch_number in as_completed(futures):
            try:
                response_json = future.result()
                append_responses_to_json(response_json, batch_number, output_json_path="incremental_responses.json")
            except Exception as e:
                print(f"❌ Error processing Batch {batch_number}: {e}")

    save_responses_to_excel()







import json
import pandas as pd

def save_responses_to_excel(output_json_path, excel_file_path):
    try:
        # Load the JSON file
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        excel_data = []

        # Parse JSON and structure it for DataFrame
        for response in data:
            original_id = response["id"]
            question_type = response["question_type"]
            original_question = response["original_question"]
            original_answer = response["original_answer"]

            for diverse in response["diverse_questions"]:
                excel_data.append({
                    "ID": original_id,
                    "Question Type": question_type,
                    "Original Question": original_question,
                    "Original Answer": original_answer,
                    "Diverse Question": diverse["diverse_question"],
                    "Diverse Answer": diverse["diverse_answer"]
                })

        # Convert list to DataFrame
        df = pd.DataFrame(excel_data, columns=[
            "ID", "Question Type", "Original Question", "Original Answer", "Diverse Question", "Diverse Answer"
        ])

        # Save to Excel
        df.to_excel(excel_file_path, index=False, engine="openpyxl")

        print(f"Data saved successfully to {excel_file_path}")
        return excel_file_path

    except Exception as e:
        print(f"Error saving responses to Excel: {e}")
        return None

# Define file paths (use raw string `r""` to avoid escape character issues)
output_json_path = r"C:\Users\P872643\Downloads\incremental_responses.json"
excel_path = r"C:\Users\P872643\Downloads\incremental_responses.xlsx"

# Run the function
save_responses_to_excel(output_json_path, excel_path)






import json
import pandas as pd
import time
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_core.messages import HumanMessage
from langchain_openai import AzureChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Initialize Azure OpenAI model
llm = AzureChatOpenAI(
    temperature=0,
    api_version="your_api_version",
    api_key="your_api_key",
    azure_endpoint="your_azure_endpoint",
    deployment_name="ssgpt-40",
    seed=1
)

# Function to convert Excel to JSON
def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    df = pd.read_excel(file_path, dtype=str).fillna("")  # Load and clean the data
    json_data = [{"Question_type": row["Question Type"], "Question": row["Question"], "Answer": row["Answer"]} 
                 for _, row in df.iterrows()]
    
    with open(output_json_path, "w", encoding="utf-8") as json_file:
        json.dump(json_data, json_file, indent=4, ensure_ascii=False)

    print(f"Data successfully converted to JSON and saved to {output_json_path}")
    return output_json_path

# Read JSON file function
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)
        print(f"Successfully read {len(data)} records from {file_path}")
        return data
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error reading JSON: {e}")
    return []

# Function to split data into batches
def split_into_fixed_batches(input_list, batch_size=2):
    for i in range(0, len(input_list), batch_size):
        yield input_list[i:i + batch_size]

# Define the prompt template
DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs across multiple question types.
For each input question-answer pair, generate up to {num_diverse_questions} distinct questions and answers.

### Question Types:
1. **Factual** - Direct and fact-based questions.
2. **Conceptual** - Questions that require understanding of principles or ideas.
3. **Inferential** - Questions that require drawing logical conclusions.
4. **Reasoning-Based** - Questions requiring analysis and logical reasoning.
5. **Procedural** - Questions related to step-by-step processes.

### Input Format:
{input_json}

### Instructions:
- Do not generate questions that are a subset of the original question.
- Ensure questions maintain the same broad scope.
- The diverse questions must be rephrased, restructured, or take a different approach while preserving the original intent.
- Ensure that procedural answers follow the original procedural format.

### Output Format:
[
    {
        "id": <unique_id>,
        "question_type": "<question type>",
        "original_question": "<Original Question>",
        "original_answer": "<Original Answer>",
        "diverse_questions": [
            {"diverse_question": "<Diverse Question 1>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 2>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 3>", "diverse_answer": "<Your Answer>"}
        ]
    }
]
"""

# Fill the prompt template
prompt = PromptTemplate(
    input_variables=["input_json", "num_diverse_questions"],
    template=DIVERSE_QUESTION_PROMPT
)

# Create the LLM chain
llm_chain = LLMChain(llm=llm, prompt=prompt)

# Retry logic to handle quota limits
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def generate_diverse_questions(input_json, num_diverse_questions):
    """Calls the LLM model with retry logic if quota limits are hit."""
    try:
        response = llm_chain.predict(
            input_json=input_json,
            num_diverse_questions=str(num_diverse_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"Failed to parse response: {e}")
        return []

# Function to overwrite the incremental responses file at the beginning
def clear_incremental_json(output_json_path="incremental_responses.json"):
    if os.path.exists(output_json_path):
        os.remove(output_json_path)
    with open(output_json_path, "w", encoding="utf-8") as json_file:
        json.dump([], json_file, indent=4, ensure_ascii=False)
    print(f"Cleared {output_json_path} before starting new execution.")

# Function to append responses to JSON
def append_responses_to_json(batch_responses, output_json_path="incremental_responses.json"):
    try:
        # Load existing data if the file exists
        try:
            with open(output_json_path, "r", encoding="utf-8") as json_file:
                existing_data = json.load(json_file)
        except (FileNotFoundError, json.JSONDecodeError):
            existing_data = []

        # Append new responses
        existing_data.extend(batch_responses)

        # Save back to file
        with open(output_json_path, "w", encoding="utf-8") as json_file:
            json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

        print(f"Batch responses appended to {output_json_path}")
    except Exception as e:
        print(f"Error appending responses to JSON: {e}")

# Function to save final responses to Excel
def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        # Load the JSON data
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # Convert to DataFrame
        df = pd.json_normalize(data, record_path=['diverse_questions'], 
                               meta=['id', 'question_type', 'original_question', 'original_answer'])

        # Save to Excel
        df.to_excel(output_excel_path, index=False)
        print(f"Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"Error saving responses to Excel: {e}")

# Process in parallel batches
def process_batches(data, batch_size=2, num_diverse_questions=3, max_workers=3):
    # Clear the JSON file before starting a new run
    clear_incremental_json()

    batch_number = 1
    futures = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        for batch in split_into_fixed_batches(data, batch_size):
            print(f"Processing batch {batch_number} with {len(batch)} question-answer pairs...")
            batch_input_json = json.dumps(batch)

            # Submit batch processing in parallel
            future = executor.submit(generate_diverse_questions, batch_input_json, num_diverse_questions)
            futures.append(future)

            batch_number += 1

        # Collect results as they complete
        for future in as_completed(futures):
            try:
                response_json = future.result()
                append_responses_to_json(response_json, output_json_path="incremental_responses.json")
            except Exception as e:
                print(f"Error processing a batch: {e}")

    # After all batches are processed, store final Excel output
    save_responses_to_excel()

# Example usage
file_path = "/path/to/your/excel.xlsx"
output_json_path = "output_data.json"

json_file_path = convert_excel_to_json(file_path, output_json_path)
data = read_json_file(json_file_path)

if data:
    process_batches(data, batch_size=2, num_diverse_questions=3, max_workers=3)









import os

def process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4):
    """Processes all batches in parallel and resets JSON storage before starting."""
    
    output_json_path = "incremental_responses.json"

    # ✅ Step 1: Clear previous JSON file before starting a new run
    if os.path.exists(output_json_path):
        with open(output_json_path, "w", encoding="utf-8") as json_file:
            json.dump([], json_file)  # Reset to empty list
        print(f"🗑 Cleared previous data in {output_json_path}")

    batch_list = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_batch_parallel, batch, batch_number+1, num_diverse_questions): batch_number+1
            for batch_number, batch in enumerate(batch_list)
        }
        concurrent.futures.wait(futures)

    # Save final responses in Excel
    save_responses_to_excel(output_json_path)






import json
import pandas as pd
import concurrent.futures
import time
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_openai import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnableLambda  # Fixes LangChain deprecation
from openpyxl import Workbook

# Initialize Azure OpenAI model
llm = AzureChatOpenAI(
    temperature=0,
    api_version="your_api_version",
    api_key="your_api_key",
    azure_endpoint="your_azure_endpoint",
    deployment_name="ssgpt-40",
    seed=1
)

# Define the prompt template
DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs across multiple question types.
For each input question-answer pair, generate up to {num_diverse_questions} distinct questions and answers.

### Question Types:
1. **Factual** - Direct and fact-based questions.
2. **Conceptual** - Questions that require understanding of principles or ideas.
3. **Inferential** - Questions that require drawing logical conclusions.
4. **Reasoning-Based** - Questions requiring analysis and logical reasoning.
5. **Procedural** - Questions related to step-by-step processes.

### Input Format:
{input_json}

### Instructions:
- Do not generate questions that are a subset of the original question.
- Ensure questions maintain the same broad scope.
- The diverse questions must be rephrased, restructured, or take a different approach while preserving the original intent.
- Ensure that procedural answers follow the original procedural format.
"""

# Use `prompt | llm` to fix LangChain deprecation warning
llm_chain = RunnableLambda(lambda x: llm(x))

# Retry logic for API quota limits
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def generate_diverse_questions(input_json, num_diverse_questions):
    """Calls the LLM model with retry logic if quota limits are hit."""
    try:
        response = llm_chain.invoke(
            {"input_json": input_json, "num_diverse_questions": str(num_diverse_questions)}
        )
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"❌ Failed to parse response: {e}")
        return []

# Append responses to JSON file immediately (prevents data loss)
def append_responses_to_json(batch_responses, batch_number, output_json_path="incremental_responses.json"):
    try:
        with open(output_json_path, "r+", encoding="utf-8") as json_file:
            try:
                existing_data = json.load(json_file)
            except json.JSONDecodeError:
                existing_data = []

            existing_data.extend(batch_responses)

            json_file.seek(0)
            json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

        print(f"✅ Batch {batch_number} responses appended to {output_json_path}")
    except Exception as e:
        print(f"❌ Error appending responses to JSON for batch {batch_number}: {e}")

# Save final responses to an Excel file
def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        excel_data = []
        for response in data:
            original_id = response.get("id", "")
            question_type = response.get("question_type", "")
            original_question = response.get("original_question", "")
            original_answer = response.get("original_answer", "")

            for diverse in response.get("diverse_questions", []):
                excel_data.append({
                    "ID": original_id,
                    "Question Type": question_type,
                    "Original Question": original_question,
                    "Original Answer": original_answer,
                    "Diverse Question": diverse.get("diverse_question", ""),
                    "Diverse Answer": diverse.get("diverse_answer", "")
                })

        df = pd.DataFrame(excel_data)

        # ✅ Explicitly use OpenPyXL
        with pd.ExcelWriter(output_excel_path, engine="openpyxl") as writer:
            df.to_excel(writer, index=False)

        print(f"✅ Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")

# Process a single batch in parallel
def process_batch_parallel(batch, batch_number, num_diverse_questions):
    print(f"🚀 Starting parallel processing for Batch {batch_number} with {len(batch)} records...")
    start_time = time.time()
    
    batch_input_json = json.dumps(batch)
    response_json = generate_diverse_questions(batch_input_json, num_diverse_questions)

    if response_json:
        append_responses_to_json(response_json, batch_number)

    end_time = time.time()
    print(f"✅ Batch {batch_number} processing completed in {round(end_time - start_time, 2)} seconds.")

# Process all batches in parallel
def process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4):
    batch_list = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_batch_parallel, batch, batch_number+1, num_diverse_questions): batch_number+1
            for batch_number, batch in enumerate(batch_list)
        }
        concurrent.futures.wait(futures)

    # Save final responses in Excel
    save_responses_to_excel()

# Convert Excel to JSON
def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    try:
        df = pd.read_excel(file_path, dtype=str, engine="openpyxl").fillna("")  
        json_data = [{"Question_type": row["Question Type"], "Question": row["Question"], "Answer": row["Answer"]} 
                     for _, row in df.iterrows()]

        with open(output_json_path, "w", encoding="utf-8") as json_file:
            json.dump(json_data, json_file, indent=4, ensure_ascii=False)

        print(f"✅ Data successfully converted to JSON and saved to {output_json_path}")
        return output_json_path
    except Exception as e:
        print(f"❌ Error reading Excel file: {e}")
        return None

# Read JSON file
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)
        print(f"✅ Successfully read {len(data)} records from {file_path}")
        return data
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"❌ Error reading JSON: {e}")
    return []

# Convert Excel to JSON & Process Data
file_path = "/path/to/your/excel.xlsx"  # Replace with your actual file path
json_file_path = convert_excel_to_json(file_path)

if json_file_path:
    data = read_json_file(json_file_path)
    if data:
        process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4)







import json
import pandas as pd
import concurrent.futures
import time
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_openai import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnableLambda  # Fixes LangChain deprecation
from openpyxl import Workbook

# Initialize Azure OpenAI model
llm = AzureChatOpenAI(
    temperature=0,
    api_version="your_api_version",
    api_key="your_api_key",
    azure_endpoint="your_azure_endpoint",
    deployment_name="ssgpt-40",
    seed=1
)

# Define the prompt template
DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs across multiple question types.
For each input question-answer pair, generate up to {num_diverse_questions} distinct questions and answers.

### Question Types:
1. **Factual** - Direct and fact-based questions.
2. **Conceptual** - Questions that require understanding of principles or ideas.
3. **Inferential** - Questions that require drawing logical conclusions.
4. **Reasoning-Based** - Questions requiring analysis and logical reasoning.
5. **Procedural** - Questions related to step-by-step processes.

### Input Format:
{input_json}

### Instructions:
- Do not generate questions that are a subset of the original question.
- Ensure questions maintain the same broad scope.
- The diverse questions must be rephrased, restructured, or take a different approach while preserving the original intent.
- Ensure that procedural answers follow the original procedural format.
"""

# Use `prompt | llm` to fix LangChain deprecation warning
llm_chain = RunnableLambda(lambda x: llm(x))

# Retry logic for API quota limits
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def generate_diverse_questions(input_json, num_diverse_questions):
    """Calls the LLM model with retry logic if quota limits are hit."""
    try:
        response = llm_chain.invoke(
            {"input_json": input_json, "num_diverse_questions": str(num_diverse_questions)}
        )
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"❌ Failed to parse response: {e}")
        return []

# Append responses to JSON file immediately (prevents data loss)
def append_responses_to_json(batch_responses, batch_number, output_json_path="incremental_responses.json"):
    try:
        with open(output_json_path, "r+", encoding="utf-8") as json_file:
            try:
                existing_data = json.load(json_file)
            except json.JSONDecodeError:
                existing_data = []

            existing_data.extend(batch_responses)

            json_file.seek(0)
            json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

        print(f"✅ Batch {batch_number} responses appended to {output_json_path}")
    except Exception as e:
        print(f"❌ Error appending responses to JSON for batch {batch_number}: {e}")

# Save final responses to an Excel file
def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        excel_data = []
        for response in data:
            original_id = response.get("id", "")
            question_type = response.get("question_type", "")
            original_question = response.get("original_question", "")
            original_answer = response.get("original_answer", "")

            for diverse in response.get("diverse_questions", []):
                excel_data.append({
                    "ID": original_id,
                    "Question Type": question_type,
                    "Original Question": original_question,
                    "Original Answer": original_answer,
                    "Diverse Question": diverse.get("diverse_question", ""),
                    "Diverse Answer": diverse.get("diverse_answer", "")
                })

        df = pd.DataFrame(excel_data)

        # ✅ Explicitly use OpenPyXL
        with pd.ExcelWriter(output_excel_path, engine="openpyxl") as writer:
            df.to_excel(writer, index=False)

        print(f"✅ Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")

# Process a single batch in parallel
def process_batch_parallel(batch, batch_number, num_diverse_questions):
    print(f"🚀 Starting parallel processing for Batch {batch_number} with {len(batch)} records...")
    start_time = time.time()
    
    batch_input_json = json.dumps(batch)
    response_json = generate_diverse_questions(batch_input_json, num_diverse_questions)

    if response_json:
        append_responses_to_json(response_json, batch_number)

    end_time = time.time()
    print(f"✅ Batch {batch_number} processing completed in {round(end_time - start_time, 2)} seconds.")

# Process all batches in parallel
def process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4):
    batch_list = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_batch_parallel, batch, batch_number+1, num_diverse_questions): batch_number+1
            for batch_number, batch in enumerate(batch_list)
        }
        concurrent.futures.wait(futures)

    # Save final responses in Excel
    save_responses_to_excel()

# Convert Excel to JSON
def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    try:
        df = pd.read_excel(file_path, dtype=str, engine="openpyxl").fillna("")  
        json_data = [{"Question_type": row["Question Type"], "Question": row["Question"], "Answer": row["Answer"]} 
                     for _, row in df.iterrows()]

        with open(output_json_path, "w", encoding="utf-8") as json_file:
            json.dump(json_data, json_file, indent=4, ensure_ascii=False)

        print(f"✅ Data successfully converted to JSON and saved to {output_json_path}")
        return output_json_path
    except Exception as e:
        print(f"❌ Error reading Excel file: {e}")
        return None

# Read JSON file
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)
        print(f"✅ Successfully read {len(data)} records from {file_path}")
        return data
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"❌ Error reading JSON: {e}")
    return []

# Convert Excel to JSON & Process Data
file_path = "/path/to/your/excel.xlsx"  # Replace with your actual file path
json_file_path = convert_excel_to_json(file_path)

if json_file_path:
    data = read_json_file(json_file_path)
    if data:
        process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4)










import concurrent.futures

# Function to process a batch in parallel
def process_single_batch(batch, num_diverse_questions):
    """Processes a single batch of data and returns the response."""
    batch_input_json = json.dumps(batch)
    try:
        response_json = generate_diverse_questions(batch_input_json, num_diverse_questions)
        append_responses_to_json(response_json, output_json_path="incremental_responses.json")
        return response_json
    except Exception as e:
        print(f"Error processing batch: {e}")
        return []

# Function to process all batches in parallel
def process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4):
    """Processes multiple batches of data in parallel."""
    batches = list(split_into_fixed_batches(data, batch_size))

    # Using ThreadPoolExecutor for parallel API calls
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_single_batch, batch, num_diverse_questions): batch for batch in batches}

        for future in concurrent.futures.as_completed(futures):
            try:
                _ = future.result()  # Fetch result to catch exceptions
            except Exception as e:
                print(f"Error in parallel execution: {e}")

    # Save final responses to Excel after processing all batches
    save_responses_to_excel()

# Example usage
if data:
    process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4)








import json
import pandas as pd
import time
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_openai import AzureChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Initialize Azure OpenAI model
llm = AzureChatOpenAI(
    temperature=0,
    api_version="your_api_version",
    api_key="your_api_key",
    azure_endpoint="your_azure_endpoint",
    deployment_name="ssgpt-40",
    seed=1
)

# Function to convert Excel to JSON
def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    df = pd.read_excel(file_path, dtype=str).fillna("")  
    json_data = [{"Question_type": row["Question Type"], "Question": row["Question"], "Answer": row["Answer"]} 
                 for _, row in df.iterrows()]
    
    with open(output_json_path, "w", encoding="utf-8") as json_file:
        json.dump(json_data, json_file, indent=4, ensure_ascii=False)

    print(f"Data successfully converted to JSON and saved to {output_json_path}")
    return output_json_path

# Read JSON file function
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)
        print(f"Successfully read {len(data)} records from {file_path}")
        return data
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error reading JSON: {e}")
    return []

# Function to split data into batches
def split_into_fixed_batches(input_list, batch_size=2):
    for i in range(0, len(input_list), batch_size):
        yield input_list[i:i + batch_size]

# Define the prompt template
DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs across multiple question types.
For each input question-answer pair, generate up to {num_diverse_questions} distinct questions and answers.

### Question Types:
1. **Factual** - Direct and fact-based questions.
2. **Conceptual** - Questions that require understanding of principles or ideas.
3. **Inferential** - Questions that require drawing logical conclusions.
4. **Reasoning-Based** - Questions requiring analysis and logical reasoning.
5. **Procedural** - Questions related to step-by-step processes.

### Input Format:
{input_json}

### Instructions:
- Do not generate questions that are a subset of the original question.
- Ensure questions maintain the same broad scope.
- The diverse questions must be rephrased, restructured, or take a different approach while preserving the original intent.
- Ensure that procedural answers follow the original procedural format.

### Output Format:
[
    {
        "id": <unique_id>,
        "question_type": "<question type>",
        "original_question": "<Original Question>",
        "original_answer": "<Original Answer>",
        "diverse_questions": [
            {"diverse_question": "<Diverse Question 1>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 2>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 3>", "diverse_answer": "<Your Answer>"}
        ]
    }
]
"""

# Fill the prompt template
prompt = PromptTemplate(
    input_variables=["input_json", "num_diverse_questions"],
    template=DIVERSE_QUESTION_PROMPT
)

# Create the LLM chain
llm_chain = LLMChain(llm=llm, prompt=prompt)

# Retry logic to handle quota limits
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def generate_diverse_questions(input_json, num_diverse_questions):
    """Calls the LLM model with retry logic if quota limits are hit."""
    try:
        response = llm_chain.predict(
            input_json=input_json,
            num_diverse_questions=str(num_diverse_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"Failed to parse response: {e}")
        return []

# Process in batches & save output
def process_batches(data, batch_size=2, num_diverse_questions=3):
    output_json_path = "diverse_questions_output.json"
    output_excel_path = "diverse_questions_output.xlsx"
    
    all_responses = []
    batch_number = 1

    # Open JSON file in append mode to store responses immediately
    with open(output_json_path, "a", encoding="utf-8") as json_file:
        json_file.write("[")  # Start JSON array

        for batch in split_into_fixed_batches(data, batch_size):
            print(f"Processing batch {batch_number} with {len(batch)} question-answer pairs...")
            batch_input_json = json.dumps(batch)

            try:
                response_json = generate_diverse_questions(batch_input_json, num_diverse_questions)
                all_responses.extend(response_json)

                # Append each response immediately to JSON file
                json.dump(response_json, json_file, indent=4, ensure_ascii=False)
                json_file.write(",")  # Add comma between responses
            except Exception as e:
                print(f"Error processing batch {batch_number}: {e}")

            batch_number += 1
            time.sleep(1)  # Adding slight delay to avoid quota issues

        json_file.write("]")  # End JSON array

    print(f"Generated diverse questions saved to {output_json_path}")

    # Save to Excel
    df = pd.DataFrame(all_responses)
    df.to_excel(output_excel_path, index=False)
    print(f"Final output stored in Excel file: {output_excel_path}")

    return all_responses

# Example usage
file_path = "/path/to/your/excel.xlsx"
output_json_path = "output_data.json"

json_file_path = convert_excel_to_json(file_path, output_json_path)
data = read_json_file(json_file_path)

if data:
    diverse_questions = process_batches(data, batch_size=2, num_diverse_questions=3)
    print(json.dumps(diverse_questions, indent=4))








import json
import pandas as pd
import concurrent.futures
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_openai import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnableLambda  # New LangChain import
from openpyxl import Workbook

# Initialize Azure OpenAI model
llm = AzureChatOpenAI(
    temperature=0,
    api_version="your_api_version",
    api_key="your_api_key",
    azure_endpoint="your_azure_endpoint",
    deployment_name="ssgpt-40",
    seed=1
)

# Define the prompt template
DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs across multiple question types.
For each input question-answer pair, generate up to {num_diverse_questions} distinct questions and answers.

### Question Types:
1. **Factual** - Direct and fact-based questions.
2. **Conceptual** - Questions that require understanding of principles or ideas.
3. **Inferential** - Questions that require drawing logical conclusions.
4. **Reasoning-Based** - Questions requiring analysis and logical reasoning.
5. **Procedural** - Questions related to step-by-step processes.

### Input Format:
{input_json}

### Instructions:
- Do not generate questions that are a subset of the original question.
- Ensure questions maintain the same broad scope.
- The diverse questions must be rephrased, restructured, or take a different approach while preserving the original intent.
- Ensure that procedural answers follow the original procedural format.

### Output Format:
[
    {
        "id": <unique_id>,
        "question_type": "<question type>",
        "original_question": "<Original Question>",
        "original_answer": "<Original Answer>",
        "diverse_questions": [
            {"diverse_question": "<Diverse Question 1>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 2>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 3>", "diverse_answer": "<Your Answer>"}
        ]
    }
]
"""

# Fix LangChain deprecation: use `prompt | llm` instead of LLMChain
llm_chain = RunnableLambda(lambda x: llm(x))

# Retry logic for API quota limits
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def generate_diverse_questions(input_json, num_diverse_questions):
    """Calls the LLM model with retry logic if quota limits are hit."""
    try:
        response = llm_chain.invoke(
            {"input_json": input_json, "num_diverse_questions": str(num_diverse_questions)}
        )
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"❌ Failed to parse response: {e}")
        return []

# Append responses to JSON file immediately
def append_responses_to_json(batch_responses, output_json_path="incremental_responses.json"):
    try:
        with open(output_json_path, "r+", encoding="utf-8") as json_file:
            try:
                existing_data = json.load(json_file)
            except json.JSONDecodeError:
                existing_data = []

            existing_data.extend(batch_responses)

            json_file.seek(0)
            json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

        print(f"✅ Batch responses appended to {output_json_path}")
    except Exception as e:
        print(f"❌ Error appending responses to JSON: {e}")

# Save responses to Excel
def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        excel_data = []
        for response in data:
            original_id = response.get("id", "")
            question_type = response.get("question_type", "")
            original_question = response.get("original_question", "")
            original_answer = response.get("original_answer", "")

            for diverse in response.get("diverse_questions", []):
                excel_data.append({
                    "ID": original_id,
                    "Question Type": question_type,
                    "Original Question": original_question,
                    "Original Answer": original_answer,
                    "Diverse Question": diverse.get("diverse_question", ""),
                    "Diverse Answer": diverse.get("diverse_answer", "")
                })

        df = pd.DataFrame(excel_data)

        # ✅ Explicitly use OpenPyXL
        with pd.ExcelWriter(output_excel_path, engine="openpyxl") as writer:
            df.to_excel(writer, index=False)

        print(f"✅ Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")

# Parallel batch processing
def process_batch_parallel(batch, num_diverse_questions):
    batch_input_json = json.dumps(batch)
    response_json = generate_diverse_questions(batch_input_json, num_diverse_questions)

    if response_json:
        append_responses_to_json(response_json)

# Run batches in parallel
def process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4):
    batch_list = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_batch_parallel, batch, num_diverse_questions) for batch in batch_list]
        concurrent.futures.wait(futures)

    # Save final responses in Excel
    save_responses_to_excel()

# Convert Excel to JSON
def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    try:
        df = pd.read_excel(file_path, dtype=str, engine="openpyxl").fillna("")  
        json_data = [{"Question_type": row["Question Type"], "Question": row["Question"], "Answer": row["Answer"]} 
                     for _, row in df.iterrows()]

        with open(output_json_path, "w", encoding="utf-8") as json_file:
            json.dump(json_data, json_file, indent=4, ensure_ascii=False)

        print(f"✅ Data successfully converted to JSON and saved to {output_json_path}")
        return output_json_path
    except Exception as e:
        print(f"❌ Error reading Excel file: {e}")
        return None

# Read JSON file
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)
        print(f"✅ Successfully read {len(data)} records from {file_path}")
        return data
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"❌ Error reading JSON: {e}")
    return []

# Convert Excel to JSON & Process Data
file_path = "/path/to/your/excel.xlsx"  # Replace with your actual file path
json_file_path = convert_excel_to_json(file_path)

if json_file_path:
    data = read_json_file(json_file_path)
    if data:
        process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4)










import json
import pandas as pd
import time
import concurrent.futures
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_openai import AzureChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from openpyxl import Workbook

# Initialize Azure OpenAI model
llm = AzureChatOpenAI(
    temperature=0,
    api_version="your_api_version",
    api_key="your_api_key",
    azure_endpoint="your_azure_endpoint",
    deployment_name="ssgpt-40",
    seed=1
)

# Define the prompt template
DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs across multiple question types.
For each input question-answer pair, generate up to {num_diverse_questions} distinct questions and answers.

### Question Types:
1. **Factual** - Direct and fact-based questions.
2. **Conceptual** - Questions that require understanding of principles or ideas.
3. **Inferential** - Questions that require drawing logical conclusions.
4. **Reasoning-Based** - Questions requiring analysis and logical reasoning.
5. **Procedural** - Questions related to step-by-step processes.

### Input Format:
{input_json}

### Instructions:
- Do not generate questions that are a subset of the original question.
- Ensure questions maintain the same broad scope.
- The diverse questions must be rephrased, restructured, or take a different approach while preserving the original intent.
- Ensure that procedural answers follow the original procedural format.

### Output Format:
[
    {
        "id": <unique_id>,
        "question_type": "<question type>",
        "original_question": "<Original Question>",
        "original_answer": "<Original Answer>",
        "diverse_questions": [
            {"diverse_question": "<Diverse Question 1>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 2>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 3>", "diverse_answer": "<Your Answer>"}
        ]
    }
]
"""

# Fill the prompt template
prompt = PromptTemplate(
    input_variables=["input_json", "num_diverse_questions"],
    template=DIVERSE_QUESTION_PROMPT
)

# Create the LLM chain
llm_chain = LLMChain(llm=llm, prompt=prompt)

# Retry logic to handle quota limits
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def generate_diverse_questions(input_json, num_diverse_questions):
    """Calls the LLM model with retry logic if quota limits are hit."""
    try:
        response = llm_chain.predict(
            input_json=input_json,
            num_diverse_questions=str(num_diverse_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"❌ Failed to parse response: {e}")
        return []

# Append responses to JSON file immediately
def append_responses_to_json(batch_responses, output_json_path="incremental_responses.json"):
    try:
        with open(output_json_path, "r+", encoding="utf-8") as json_file:
            try:
                existing_data = json.load(json_file)
            except json.JSONDecodeError:
                existing_data = []

            existing_data.extend(batch_responses)

            json_file.seek(0)
            json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

        print(f"✅ Batch responses appended to {output_json_path}")
    except Exception as e:
        print(f"❌ Error appending responses to JSON: {e}")

# Save final responses to an Excel file
def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # Convert JSON to Pandas DataFrame
        excel_data = []
        for response in data:
            original_id = response.get("id", "")
            question_type = response.get("question_type", "")
            original_question = response.get("original_question", "")
            original_answer = response.get("original_answer", "")

            for diverse in response.get("diverse_questions", []):
                excel_data.append({
                    "ID": original_id,
                    "Question Type": question_type,
                    "Original Question": original_question,
                    "Original Answer": original_answer,
                    "Diverse Question": diverse.get("diverse_question", ""),
                    "Diverse Answer": diverse.get("diverse_answer", "")
                })

        df = pd.DataFrame(excel_data)

        # ✅ Ensure OpenPyXL is explicitly used
        with pd.ExcelWriter(output_excel_path, engine="openpyxl") as writer:
            df.to_excel(writer, index=False)

        print(f"✅ Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")

# Parallel processing function
def process_batch_parallel(batch, num_diverse_questions):
    """Processes a single batch in parallel."""
    batch_input_json = json.dumps(batch)
    response_json = generate_diverse_questions(batch_input_json, num_diverse_questions)

    if response_json:
        append_responses_to_json(response_json, output_json_path="incremental_responses.json")

# Process all batches in parallel
def process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4):
    """Processes data in parallel batches using ThreadPoolExecutor."""
    batch_list = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_batch_parallel, batch, num_diverse_questions) for batch in batch_list]
        concurrent.futures.wait(futures)

    # Save final responses in Excel
    save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx")

# Convert Excel to JSON
def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    df = pd.read_excel(file_path, dtype=str).fillna("")  
    json_data = [{"Question_type": row["Question Type"], "Question": row["Question"], "Answer": row["Answer"]} 
                 for _, row in df.iterrows()]

    with open(output_json_path, "w", encoding="utf-8") as json_file:
        json.dump(json_data, json_file, indent=4, ensure_ascii=False)

    print(f"✅ Data successfully converted to JSON and saved to {output_json_path}")
    return output_json_path

# Read JSON file function
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)
        print(f"✅ Successfully read {len(data)} records from {file_path}")
        return data
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"❌ Error reading JSON: {e}")
    return []

# Convert Excel to JSON & Process Data
file_path = "/path/to/your/excel.xlsx"
json_file_path = convert_excel_to_json(file_path)
data = read_json_file(json_file_path)

if data:
    process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4)









import json
import pandas as pd
from openpyxl import Workbook

def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        # Load JSON file and check structure
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        if not isinstance(data, list):
            raise ValueError("JSON data should be a list of dictionaries.")

        # Debug JSON structure
        print("✅ JSON loaded successfully.")
        print("🔍 Sample entry:", data[0])

        # Convert JSON to DataFrame
        df = pd.json_normalize(
            data,
            record_path=['diverse_questions'],  
            meta=['id', 'question_type', 'original_question', 'original_answer'],
            errors='ignore'  
        )

        # Debug DataFrame structure
        print("📊 DataFrame preview:\n", df.head())

        # Write to Excel with explicit engine
        with pd.ExcelWriter(output_excel_path, engine="openpyxl", mode="w") as writer:
            df.to_excel(writer, index=False, sheet_name="Diverse Questions")

        print(f"✅ Final responses saved to {output_excel_path}")

    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")

# Run the function
save_responses_to_excel()






import json
import pandas as pd
from openpyxl import Workbook

def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        # Load JSON data
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # Check if data is a list
        if not isinstance(data, list):
            raise ValueError("JSON data should be a list of dictionaries.")

        # Normalize JSON
        df = pd.json_normalize(
            data,
            record_path=['diverse_questions'],  # Extract questions
            meta=['id', 'question_type', 'original_question', 'original_answer'],
            errors='ignore'  # Ignore missing keys instead of raising an error
        )

        # ✅ Ensure OpenPyXL is explicitly used
        with pd.ExcelWriter(output_excel_path, engine='openpyxl', mode='w') as writer:
            df.to_excel(writer, index=False)

        print(f"✅ Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")

# Call function to test
save_responses_to_excel()









import json
import pandas as pd
from openpyxl import Workbook

def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        # Load JSON data
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # Check if data is a list
        if not isinstance(data, list):
            raise ValueError("JSON data should be a list of dictionaries.")

        # Normalize JSON
        df = pd.json_normalize(
            data,
            record_path=['diverse_questions'],  # Extract questions
            meta=['id', 'question_type', 'original_question', 'original_answer'],
            errors='ignore'  # Ignore missing keys instead of raising an error
        )

        # ✅ Ensure OpenPyXL is explicitly used
        with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)

        print(f"✅ Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")

# Call function to test
save_responses_to_excel()








import pandas as pd
from openpyxl import Workbook

def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        # Load JSON data
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # Convert JSON to Pandas DataFrame
        df = pd.json_normalize(data, record_path=['diverse_questions'], 
                               meta=['id', 'question_type', 'original_question', 'original_answer'])

        # ✅ Ensure OpenPyXL is explicitly used
        with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)

        print(f"✅ Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")









# Ensure openpyxl is imported
import pandas as pd
from openpyxl import Workbook

def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        # Load JSON data
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # Convert JSON to Pandas DataFrame (handling nested structures)
        df = pd.json_normalize(data, record_path=['diverse_questions'], 
                               meta=['id', 'question_type', 'original_question', 'original_answer'])

        # ✅ Use ExcelWriter with explicit engine
        with pd.ExcelWriter(output_excel_path, mode='w', engine='openpyxl') as writer:
            df.to_excel(writer, index=False)

        print(f"✅ Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")






import pandas as pd
from openpyxl import Workbook  # Ensure openpyxl is available

# Function to save final responses to an Excel file
def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        # Load JSON data
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # Convert JSON to a Pandas DataFrame (handling nested structures)
        df = pd.json_normalize(data, record_path=['diverse_questions'], 
                               meta=['id', 'question_type', 'original_question', 'original_answer'])

        # Use ExcelWriter to specify engine explicitly
        with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)

        print(f"✅ Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")








import json
import pandas as pd
import time
import concurrent.futures
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_openai import AzureChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from openpyxl import load_workbook

# Initialize Azure OpenAI model
llm = AzureChatOpenAI(
    temperature=0,
    api_version="your_api_version",
    api_key="your_api_key",
    azure_endpoint="your_azure_endpoint",
    deployment_name="ssgpt-40",
    seed=1
)

# Define the prompt template
DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs across multiple question types.
For each input question-answer pair, generate up to {num_diverse_questions} distinct questions and answers.

### Question Types:
1. **Factual** - Direct and fact-based questions.
2. **Conceptual** - Questions that require understanding of principles or ideas.
3. **Inferential** - Questions that require drawing logical conclusions.
4. **Reasoning-Based** - Questions requiring analysis and logical reasoning.
5. **Procedural** - Questions related to step-by-step processes.

### Input Format:
{input_json}

### Instructions:
- Do not generate questions that are a subset of the original question.
- Ensure questions maintain the same broad scope.
- The diverse questions must be rephrased, restructured, or take a different approach while preserving the original intent.
- Ensure that procedural answers follow the original procedural format.

### Output Format:
[
    {
        "id": <unique_id>,
        "question_type": "<question type>",
        "original_question": "<Original Question>",
        "original_answer": "<Original Answer>",
        "diverse_questions": [
            {"diverse_question": "<Diverse Question 1>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 2>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 3>", "diverse_answer": "<Your Answer>"}
        ]
    }
]
"""

# Fill the prompt template
prompt = PromptTemplate(
    input_variables=["input_json", "num_diverse_questions"],
    template=DIVERSE_QUESTION_PROMPT
)

# Create the LLM chain
llm_chain = LLMChain(llm=llm, prompt=prompt)

# Retry logic to handle quota limits
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def generate_diverse_questions(input_json, num_diverse_questions):
    """Calls the LLM model with retry logic if quota limits are hit."""
    try:
        response = llm_chain.predict(
            input_json=input_json,
            num_diverse_questions=str(num_diverse_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"Failed to parse response: {e}")
        return []

# Append responses to JSON file immediately
def append_responses_to_json(batch_responses, output_json_path="incremental_responses.json"):
    try:
        with open(output_json_path, "r+", encoding="utf-8") as json_file:
            try:
                existing_data = json.load(json_file)
            except json.JSONDecodeError:
                existing_data = []

            existing_data.extend(batch_responses)

            json_file.seek(0)
            json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

        print(f"✅ Batch responses appended to {output_json_path}")
    except Exception as e:
        print(f"❌ Error appending responses to JSON: {e}")

# Save final responses to an Excel file
def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        df = pd.json_normalize(data, record_path=['diverse_questions'], 
                               meta=['id', 'question_type', 'original_question', 'original_answer'])

        with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)

        print(f"✅ Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")

# Parallel processing function
def process_batch_parallel(batch, num_diverse_questions):
    """Processes a single batch in parallel."""
    batch_input_json = json.dumps(batch)
    response_json = generate_diverse_questions(batch_input_json, num_diverse_questions)

    if response_json:
        append_responses_to_json(response_json, output_json_path="incremental_responses.json")

# Process all batches in parallel
def process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4):
    """Processes data in parallel batches using ThreadPoolExecutor."""
    batch_list = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_batch_parallel, batch, num_diverse_questions) for batch in batch_list]
        concurrent.futures.wait(futures)  # Ensure all tasks are completed before moving forward

    # Save final responses in Excel
    save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx")

# Example usage
file_path = "/path/to/your/excel.xlsx"
output_json_path = "output_data.json"

# Convert Excel to JSON
def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    df = pd.read_excel(file_path, dtype=str).fillna("")  
    json_data = [{"Question_type": row["Question Type"], "Question": row["Question"], "Answer": row["Answer"]} 
                 for _, row in df.iterrows()]

    with open(output_json_path, "w", encoding="utf-8") as json_file:
        json.dump(json_data, json_file, indent=4, ensure_ascii=False)

    print(f"✅ Data successfully converted to JSON and saved to {output_json_path}")
    return output_json_path

# Read JSON file function
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)
        print(f"✅ Successfully read {len(data)} records from {file_path}")
        return data
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"❌ Error reading JSON: {e}")
    return []

# Convert Excel to JSON & Process Data
json_file_path = convert_excel_to_json(file_path, output_json_path)
data = read_json_file(json_file_path)

if data:
    process_batches_parallel(data, batch_size=2, num_diverse_questions=3, max_workers=4)











import openpyxl  # Ensure openpyxl is imported

# Function to save final responses to an Excel file
def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        # Load the JSON data
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # Convert JSON to a Pandas DataFrame (handling nested structures)
        df = pd.json_normalize(data, record_path=['diverse_questions'], 
                               meta=['id', 'question_type', 'original_question', 'original_answer'])

        # **Use an explicit ExcelWriter to avoid format issues**
        with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)

        print(f"✅ Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"❌ Error saving responses to Excel: {e}")






import json
import pandas as pd
import time
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_core.messages import HumanMessage
from langchain_openai import AzureChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Initialize Azure OpenAI model
llm = AzureChatOpenAI(
    temperature=0,
    api_version="your_api_version",
    api_key="your_api_key",
    azure_endpoint="your_azure_endpoint",
    deployment_name="ssgpt-40",
    seed=1
)

# Function to convert Excel to JSON
def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    df = pd.read_excel(file_path, dtype=str).fillna("")  # Load and clean the data
    json_data = [{"Question_type": row["Question Type"], "Question": row["Question"], "Answer": row["Answer"]} 
                 for _, row in df.iterrows()]
    
    with open(output_json_path, "w", encoding="utf-8") as json_file:
        json.dump(json_data, json_file, indent=4, ensure_ascii=False)

    print(f"Data successfully converted to JSON and saved to {output_json_path}")
    return output_json_path

# Read JSON file function
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)
        print(f"Successfully read {len(data)} records from {file_path}")
        return data
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error reading JSON: {e}")
    return []

# Function to split data into batches
def split_into_fixed_batches(input_list, batch_size=2):
    for i in range(0, len(input_list), batch_size):
        yield input_list[i:i + batch_size]

# Define the prompt template
DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs across multiple question types.
For each input question-answer pair, generate up to {num_diverse_questions} distinct questions and answers.

### Question Types:
1. **Factual** - Direct and fact-based questions.
2. **Conceptual** - Questions that require understanding of principles or ideas.
3. **Inferential** - Questions that require drawing logical conclusions.
4. **Reasoning-Based** - Questions requiring analysis and logical reasoning.
5. **Procedural** - Questions related to step-by-step processes.

### Input Format:
{input_json}

### Instructions:
- Do not generate questions that are a subset of the original question.
- Ensure questions maintain the same broad scope.
- The diverse questions must be rephrased, restructured, or take a different approach while preserving the original intent.
- Ensure that procedural answers follow the original procedural format.

### Output Format:
[
    {
        "id": <unique_id>,
        "question_type": "<question type>",
        "original_question": "<Original Question>",
        "original_answer": "<Original Answer>",
        "diverse_questions": [
            {"diverse_question": "<Diverse Question 1>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 2>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 3>", "diverse_answer": "<Your Answer>"}
        ]
    }
]
"""

# Fill the prompt template
prompt = PromptTemplate(
    input_variables=["input_json", "num_diverse_questions"],
    template=DIVERSE_QUESTION_PROMPT
)

# Create the LLM chain
llm_chain = LLMChain(llm=llm, prompt=prompt)

# Retry logic to handle quota limits
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def generate_diverse_questions(input_json, num_diverse_questions):
    """Calls the LLM model with retry logic if quota limits are hit."""
    try:
        response = llm_chain.predict(
            input_json=input_json,
            num_diverse_questions=str(num_diverse_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"Failed to parse response: {e}")
        return []

# Function to append responses to JSON immediately after each batch
def append_responses_to_json(batch_responses, output_json_path="incremental_responses.json"):
    try:
        # Load existing data if the file exists
        try:
            with open(output_json_path, "r", encoding="utf-8") as json_file:
                existing_data = json.load(json_file)
        except (FileNotFoundError, json.JSONDecodeError):
            existing_data = []

        # Append new responses
        existing_data.extend(batch_responses)

        # Save back to file
        with open(output_json_path, "w", encoding="utf-8") as json_file:
            json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

        print(f"Batch responses appended to {output_json_path}")
    except Exception as e:
        print(f"Error appending responses to JSON: {e}")

# Function to save final responses to Excel
def save_responses_to_excel(output_json_path="incremental_responses.json", output_excel_path="final_diverse_questions.xlsx"):
    try:
        # Load the JSON data
        with open(output_json_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # Convert to DataFrame
        df = pd.json_normalize(data, record_path=['diverse_questions'], 
                               meta=['id', 'question_type', 'original_question', 'original_answer'])

        # Save to Excel
        df.to_excel(output_excel_path, index=False)
        print(f"Final responses saved to Excel at {output_excel_path}")
    except Exception as e:
        print(f"Error saving responses to Excel: {e}")

# Process in batches
def process_batches(data, batch_size=2, num_diverse_questions=3):
    batch_number = 1

    for batch in split_into_fixed_batches(data, batch_size):
        print(f"Processing batch {batch_number} with {len(batch)} question-answer pairs...")
        batch_input_json = json.dumps(batch)

        try:
            response_json = generate_diverse_questions(batch_input_json, num_diverse_questions)

            # Append responses immediately to JSON
            append_responses_to_json(response_json, output_json_path="incremental_responses.json")

        except Exception as e:
            print(f"Error processing batch {batch_number}: {e}")

        batch_number += 1
        time.sleep(1)  # Adding slight delay to avoid quota issues

    # After all batches are processed, store final Excel output
    save_responses_to_excel()

# Example usage
file_path = "/path/to/your/excel.xlsx"
output_json_path = "output_data.json"

json_file_path = convert_excel_to_json(file_path, output_json_path)
data = read_json_file(json_file_path)

if data:
    process_batches(data, batch_size=2, num_diverse_questions=3)









import json
import pandas as pd
import time
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_core.messages import HumanMessage
from langchain_openai import AzureChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Initialize Azure OpenAI model
llm = AzureChatOpenAI(
    temperature=0,
    api_version="your_api_version",
    api_key="your_api_key",
    azure_endpoint="your_azure_endpoint",
    deployment_name="ssgpt-40",
    seed=1
)

# Function to convert Excel to JSON
def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    df = pd.read_excel(file_path, dtype=str).fillna("")  # Load and clean the data
    json_data = [{"Question_type": row["Question Type"], "Question": row["Question"], "Answer": row["Answer"]} 
                 for _, row in df.iterrows()]
    
    with open(output_json_path, "w", encoding="utf-8") as json_file:
        json.dump(json_data, json_file, indent=4, ensure_ascii=False)

    print(f"Data successfully converted to JSON and saved to {output_json_path}")
    return output_json_path

# Read JSON file function
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)
        print(f"Successfully read {len(data)} records from {file_path}")
        return data
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error reading JSON: {e}")
    return []

# Function to split data into batches
def split_into_fixed_batches(input_list, batch_size=2):
    for i in range(0, len(input_list), batch_size):
        yield input_list[i:i + batch_size]

# Define the prompt template
DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs across multiple question types.
For each input question-answer pair, generate up to {num_diverse_questions} distinct questions and answers.

### Question Types:
1. **Factual** - Direct and fact-based questions.
2. **Conceptual** - Questions that require understanding of principles or ideas.
3. **Inferential** - Questions that require drawing logical conclusions.
4. **Reasoning-Based** - Questions requiring analysis and logical reasoning.
5. **Procedural** - Questions related to step-by-step processes.

### Input Format:
{input_json}

### Instructions:
- Do not generate questions that are a subset of the original question.
- Ensure questions maintain the same broad scope.
- The diverse questions must be rephrased, restructured, or take a different approach while preserving the original intent.
- Ensure that procedural answers follow the original procedural format.

### Output Format:
[
    {
        "id": <unique_id>,
        "question_type": "<question type>",
        "original_question": "<Original Question>",
        "original_answer": "<Original Answer>",
        "diverse_questions": [
            {"diverse_question": "<Diverse Question 1>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 2>", "diverse_answer": "<Your Answer>"},
            {"diverse_question": "<Diverse Question 3>", "diverse_answer": "<Your Answer>"}
        ]
    }
]
"""

# Fill the prompt template
prompt = PromptTemplate(
    input_variables=["input_json", "num_diverse_questions"],
    template=DIVERSE_QUESTION_PROMPT
)

# Create the LLM chain
llm_chain = LLMChain(llm=llm, prompt=prompt)

# Retry logic to handle quota limits
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def generate_diverse_questions(input_json, num_diverse_questions):
    """Calls the LLM model with retry logic if quota limits are hit."""
    try:
        response = llm_chain.predict(
            input_json=input_json,
            num_diverse_questions=str(num_diverse_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"Failed to parse response: {e}")
        return []

# Process in batches
def process_batches(data, batch_size=2, num_diverse_questions=3):
    all_responses = []
    batch_number = 1

    for batch in split_into_fixed_batches(data, batch_size):
        print(f"Processing batch {batch_number} with {len(batch)} question-answer pairs...")
        batch_input_json = json.dumps(batch)

        try:
            response_json = generate_diverse_questions(batch_input_json, num_diverse_questions)
            all_responses.extend(response_json)
        except Exception as e:
            print(f"Error processing batch {batch_number}: {e}")

        batch_number += 1
        time.sleep(1)  # Adding slight delay to avoid quota issues

    # Save final results
    output_json_path = "diverse_questions_output.json"
    with open(output_json_path, "w", encoding="utf-8") as json_file:
        json.dump(all_responses, json_file, indent=4, ensure_ascii=False)

    print(f"Generated diverse questions saved to {output_json_path}")
    return all_responses

# Example usage
file_path = "/path/to/your/excel.xlsx"
output_json_path = "output_data.json"

json_file_path = convert_excel_to_json(file_path, output_json_path)
data = read_json_file(json_file_path)

if data:
    diverse_questions = process_batches(data, batch_size=2, num_diverse_questions=3)
    print(json.dumps(diverse_questions, indent=4))









"In the Databricks environment, I am generating diverse question-answer pairs. Up to 511 questions are generated successfully, but after that, the process becomes stable and stops generating more questions. Can you please suggest a solution?"






import pandas as pd

# Load the Excel file
file_path = "your_file.xlsx"  # Update with your actual file path
df = pd.read_excel(file_path, sheet_name="Sheet1")  # Update the sheet name if needed

# Remove all occurrences of duplicate questions
df_cleaned = df[df.duplicated(subset=['Question'], keep=False) == False]

# Save the cleaned data back to a new Excel file
cleaned_file_path = "cleaned_data.xlsx"
df_cleaned.to_excel(cleaned_file_path, index=False)

# Display the cleaned DataFrame
print("Cleaned DataFrame:")
print(df_cleaned)

print(f"\nCleaned data saved to: {cleaned_file_path}")






import pandas as pd
import ace_tools as tools  # For displaying the cleaned DataFrame

# Load the Excel file
file_path = "your_file.xlsx"  # Update with your actual file path
df = pd.read_excel(file_path, sheet_name="Sheet1")  # Update the sheet name if needed

# Remove all occurrences of duplicate questions
df_cleaned = df[df.duplicated(subset=['Question'], keep=False) == False]

# Save the cleaned data back to a new Excel file
cleaned_file_path = "cleaned_data.xlsx"
df_cleaned.to_excel(cleaned_file_path, index=False)

# Display the cleaned DataFrame
tools.display_dataframe_to_user(name="Cleaned DataFrame", dataframe=df_cleaned)

print(f"Cleaned data saved to: {cleaned_file_path}")






FACTUAL PROMPT

You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.

Your task is to generate up to (num_questions) distinct factual question-answer pairs based on the Input.

### Guidelines:

- **Context-Specific Questions:** The questions must be **directly related** to the input content, focusing on **key facts, entities, names, titles, figures, or specific financial terms**.
- **No Repetitive or Generic Questions:** Avoid **generic** or **redundant** questions.
- **Exclude Template-Related & Procedural Questions:** **DO NOT** include questions related to **document templates, version numbers, dates, procedural identifiers, or metadata**.
- **Strictly Text-Based:** **DO NOT** generate questions from **image descriptions, figures, or diagrams**.
- **Valid JSON Format:** Ensure **properly formatted JSON** with **no additional text explanations**.

### Input:

(paragraph)

### Your Goal:

1. **Analyze the input content** to generate factual questions.
2. Ensure the **questions and answers** reflect **unique aspects** of the provided input.
3. If fewer distinct questions are possible, **return only the available ones**.

### Output Format:

[
  {
    "question": "What is the name of the entity mentioned in the input text?",
    "answer": "Name of the entity mentioned in the text."
  },
  {
    "question": "What financial term is discussed in the input?",
    "answer": "The financial term discussed is XYZ."
  },
  {
    "question": "What investment strategy is outlined in the input?",
    "answer": "The investment strategy outlined is ABC."
  }
]

### Constraints:

- **DO NOT** include text explanations—**only provide the JSON output**.
- **DO NOT** generate questions from images or their descriptions.
- **DO NOT** include questions related to version numbers, dates, document templates, or metadata.
- **Ensure the JSON array is properly formatted and fully closed**.
- **All questions must be strictly context-related**.










REASONING_PROMPT = """
You are a financial expert with expertise in corporate finance, investment strategies, and accounting.

Your task is to analyze and generate up to (num_questions) distinct **context-specific reasoning-based** question-answer pairs based on the input text.

### **Ensure:**
1. **All questions must begin with "Why".**
2. **Each question explicitly references a specific part of the input text** to ensure strong contextual relevance.
3. **Questions require logical argumentation, critical thinking, or evaluation** based on the provided data.
4. **Answers must justify conclusions** with clear and logical reasoning.
5. **Questions and answers must be unique** and should not repeat.
6. **If fewer distinct questions are possible, return only the available ones.**
7. **The response must be a valid JSON array**, properly formatted and fully closed.

### **Input Format:**  
{paragraph}

### **Output Format:**  
[
  {
    "question": "Why did [specific event, trend, or decision from input] occur, and what are the underlying reasons?",
    "answer": "A detailed reasoning or justification based on logical argumentation and input context."
  },
  {
    "question": "Why does the company’s decision to [specific financial action from input] impact its short-term and long-term financial stability?",
    "answer": "An evaluation of potential risks and benefits, referencing financial data from the input."
  },
  {
    "question": "Why has [specific financial metric or trend from input] changed, and what does this indicate about the company's overall financial health?",
    "answer": "A reasoned explanation of the factors influencing this change and its implications."
  },
  {
    "question": "Why is [investment strategy, debt management, or market expansion from input] a critical factor in the company's growth trajectory?",
    "answer": "A discussion of the strategic importance, potential risks, and expected outcomes."
  }
]
"""










CONCEPTUAL_PROMPT = """
You are a financial expert with expertise in corporate finance, investment strategies, and accounting.

Your task is to analyze and generate (num_questions) unique **conceptual question-answer** pairs based on the input text.

### **Ensure:**
1. **Each question explores theories, principles, or underlying ideas** related to the input context.
2. **Questions and answers must be distinct** and should not repeat the question.
3. **Answers must provide detailed explanations** of the relevant concepts.
4. **Do not include factual, inferential, or procedural questions—focus only on conceptual aspects.**
5. **The response must be a valid JSON array**, properly formatted and fully closed.

### **Input Format:**  
{paragraph}

### **Output Format:**  
[
  {
    "question": "Your conceptual question exploring a financial principle or theory from the input",
    "answer": "Your conceptual answer providing a detailed explanation of that principle or theory."
  },
  {
    "question": "Your conceptual question exploring another financial principle or theory from the input",
    "answer": "Your conceptual answer providing a detailed explanation of that principle or theory."
  }
]
"""









import pandas as pd
import json

def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    # Read Excel file
    df = pd.read_excel(file_path, dtype=str)

    # Fill NaN values with empty strings
    df = df.fillna("")

    # Convert DataFrame to list of dictionaries
    json_data = df.to_dict(orient="records")

    # Write JSON file
    with open(output_json_path, "w", encoding="utf-8") as json_file:
        json.dump(json_data, json_file, indent=4, ensure_ascii=False)

    print(f"Data successfully converted to JSON and saved to {output_json_path}")

    return output_json_path




INFERENTIAL_PROMPT = """
You are a financial expert with expertise in corporate finance, investment strategies, and accounting.

Your task is to analyze and generate (num_questions) unique **context-specific inferential** question-answer pairs based on the input text.

### **Ensure:**
1. **Each question explicitly references a specific part of the input text.**  
2. **Questions and answers must be distinct** and should not repeat information.  
3. **Questions should encourage deeper analysis, requiring interpretation or conclusions based on the provided financial data, strategy, or scenario.**  
4. **Answers must justify inferences with logical reasoning, using details from the input text.**  
5. **The response must be a valid JSON array**, properly formatted and fully closed.  

### **Input Format:**  
{paragraph}

### **Output Format:**  
[
  {
    "question": "Your inferential question explicitly referencing a specific part of the input context",
    "answer": "Your inference-based answer with justification referencing that specific context."
  },
  {
    "question": "Your inferential question explicitly referencing another specific part of the input context",
    "answer": "Your inference-based answer with justification referencing that specific context."
  }
]
"""









import pandas as pd

def save_responses_to_excel(all_responses, excel_file_path="diverse_questions_and_answers.xlsx"):
    excel_data = []

    for response in all_responses:
        original_id = response["id"]
        question_type = response["question_type"]
        original_question = response["original_question"]
        original_answer = response["original_answer"]

        for diverse in response["diverse_questions"]:
            excel_data.append({
                "ID": original_id,
                "Question Type": question_type,
                "Original Question": original_question,
                "Original Answer": original_answer,
                "Diverse Question": diverse["diverse_question"],
                "Diverse Answer": diverse["diverse_answer"]
            })

    # Convert to DataFrame
    df = pd.DataFrame(excel_data, columns=[
        "ID", "Question Type", "Original Question", "Original Answer",
        "Diverse Question", "Diverse Answer"
    ])

    # Define a custom sort order for question types
    question_type_order = {
        "Factual": 1,
        "Procedural": 2,
        "Conceptual": 3,
        "Inferential": 4,
        "Reasoning-Based": 5
    }

    # Sort DataFrame based on custom question type order
    df["Question Type Rank"] = df["Question Type"].map(question_type_order)
    df.sort_values(by=["Question Type Rank", "ID"], inplace=True)
    df.drop(columns=["Question Type Rank"], inplace=True)  # Remove temporary column

    # Save to Excel
    df.to_excel(excel_file_path, index=False, engine="openpyxl")

    print(f"Data saved successfully to {excel_file_path}")
    return excel_file_path

# Call function
save_responses_to_excel(all_responses, "diverse_questions_and_answers_1000.xlsx")





import pandas as pd

def save_responses_to_excel(all_responses, excel_file_path="diverse_questions_and_answers.xlsx"):
    excel_data = []

    for response in all_responses:
        original_id = response["id"]
        question_type = response["question_type"]
        original_question = response["original_question"]
        original_answer = response["original_answer"]

        for diverse in response["diverse_questions"]:
            excel_data.append({
                "ID": original_id,
                "Question Type": question_type,
                "Original Question": original_question,
                "Original Answer": original_answer,
                "Diverse Question": diverse["diverse_question"],
                "Diverse Answer": diverse["diverse_answer"]
            })

    # Convert to DataFrame
    df = pd.DataFrame(excel_data, columns=[
        "ID", "Question Type", "Original Question", "Original Answer",
        "Diverse Question", "Diverse Answer"
    ])

    # Sort by Question Type to group similar types together
    df.sort_values(by=["Question Type", "ID"], inplace=True)

    # Save to Excel
    df.to_excel(excel_file_path, index=False, engine="openpyxl")

    print(f"Data saved successfully to {excel_file_path}")
    return excel_file_path

# Call function
save_responses_to_excel(all_responses, "diverse_questions_and_answers_1000.xlsx")






import json
import uuid

# Define constants
BATCH_SIZE = 2
NUM_DIVERSE_QUESTIONS = 5  # Number of diverse questions per input

# Define a general prompt template for generating diverse question types
DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs across multiple question types.

For each input question-answer pair, generate up to {num_diverse_questions} distinct questions
ensuring that the answers remain consistent and relevant to the original answer provided.

### Question Types:
1. **Factual** - Direct and fact-based questions.
2. **Conceptual** - Questions that require understanding of principles or ideas.
3. **Inferential** - Questions that require drawing logical conclusions.
4. **Reasoning-Based** - Questions requiring analysis and logical reasoning.
5. **Procedural** - Questions related to step-by-step processes.

### Input Format:


### Instructions:
- Iterate through each input question-answer pair.
- Generate {num_diverse_questions} diverse questions, ensuring one for each type above.
- Maintain the original answer where applicable.
- **Do not include JSON in your response.**

### Output Format (JSON):


### Instructions:
- Iterate through each input question-answer pair.
- Generate {num_diverse_questions} diverse questions, ensuring one for each type above.
- Maintain the original answer where applicable.
- **Do not include JSON in your response.**

### Output Format (JSON):

"""

# Function to split input into batches of fixed size
def split_into_fixed_batches(input_list, batch_size=BATCH_SIZE):
    """Splits the input list into fixed-size batches."""
    for i in range(0, len(input_list), batch_size):
        yield input_list[i:i + batch_size]

# Function to generate diverse questions using an LLM
def generate_diverse_questions(input_data, llm_chain):
    all_responses = []
    batch_number = 1

    for batch in split_into_fixed_batches(input_data, BATCH_SIZE):
        print(f"Processing batch {batch_number} with {len(batch)} question-answer pairs...")

        # Convert batch input to JSON
        batch_input_json = json.dumps(batch)

        # Fill prompt template
        filled_prompt = DIVERSE_QUESTION_PROMPT.format(
            input_json=batch_input_json,
            num_diverse_questions=NUM_DIVERSE_QUESTIONS
        )

        # Predict diverse questions using the LLM
        response = llm_chain.predict(
            input_json=batch_input_json,
            num_diverse_questions=str(NUM_DIVERSE_QUESTIONS),
            stream=True
        )

        # Parse the response JSON
        try:
            response_json = json.loads(response)
            all_responses.extend(response_json)
        except json.JSONDecodeError as e:
            print(f"Failed to parse response for batch {batch_number}: {e}")

        batch_number += 1

    # Output the combined results
    print(json.dumps(all_responses, indent=4))
    return all_responses








The Predicted response lacks the structured format, step-by-step procedure, and key details from the Ground Truth (such as the interface name, image reference, and prerequisites). To improve accuracy, it should retain the procedural instructions and structured format while keeping the importance explanation as a supporting point.





The Predicted response is more detailed and structured but introduces unnecessary changes, omits a critical folder copying step, and shifts the scope. It needs fine-tuning to retain key original steps while keeping the added validation process








Title Difference

Ground Truth: No title mentioned.
Predicted: Includes a title "Preparing Raw Data for HKHA Annual Currency Risk Exposure Report", specifying a report type.
Scope & Purpose

Ground Truth: Focuses on preparing raw data, including copying a folder and downloading specific files.
Predicted: Focuses on data preparation for the HKHA Annual Currency Risk Exposure Report, making it more specific.
Prerequisites

Ground Truth: None specified.
Predicted: Adds "Access to MYSS" and "Raw data templates" as prerequisites.
Data Downloading Steps

Ground Truth: Focuses only on copying last year’s folder and downloading Pending FX and Open Trade file (Mapping: 13210).
Predicted: Expands the data list to include:
1-day adjustment (with/without accrual)
FX Pending Forward Activity
Currency Exposure
Open Trades, Tax Reclaim Receivable, Interest Receivable, Dividend Receivable, MBS Interest Receivable.
Data Review Process

Ground Truth: Does not mention reviewing data.
Predicted: Adds a "Review Raw Data" section, including checks for:
Date period accuracy
Fund completeness
Open trades, tax reclaim, interest income, and missing funds validation.
Structural Differences

Ground Truth: Follows a step-based procedural format.
Predicted: Uses bullet points for clarity and adds extra validation steps.
Missing Folder Copying Step

Ground Truth: Mentions copying the last year’s folder as Step 1.
Predicted: Completely omits this step.










import requests
import json
import pandas as pd

# Disable SSL warnings
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Constants
HOST = "https://api-uat.statestr.com"
AUTH_URL = f"{HOST}/oauth/accesstoken"
HALLUCINATION_URL = f"{HOST}/rai-obs-compute/v1/hallucination"
EVALUATIONS_URL = f"{HOST}/rai-obs-compute/v1/evaluations"

# Client credentials (Replace with actual values)
CLIENT_ID = "pwiRDNVzPTSUoFocQwQ5PzAJsEdS9DcU"
CLIENT_SECRET = "130QKQ9jjCI7bEmD"

def get_access_token():
    """Fetch the access token."""
    headers = {"Accept": "application/json"}
    data = {"grant_type": "client_credentials"}
    
    response = requests.post(
        AUTH_URL,
        headers=headers,
        data=data,
        auth=(CLIENT_ID, CLIENT_SECRET),
        verify=False
    )
    
    response_dict = response.json()
    return response_dict.get("access_token")

def call_hallucination_api(access_token, question, answer, context, threshold_score=0.5):
    """Call the hallucination API."""
    payload = json.dumps({
        "question": question,
        "answer": answer,
        "context": context,
        "threshold_score": threshold_score
    })
    
    headers = {
        "SSGenAIClient": CLIENT_ID,
        "SSGenAIAuth": CLIENT_SECRET,
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/json"
    }
    
    response = requests.post(
        HALLUCINATION_URL,
        headers=headers,
        data=payload,
        verify=False
    )
    
    return response.json()

def call_evaluations_api(access_token, response_data, context):
    """Call the evaluations API and return the results."""
    results = []
    
    headers = {
        "SSGenAIClient": CLIENT_ID,
        "SSGenAIAuth": CLIENT_SECRET,
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/json"
    }
    
    for qa in response_data:
        payload = json.dumps({
            "payload": {
                "clientId": "2xy7g&Brmi83t2v49",
                "question": qa["Question"],
                "answer": qa["Answer"],
                "context": [context],
                "groundTruth": [qa["Answer"]],
            },
            "metricNames": ["PRECISION_RECALL_F1"]
        })
        
        eval_response = requests.post(
            EVALUATIONS_URL,
            headers=headers,
            data=payload,
            verify=False
        ).json()
        
        # Extracting the results
        metrics = eval_response.get("metric_result", [{}])[0].get("Result", {})
        results.append({
            "Question": qa["Question"],
            "Answer": qa["Answer"],
            "context_precision": metrics.get("context_precision", 0.0),
            "context_recall": metrics.get("context_recall", 0.0),
            "context_f1": metrics.get("context_f1", 0.0)
        })
    
    return results

def save_results_to_excel(results, filename="evaluation_results.xlsx"):
    """Save the evaluation results to an Excel file."""
    df = pd.DataFrame(results)
    df.to_excel(filename, index=False)
    print(f"Results saved to {filename}")

# Sample response_data
response_data = [
    {"Question": "What is the identifier for the NCS stock recon SOP?", "Answer": "CSIP105P22-15071"},
    {"Question": "What is the effective date of the VIAC NAS stock recon SOP?", "Answer": "2024/10/30"},
    {"Question": "What is the next review date for the VIPIC NAS stock recon SOP?", "Answer": "2025/10/30"},
    {"Question": "Is the VFHC NAS stock recon SOP a corporate policy SOP?", "Answer": "No"}
]

# Sample context
context = "Global Delivery Standard Operating Procedure (SOP) Template"

# Run the workflow
access_token = get_access_token()
results = call_evaluations_api(access_token, response_data, context)
save_results_to_excel(results)








import requests
import json
import pandas as pd

# ====== STEP 1: GET ACCESS TOKEN ======
host = "https://api-uat.statestr.com"
auth_url = f"{host}/oauth/accesstoken"

headers = {"Accept": "application/json"}
data = {"grant_type": "client_credentials"}

# Client credentials (Replace with actual values)
client_id = "pwiRDNVzPTSUoFocQwQ5PzAJsEdS9DcU"
client_secret = "130QKQ9jjCI7bEmD"

response = requests.post(auth_url, headers=headers, data=data, auth=(client_id, client_secret), verify=False)
response_dict = response.json()

access_token = response_dict.get("access_token", "")

# ====== STEP 2: CALL HALLUCINATION API ======
hallucination_url = f"{host}/rai-obs-compute/v1/hallucination"

payload = json.dumps({
    "question": "When was State Street founded",
    "answer": "State Street was found in 1792",
    "context": ["State Street is a financial services company that provides investment management, research, trading, and servicing. It was found in 1792"],
    "threshold_score": 0.5,
})

headers = {
    "SSGenAIClient": client_id,
    "SSGenAIAuth": client_secret,
    "Authorization": f"Bearer {access_token}",
    "Content-Type": "application/json",
}

hal_response = requests.post(hallucination_url, headers=headers, data=payload, verify=False).json()
print("\nhal_response \n", hal_response)

# ====== STEP 3: CALL EVALUATIONS API ======
evaluations_url = f"{host}/rai-obs-compute/v1/evaluations"

response_data = [
    {"Question": "What is the identifier for the VFMC NAS stock recon SOP?", "Answer": "CSIP105P22-15071"},
    {"Question": "What is the effective date of the VFMC NAS stock recon SOP?", "Answer": "2024/10/30"},
    {"Question": "What is the next review date for the VFMC NAS stock recon SOP?", "Answer": "2025/10/30"},
    {"Question": "Is the VFHC NAS stock recon SOP a corporate policy SOP?", "Answer": "No"},
]

context = "Global Delivery Standard Operating Procedure (SOP) Template"

# Load previous data if file exists (appending results)
file_path = "evaluation_results.xlsx"
try:
    df_existing = pd.read_excel(file_path)
except FileNotFoundError:
    df_existing = pd.DataFrame(columns=['Question', 'Answer', 'Context Precision', 'Context Recall', 'Context F1'])

# Store new results
new_results = []

for qa in response_data:
    payload = json.dumps({
        "payload": {
            "clientId": "2xy7g&Brmi83t2v49",
            "question": qa["Question"],
            "answer": qa["Answer"],
            "context": [context],
            "groundTruth": [qa["Answer"]],
        },
        "metricNames": ["PRECISION_RECALL_F1"],
    })

    print("Payload is:", payload)

    eval_response = requests.post(evaluations_url, headers=headers, data=payload, verify=False).json()
    print("\neval_response \n", eval_response)

    # Extract evaluation results
    if eval_response.get("status") == "success" and eval_response.get("metric_result"):
        metric_result = eval_response["metric_result"][0]["Result"]
        new_results.append([
            qa["Question"],
            qa["Answer"],
            metric_result.get("context_precision", 0.0),
            metric_result.get("context_recall", 0.0),
            metric_result.get("context_f1", 0.0)
        ])

# Convert new results to DataFrame and append to existing data
df_new = pd.DataFrame(new_results, columns=['Question', 'Answer', 'Context Precision', 'Context Recall', 'Context F1'])
df_final = pd.concat([df_existing, df_new], ignore_index=True)

# Save to Excel
df_final.to_excel(file_path, index=False)

print(f"\nResults appended and saved to {file_path}")







import requests
import json
import pandas as pd

# ====== STEP 1: GET ACCESS TOKEN ======
host = "https://api-uat.statestr.com"
auth_url = f"{host}/oauth/accesstoken"

headers = {"Accept": "application/json"}
data = {"grant_type": "client_credentials"}

# Client credentials (Replace with actual values)
client_id = "pwiRDNVzPTSUoFocQwQ5PzAJsEdS9DcU"
client_secret = "130QKQ9jjCI7bEmD"

response = requests.post(auth_url, headers=headers, data=data, auth=(client_id, client_secret), verify=False)
response_dict = response.json()

access_token = response_dict.get("access_token", "")

# ====== STEP 2: CALL HALLUCINATION API ======
hallucination_url = f"{host}/rai-obs-compute/v1/hallucination"

payload = json.dumps({
    "question": "When was State Street founded",
    "answer": "State Street was found in 1792",
    "context": ["State Street is a financial services company that provides investment management, research, trading, and servicing. It was found in 1792"],
    "threshold_score": 0.5,
})

headers = {
    "SSGenAIClient": client_id,
    "SSGenAIAuth": client_secret,
    "Authorization": f"Bearer {access_token}",
    "Content-Type": "application/json",
}

hal_response = requests.post(hallucination_url, headers=headers, data=payload, verify=False).json()
print("\nhal_response \n", hal_response)

# ====== STEP 3: CALL EVALUATIONS API ======
evaluations_url = f"{host}/rai-obs-compute/v1/evaluations"

response_data = [
    {"Question": "What is the identifier for the VFMC NAS stock recon SOP?", "Answer": "CSIP105P22-15071"},
    {"Question": "What is the effective date of the VFMC NAS stock recon SOP?", "Answer": "2024/10/30"},
    {"Question": "What is the next review date for the VFMC NAS stock recon SOP?", "Answer": "2025/10/30"},
    {"Question": "Is the VFHC NAS stock recon SOP a corporate policy SOP?", "Answer": "No"},
]

context = "Global Delivery Standard Operating Procedure (SOP) Template"

results = []

for qa in response_data:
    payload = json.dumps({
        "payload": {
            "clientId": "2xy7g&Brmi83t2v49",
            "question": qa["Question"],
            "answer": qa["Answer"],
            "context": [context],
            "groundTruth": [qa["Answer"]],
        },
        "metricNames": ["PRECISION_RECALL_F1"],
    })

    print("Payload is:", payload)

    eval_response = requests.post(evaluations_url, headers=headers, data=payload, verify=False).json()
    print("\neval_response \n", eval_response)

    # Extract evaluation results
    if eval_response.get("status") == "success" and eval_response.get("metric_result"):
        metric_result = eval_response["metric_result"][0]["Result"]
        results.append([
            qa["Question"],
            qa["Answer"],
            metric_result.get("context_precision", 0.0),
            metric_result.get("context_recall", 0.0),
            metric_result.get("context_f1", 0.0)
        ])

# Create DataFrame
df = pd.DataFrame(results, columns=['Question', 'Answer', 'Context Precision', 'Context Recall', 'Context F1'])

# Save to Excel
file_path = "evaluation_results.xlsx"
df.to_excel(file_path, index=False)

print(f"\nResults saved to {file_path}")









import requests
import json

#====== STEP 1: GET ACCESS TOKEN ======
host = "https://api-uat.statestr.com"
auth_url = f"{host}/oauth/accesstoken"

headers = {'Accept': 'application/json'}
data = {"grant_type": "client_credentials"}

# Client credentials (Replace with actual values)
client_id = "pwiRDNVzPTSUoFocQwQ5PzAJsEdS9DcU"
client_secret = "130QKQ9jjCI7bEmD"

response = requests.post(auth_url, headers=headers, data=data, auth=(client_id, client_secret), verify=False)

if response.status_code != 200:
    print(f"Failed to get access token: {response.text}")
    exit()

response_dict = response.json()
access_token = response_dict.get("access_token")

if not access_token:
    print("Access token not found in response.")
    exit()

print("Access token retrieved successfully.")

#====== STEP 2: CALL HALLUCINATION API ======
hallucination_url = f"{host}/rai-obs-compute/v1/hallucination"

response_data = [
    {'Question_type': 'Factual',
     'Question': 'What is the name of the team responsible for the Global Delivery Standard Operating Procedure?',
     'Answer': 'Global Delivery Business Risk Management Team'},
    {'Question_type': 'Factual',
     'Question': "What is the identifier for the VFMC NAS stock recon SOP?",
     'Answer': 'CS1P10SP22-15071'},
    {'Question_type': 'Factual',
     'Question': 'What is the effective date of the VFMC NAS stock recon SOP?',
     'Answer': '2024/10/30'},
    {'Question_type': 'Factual',
     'Question': "What is the next review date for the VFMC NAS stock recon SOP?",
     'Answer': '2025/10/30'},
    {'Question_type': 'Factual',
     'Question': 'Is the VFMC NAS stock recon SOP a corporate policy SOP?',
     'Answer': 'No'}
]

context = "# Global Delivery Standard Operating Procedure (SOP) Template"

# Headers for API Requests
headers = {
    'SSGenAIClient': client_id,
    'SSGenAIAuth': client_secret,
    'Authorization': f"Bearer {access_token}",  # Ensure correct formatting
    'Content-Type': 'application/json'
}

#====== STEP 3: CALL EVALUATIONS API ======
evaluations_url = f"{host}/rai-obs-compute/v1/evaluations"

for qa in response_data:
    if "Answer" in qa:  # Ensure ground truth is provided for evaluation
        try:
            # Creating payload
            payload = {
                "payload": {
                    "clientId": "2xy7gs8rmi83t2vu9",
                    "question": qa["Question"],
                    "answer": qa["Answer"],
                    "context": context,
                    "groundTruth": [qa["Answer"]]  # Ensure it's a list
                },
                "metricNames": ["PRECISION_RECALL_F1"]
            }

            # Print payload for debugging
            print("\nPayload being sent:")
            print(json.dumps(payload, indent=4))  # Pretty-print payload

            # Making the API request
            response = requests.post(evaluations_url, headers=headers, json=payload, verify=False)

            # Debugging response
            print("\nRaw Response:")
            print(response.text)

            # Attempt JSON decoding
            eval_response = response.json()
            print("\nEvaluation Response:", eval_response)

        except json.JSONDecodeError:
            print("\nERROR: Unable to decode JSON response. Response might not be valid JSON.")
            print("Raw Response:", response.text)

        except Exception as e:
            print(f"\nUnexpected Error: {str(e)}")








import json
import requests

# Sample QA Data
response_data = [
    {'Question_type': 'Factual', 'Question': 'What is the name of the team responsible for the Global Delivery Standard Operating Procedure?', 'Answer': 'Global Delivery Business Risk Management Team'},
    {'Question_type': 'Factual', 'Question': "What is the identifier for the VFMC NAS stock recon SOP?", 'Answer': 'CS1P10SP22-15071'},
    {'Question_type': 'Factual', 'Question': 'What is the effective date of the VFMC NAS stock recon SOP?', 'Answer': '2024/10/30'},
    {'Question_type': 'Factual', 'Question': "What is the next review date for the VFMC NAS stock recon SOP?", 'Answer': '2025/10/30'},
    {'Question_type': 'Factual', 'Question': 'Is the VFMC NAS stock recon SOP a corporate policy SOP?', 'Answer': 'No'}
]

context = "# Global Delivery Standard Operating Procedure (SOP) Template"

# API Endpoint
evaluations_url = "https://api-uat.statestr.com/rai-obs-compute/v1/evaluations"

# Client credentials (Replace with actual values)
access_token = "your_access_token_here"
client_id = "your_client_id_here"
client_secret = "your_client_secret_here"

# Headers
headers = {
    'SSGenAIClient': client_id,
    'SSGenAIAuth': client_secret,
    'Authorization': f"Bearer {access_token}",  # Ensure correct formatting
    'Content-Type': 'application/json'
}

# Iterating through response_data
for qa in response_data:
    if "Answer" in qa:  # Ensure ground truth is provided for evaluation
        try:
            # Creating payload
            payload = {
                "payload": {
                    "clientId": "2xy7gs8rmi83t2vu9",
                    "question": qa["Question"],
                    "answer": qa["Answer"],
                    "context": context,
                    "groundTruth": [qa["Answer"]]  # Ensure it's a list
                },
                "metricNames": ["PRECISION_RECALL_F1"]
            }

            # Print payload for debugging
            print("\nPayload being sent:")
            print(json.dumps(payload, indent=4))  # Pretty-print payload

            # Making the API request
            response = requests.post(evaluations_url, headers=headers, json=payload, verify=False)

            # Debugging response
            print("\nRaw Response:")
            print(response.text)

            # Attempt JSON decoding
            eval_response = response.json()
            print("\nEvaluation Response:", eval_response)

        except json.JSONDecodeError:
            print("\nERROR: Unable to decode JSON response. Response might not be valid JSON.")
            print("Raw Response:", response.text)

        except Exception as e:
            print(f"\nUnexpected Error: {str(e)}")







import requests
import json

#====== STEP 1: GET ACCESS TOKEN ======
host = "https://api-uat.statestr.com"
auth_url = f"{host}/oauth/accesstoken"

headers = {'Accept': 'application/json'}
data = {"grant_type": "client_credentials"}

# Client credentials (Replace with actual values)
client_id = "pwiRDNVzPTSUoFocQwQ5PzAJsEdS9DcU"
client_secret = "130QKQ9jjCI7bEmD"

response = requests.post(auth_url, headers=headers, data=data, auth=(client_id, client_secret), verify=False)

if response.status_code != 200:
    print(f"Failed to get access token: {response.text}")
    exit()

response_dict = response.json()
access_token = response_dict.get("access_token")

if not access_token:
    print("Access token not found in response.")
    exit()

print("Access token retrieved successfully.")

#====== STEP 2: CALL HALLUCINATION API ======
hallucination_url = f"{host}/rai-obs-compute/v1/hallucination"

response_data = [
    {'Question_type': 'Factual',
     'Question': 'What is the name of the team responsible for the Global Delivery Standard Operating Procedure?',
     'Answer': 'Global Delivery Business Risk Management Team'},
    {'Question_type': 'Factual',
     'Question': "What is the identifier for the VFMC NAS stock recon SOP?",
     'Answer': 'CS1P10SP22-15071'},
    {'Question_type': 'Factual',
     'Question': 'What is the effective date of the VFMC NAS stock recon SOP?',
     'Answer': '2024/10/30'},
    {'Question_type': 'Factual',
     'Question': "What is the next review date for the VFMC NAS stock recon SOP?",
     'Answer': '2025/10/30'},
    {'Question_type': 'Factual',
     'Question': 'Is the VFMC NAS stock recon SOP a corporate policy SOP?',
     'Answer': 'No'}
]

context = "# Global Delivery Standard Operating Procedure (SOP) Template"

for qa in response_data:
    payload = json.dumps({
        "question": qa["Question"],
        "answer": qa["Answer"],
        "context": context,
        "threshold_score": qa.get("threshold_score", 0.5)
    })

    headers = {
        'SSGenAIClient': client_id,
        'SSGenAIAuth': client_secret,
        'Authorization': f"Bearer {access_token}",  # Fixed formatting
        'Content-Type': 'application/json'
    }

    hal_response = requests.post(hallucination_url, headers=headers, data=payload, verify=False).json()
    print("\nHallucination Response:\n", hal_response)

#====== STEP 3: CALL EVALUATIONS API ======
evaluations_url = f"{host}/rai-obs-compute/v1/evaluations"

for qa in response_data:
    if "Answer" in qa:  # Ensure ground truth is provided for evaluation
        payload = json.dumps({
            "payload": {
                "clientId": "2xy7gs8rmi83t2vu9",
                "question": qa["Question"],
                "answer": qa["Answer"],
                "context": context,
                "groundTruth": [qa["Answer"]]  # Should be a list
            },
            "metricNames": ["PRECISION_RECALL_F1"]
        })

        headers = {
            'SSGenAIClient': client_id,
            'SSGenAIAuth': client_secret,
            'Authorization': f"Bearer {access_token}",  # Fixed formatting
            'Content-Type': 'application/json'
        }

        eval_response = requests.post(evaluations_url, headers=headers, data=payload, verify=False).json()
        print("\nEvaluation Response:\n", eval_response)






import requests
import json

#====== STEP 1: GET ACCESS TOKEN ======
host = "https://api-uat.statestr.com"
auth_url = f"{host}/oauth/accesstoken"

headers = {'Accept': 'application/json'}
data = {"grant_type": "client_credentials"}

# Client credentials (Replace with actual values)
client_id = "pwiRDNVzPTSUoFocQwQ5PzAJsEdS9DcU"
client_secret = "130QKQ9jjCI7bEmD"

response = requests.post(auth_url, headers=headers, data=data, auth=(client_id, client_secret), verify=False)

if response.status_code != 200:
    print(f"Failed to get access token: {response.text}")
    exit()

response_dict = response.json()
access_token = response_dict.get("access_token")

# Ensure token is retrieved
if not access_token:
    print("Access token not found in response.")
    exit()

print("Access token retrieved successfully.")

#====== STEP 2: CALL HALLUCINATION API ======
hallucination_url = f"{host}/rai-obs-compute/v1/hallucination"

response_data = [
    {'Question_type': 'Factual',
     'Question': 'What is the name of the team responsible for the Global Delivery Standard Operating Procedure?',
     'Answer': 'Global Delivery Business Risk Management Team'},
    {'Question_type': 'Factual',
     'Question': "What is the identifier for the VFMC NAS stock recon SOP?",
     'Answer': 'CS1P10SP22-15071'},
    {'Question_type': 'Factual',
     'Question': 'What is the effective date of the VFMC NAS stock recon SOP?',
     'Answer': '2024/10/30'},
    {'Question_type': 'Factual',
     'Question': "What is the next review date for the VFMC NAS stock recon SOP?",
     'Answer': '2025/10/30'},
    {'Question_type': 'Factual',
     'Question': 'Is the VFMC NAS stock recon SOP a corporate policy SOP?',
     'Answer': 'No'}
]

context = "# Global Delivery Standard Operating Procedure (SOP) Template"

for qa in response_data:
    payload = json.dumps({
        "question": qa["Question"],
        "answer": qa["Answer"],
        "context": context,
        "threshold_score": qa.get("threshold_score", 0.5)  # Fixing variable reference
    })

    headers = {
        'SSGenAIClient': client_id,
        'SSGenAIAuth': client_secret,
        'Authorization': f"Bearer {access_token}",  # Fixed syntax
        'Content-Type': 'application/json'
    }

    hal_response = requests.post(hallucination_url, headers=headers, data=payload, verify=False).json()
    print("\nHallucination Response:\n", hal_response)

#====== STEP 3: CALL EVALUATIONS API ======
evaluations_url = f"{host}/rai-obs-compute/v1/evaluations"

for qa in response_data:
    if "Answer" in qa:  # Ensure ground truth is provided for evaluation
        payload = json.dumps({
            "payload": {
                "clientId": "2xy7gs8rmi83t2vu9",
                "question": qa["Question"],
                "answer": qa["Answer"],
                "context": context,
                "groundTruth": qa["Answer"]
            },
            "metricNames": ["PRECISION_RECALL_F1"]
        })

        headers = {
            'SSGenAIClient': client_id,
            'SSGenAIAuth': client_secret,
            'Authorization': f"Bearer {access_token}",  # Fixed syntax
            'Content-Type': 'application/json'
        }

        eval_response = requests.post(evaluations_url, headers=headers, data=payload, verify=False).json()
        print("\nEvaluation Response:\n", eval_response)









for qa in qa_data:
    payload = json.dumps({
        "question": qa["question"],
        "answer": qa["answer"],
        "context": qa["context"],
        "threshold_score": qa.get("threshold_score", 0.5)
    })

for qa in qa_data:
    if "groundTruth" in qa:  # Ensure ground truth is provided for evaluation
        payload = json.dumps({
            "payload": {
                "clientId": "2xy7gs8rmi83t2vu9",
                "question": qa["question"],
                "answer": qa["answer"],
                "context": qa["context"],
                "groundTruth": qa["groundTruth"]
            },
            "metricNames": ["PRECISION_RECALL_F1"]
        })




import requests
import json

# ====== STEP 1: GET ACCESS TOKEN ======
host = "https://api-dev.statestr.com"  # Replace with correct API host
auth_url = f"{host}/oauth/accesstoken"

# Client credentials (Replace with actual values)
client_id = "YOUR_CLIENT_ID"
client_secret = "YOUR_CLIENT_SECRET"

# Headers for authentication request
headers = {'Accept': 'application/json'}

# Data for OAuth authentication
data = {"grant_type": "client_credentials"}

# Request access token
auth_response = requests.post(auth_url, headers=headers, data=data, auth=(client_id, client_secret), verify=False)

# Check if authentication was successful
if auth_response.status_code == 200:
    auth_response_json = auth_response.json()
    access_token = auth_response_json.get("access_token")
    print("\n✅ Access Token Retrieved Successfully:", access_token)
else:
    print("\n❌ Error Getting Access Token:", auth_response.text)
    exit()  # Stop execution if authentication fails

# ====== STEP 2: CALL HALLUCINATION API ======
hallucination_url = f"{host}/rai-obs-compute/v1/hallucination"

# Prepare payload
hallucination_payload = json.dumps({
    "question": "When was State Street founded?",
    "answer": "State Street was founded in 1792",
    "context": [
        "State Street is a financial services company that provides investment management, research, trading, and servicing. It was founded in 1792."
    ],
    "threshold_score": 0.5
})

# Headers including access token
headers = {
    'SSGenAIClient': client_id,
    'SSGenAIAuth': client_secret,
    'Authorization': f"Bearer {access_token}",
    'Content-Type': 'application/json'
}

# Send request
hallucination_response = requests.post(hallucination_url, headers=headers, data=hallucination_payload, verify=False)

# Print response
print("\n🎯 Hallucination API Response:\n", hallucination_response.json())

# ====== STEP 3: CALL EVALUATIONS API ======
evaluations_url = f"{host}/rai-obs-compute/v1/evaluations"

# Prepare payload
evaluations_payload = json.dumps({
    "payload": {
        "clientId": "2xy7gs8rm183t2vu9",
        "question": "Which is the largest city in France?",
        "answer": "Paris",
        "context": [
            "Lyon is a major city in France.",
            "Paris is the capital of France."
        ],
        "groundTruth": [
            "Paris is the largest city in France."
        ]
    },
    "metricNames": [
        "PRECISION_RECALL_F1",
        "RANKED_RETRIEVAL"
    ]
})

# Send request
evaluations_response = requests.post(evaluations_url, headers=headers, data=evaluations_payload, verify=False)

# Print response
print("\n📊 Evaluations API Response:\n", evaluations_response.json())











import pandas as pd
from tqdm import tqdm

# List to store all results
results = []

# Define a range of hyperparameters for tuning
temperature_values = [0.6, 0.7, 0.8]
top_k_values = [50, 70, 100]
top_p_values = [0.7, 0.8, 0.9]
repetition_penalty_values = [1.2, 1.35, 1.5]

# Iterate through all hyperparameter combinations
for temp in temperature_values:
    for top_k in top_k_values:
        for top_p in top_p_values:
            for rep_penalty in repetition_penalty_values:
                print(f"Testing with Temperature={temp}, Top-k={top_k}, Top-p={top_p}, Repetition Penalty={rep_penalty}")
                
                # Process each row in the test dataset
                for index, row in tqdm(df_test.iterrows(), total=len(df_test)):
                    prompt = row['inputs']
                    actual_answer = row['actual_answer']  # Assuming 'actual_answer' column exists in df_test
                    
                    # Generate the output using the model
                    outputs = pipe(
                        prompt, 
                        max_new_tokens=400,  
                        do_sample=True, 
                        return_full_text=False, 
                        temperature=temp, 
                        top_k=top_k,
                        top_p=top_p, 
                        repetition_penalty=rep_penalty,
                        eos_token_id=pipe.tokenizer.eos_token_id, 
                        pad_token_id=pipe.tokenizer.pad_token_id
                    )
                    
                    predicted_answer = outputs[0]['generated_text']
                    
                    # Store the result with all relevant information
                    results.append({
                        'index': index,
                        'prompt': prompt,
                        'actual_answer': actual_answer,
                        'predicted_answer': predicted_answer,
                        'temperature': temp,
                        'top_k': top_k,
                        'top_p': top_p,
                        'repetition_penalty': rep_penalty
                    })

# Convert the results to a DataFrame for easy analysis
results_df = pd.DataFrame(results)

# Save the results to a CSV file
results_df.to_csv('finetuned_outputs_with_actual_answers.csv', index=False)
print("All results saved to 'finetuned_outputs_with_actual_answers.csv'")









from tqdm import tqdm
from sklearn.model_selection import GridSearchCV
import numpy as np

# Define a custom scorer for generative model evaluation
def custom_score(predictions, prompts):
    # Example: Scoring based on length of generated text as a placeholder
    # Replace this with a better heuristic or manual evaluation if needed
    scores = [len(pred.split()) for pred in predictions]
    return np.mean(scores)  # Example: Using average length as the score

# Define a scikit-learn compatible model wrapper
class TextGenerationModel:
    def __init__(self, pipe, max_new_tokens=300, temperature=0.7, top_k=50, top_p=0.9, repetition_penalty=1.5):
        self.pipe = pipe
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_k = top_k
        self.top_p = top_p
        self.repetition_penalty = repetition_penalty

    def predict(self, X):
        predictions = []
        for prompt in X:
            outputs = self.pipe(prompt, 
                                max_new_tokens=self.max_new_tokens, 
                                do_sample=True, 
                                return_full_text=False, 
                                temperature=self.temperature, 
                                top_k=self.top_k, 
                                top_p=self.top_p, 
                                repetition_penalty=self.repetition_penalty, 
                                eos_token_id=self.pipe.tokenizer.eos_token_id, 
                                pad_token_id=self.pipe.tokenizer.pad_token_id,
                                early_stopping=True)
            
            predicted_answer = outputs[0]['generated_text']
            
            # Optional: Post-processing to remove repeated lines
            unique_lines = list(dict.fromkeys(predicted_answer.split('\n')))
            predicted_answer = '\n'.join(unique_lines)
            
            predictions.append(predicted_answer)
        return predictions

    def set_params(self, **params):
        for param, value in params.items():
            setattr(self, param, value)
        return self

    def get_params(self, deep=True):
        return {
            'max_new_tokens': self.max_new_tokens,
            'temperature': self.temperature,
            'top_k': self.top_k,
            'top_p': self.top_p,
            'repetition_penalty': self.repetition_penalty,
            'pipe': self.pipe
        }

    def fit(self, X, y=None):
        return self

    def score(self, X, y=None):
        predictions = self.predict(X)
        return custom_score(predictions, X)

model = TextGenerationModel(pipe)

# Define the grid of hyperparameters
param_grid = {
    'max_new_tokens': [100, 300, 600],
    'temperature': [0.3, 0.7, 1.0],
    'top_k': [10, 50, 100],
    'top_p': [0.7, 0.9, 1.0],
    'repetition_penalty': [1.0, 1.5, 2.0]
}

# Perform Grid Search with the custom scoring function
grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=3, verbose=2)
grid_search.fit(df_test['inputs'])

print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)









from tqdm import tqdm
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score, accuracy_score

# Define a scikit-learn compatible model wrapper
class TextGenerationModel:
    def __init__(self, pipe):
        self.pipe = pipe
        self.max_new_tokens = 300
        self.temperature = 0.7
        self.top_k = 50
        self.top_p = 0.9
        self.repetition_penalty = 1.5

    def predict(self, X):
        predictions = []
        for prompt in X:
            outputs = self.pipe(prompt, 
                                max_new_tokens=self.max_new_tokens, 
                                do_sample=True, 
                                return_full_text=False, 
                                temperature=self.temperature, 
                                top_k=self.top_k, 
                                top_p=self.top_p, 
                                repetition_penalty=self.repetition_penalty, 
                                eos_token_id=self.pipe.tokenizer.eos_token_id, 
                                pad_token_id=self.pipe.tokenizer.pad_token_id,
                                early_stopping=True)
            
            predicted_answer = outputs[0]['generated_text']
            
            # Post-processing to remove repeated lines
            unique_lines = list(dict.fromkeys(predicted_answer.split('\n')))
            predicted_answer = '\n'.join(unique_lines)
            
            predictions.append(predicted_answer)
        return predictions

    def set_params(self, **params):
        for param, value in params.items():
            setattr(self, param, value)
        return self

    def fit(self, X, y):
        return self

    def score(self, X, y):
        predictions = self.predict(X)
        return f1_score(y, predictions, average='weighted')

model = TextGenerationModel(pipe)

# Define the grid of hyperparameters
param_grid = {
    'max_new_tokens': [100, 300, 600],
    'temperature': [0.3, 0.7, 1.0],
    'top_k': [10, 50, 100],
    'top_p': [0.7, 0.9, 1.0],
    'repetition_penalty': [1.0, 1.5, 2.0]
}

# Perform Grid Search with F1 Score as the metric
grid_search = GridSearchCV(model, param_grid, scoring='f1_weighted', cv=3, verbose=2)
grid_search.fit(df_test['inputs'], df_test['outputs'])

print("Best Parameters:", grid_search.best_params_)
print("Best F1 Score:", grid_search.best_score_)








# Check for potential issues in the predicted answers
for i, pred in enumerate(finetuned_output):
    if not isinstance(pred, str) or pred.strip() == "":
        print(f"Invalid output at index {i}: {pred}")

# Calculate accuracy and F1 score
accuracy = accuracy_score(true_answers, finetuned_output)
f1 = f1_score(true_answers, finetuned_output, average='weighted')

print(f"Model Accuracy: {accuracy:.2f}")
print(f"F1 Score: {f1:.2f}")








import json
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score

finetuned_output = []
true_answers = df_test['outputs']  # Assuming 'outputs' column has the structured ground truth

for _, row in tqdm(df_test.iterrows(), total=len(df_test)):
    prompt = row['inputs']
    outputs = pipe(prompt, 
                   max_new_tokens=600, 
                   do_sample=True, 
                   return_full_text=False, 
                   temperature=0.3, 
                   top_k=20, 
                   top_p=0.8, 
                   repetition_penalty=1.20, 
                   eos_token_id=pipe.tokenizer.eos_token_id, 
                   pad_token_id=pipe.tokenizer.pad_token_id)
    
    predicted_answer = outputs[0]['generated_text']
    finetuned_output.append(predicted_answer)

# Parse JSON if your outputs are structured as JSON strings
parsed_predicted = [json.loads(pred) if pred else {} for pred in finetuned_output]
parsed_true = [json.loads(true) if true else {} for true in true_answers]

# Extract the answers to compare
predicted_answers = [item.get('Answer', '') for instance in parsed_predicted for item in instance.get('instances', [])]
true_answers = [item.get('Answer', '') for instance in parsed_true for item in instance.get('instances', [])]

# Calculate accuracy
accuracy = accuracy_score(true_answers, predicted_answers)
f1 = f1_score(true_answers, predicted_answers, average='weighted')

print(f"Model Accuracy: {accuracy:.2f}")
print(f"F1 Score: {f1:.2f}")












from tqdm import tqdm
from itertools import product

# Define parameter ranges for grid search
temperature_values = [0.3, 0.5, 0.7]
top_k_values = [20, 50, 100]
top_p_values = [0.8, 0.9, 0.95]
repetition_penalty_values = [1.0, 1.2, 1.5]
max_new_tokens_values = [200, 400, 600]

best_score = 0
best_params = {}

# Heuristic function to score generated outputs without ground truth
def heuristic_score(predicted_answer):
    score = len(predicted_answer)  # Reward longer responses
    if "expected_keyword" in predicted_answer:
        score += 10
    if predicted_answer.endswith('.'):
        score += 5  # Reward well-formed sentences
    return score

# Perform grid search over all parameter combinations
for temperature, top_k, top_p, repetition_penalty, max_new_tokens in product(
    temperature_values, top_k_values, top_p_values, repetition_penalty_values, max_new_tokens_values
):
    finetuned_output = []
    total_score = 0

    # Iterate through the test dataset
    for _, row in tqdm(df_test.iterrows(), total=len(df_test)):
        prompt = row['inputs']
        
        outputs = pipe(prompt, 
                       max_new_tokens=max_new_tokens, 
                       do_sample=True, 
                       return_full_text=False, 
                       temperature=temperature,
                       top_k=top_k,
                       top_p=top_p, 
                       repetition_penalty=repetition_penalty, 
                       eos_token_id=pipe.tokenizer.eos_token_id, 
                       pad_token_id=pipe.tokenizer.pad_token_id)

        predicted_answer = outputs[0]['generated_text'].strip()
        finetuned_output.append(predicted_answer)

        # Evaluate using heuristic score
        total_score += heuristic_score(predicted_answer)

    # Calculate average heuristic score for the parameter set
    avg_score = total_score / len(finetuned_output)
    
    print(f"Avg Heuristic Score: {avg_score:.4f} | Params: temp={temperature}, top_k={top_k}, top_p={top_p}, "
          f"rep_pen={repetition_penalty}, max_tokens={max_new_tokens}")

    # Store the best parameters if the score improves
    if avg_score > best_score:
        best_score = avg_score
        best_params = {
            'temperature': temperature,
            'top_k': top_k,
            'top_p': top_p,
            'repetition_penalty': repetition_penalty,
            'max_new_tokens': max_new_tokens
        }

print(f"Best Heuristic Score: {best_score:.2f}")
print("Best Parameters:", best_params)










Hi  Ipsita,
I hope you’re doing well.
I wish to know if there are any updates on Kiran Babu M's selection process. If possible, please share any progress at your convenience.
 I wanted to take a moment to recommend Kiran Babu M for a potential opportunity at INFOSYS. I have known and worked with him around 3 years, and I strongly believe he would be a valuable addition to our organization.
I believe Kiran skills and experience would be a great asset to our organization.








import json

def read_json_file(file_path: str) -> list:
    """
    Reads a JSON file and returns its content as a list of dictionaries.
    
    :param file_path: The path to the JSON file.
    :return: The data from the JSON file as a list of dictionaries.
    """
    try:
        with open(file_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)
            print(f"Successfully read {len(data)} records from {file_path}")
            return data
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")

# Example usage
file_path = "VFMC_procedural_data.json"
data = read_json_file(file_path)

# Display the first few records
if data:
    for item in data[:5]:
        print(item)








import pandas as pd
import json

def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    """
    Converts an Excel file with procedural questions and answers into a JSON file.
    
    :param file_path: The path to the input Excel file.
    :param output_json_path: The path where the JSON file will be saved.
    :return: The path to the saved JSON file.
    """
    # Load the Excel data into a DataFrame
    df = pd.read_excel(file_path, dtype=str)
    
    # Replace NaN values with empty strings to avoid JSON serialization issues
    df = df.fillna("")

    # Create a list of dictionaries from the DataFrame
    json_data = []
    for _, row in df.iterrows():
        json_data.append({
            "Question_type": row.get("Question Type", ""),
            "Question": row.get("Question", ""),
            "Answer": row.get("Answer", ""),
            "Source File": row.get("Source File", "")
        })

    # Save the JSON data to a file
    with open(output_json_path, "w", encoding="utf-8") as json_file:
        json.dump(json_data, json_file, indent=4, ensure_ascii=False)
    
    print(f"Data successfully converted to JSON and saved to {output_json_path}")
    return output_json_path

# Example usage
file_path = "Resubmit_1021_VFMC_Accounting_Open_Trade_Check_SOP_questions.xlsx"
output_json_path = "VFMC_procedural_data.json"

convert_excel_to_json(file_path, output_json_path)










import pandas as pd
import json

def convert_excel_to_json(excel_file_path, output_json_path="output_data.json"):
    """
    Converts an Excel file with procedural questions and answers into a JSON file.
    
    :param excel_file_path: The path to the input Excel file.
    :param output_json_path: The path to save the output JSON file.
    :return: The path to the saved JSON file.
    """
    # Load the Excel data into a DataFrame
    df = pd.read_excel(excel_file_path, dtype=str)
    
    # Replace NaN values with empty strings
    df = df.fillna("")

    # Create a list of dictionaries from the DataFrame
    json_data = []
    for _, row in df.iterrows():
        json_data.append({
            "Question_type": row.get("Question Type", ""),
            "Question": row.get("Question", ""),
            "Answer": row.get("Answer", ""),
            "Source File": row.get("Source File", "")
        })

    # Save the JSON data to a file
    with open(output_json_path, "w", encoding="utf-8") as json_file:
        json.dump(json_data, json_file, indent=4, ensure_ascii=False)
    
    print(f"Data successfully converted to JSON and saved to {output_json_path}")
    return output_json_path

# Example usage
convert_excel_to_json("VFMC_Accounting_Procedures.xlsx", "VFMC_procedural_data.json")










import pandas as pd

def save_responses_to_excel(all_responses, excel_file_path="diverse_questions_and_answers.xlsx"):
    """
    Saves diverse questions and answers to an Excel file.
    
    :param all_responses: List of dictionaries containing diverse question-answer pairs.
    :param excel_file_path: The path where the Excel file will be saved.
    :return: The path to the saved Excel file.
    """
    # Prepare data for the Excel file
    excel_data = []

    for response in all_responses:
        original_id = response["id"]
        original_question = response["original_question"]
        original_answer = response["original_answer"]
        
        for diverse in response["diverse_questions"]:
            excel_data.append({
                "ID": original_id,
                "Original Question": original_question,
                "Original Answer": original_answer,
                "Diverse Question": diverse["diverse_question"],
                "Diverse Answer": diverse["diverse_answer"]
            })

    # Convert to a DataFrame
    df = pd.DataFrame(excel_data)

    # Save the DataFrame to an Excel file
    df.to_excel(excel_file_path, index=False)

    print(f"Data saved successfully to {excel_file_path}")
    return excel_file_path

# Example usage
all_responses = [
    {
        "id": 1,
        "original_question": "Sample question 1",
        "original_answer": "Sample answer 1",
        "diverse_questions": [
            {
                "diverse_question": "What is the explanation for Sample question 1?",
                "diverse_answer": "Sample answer 1"
            },
            {
                "diverse_question": "How would you describe Sample question 1?",
                "diverse_answer": "Sample answer 1"
            }
        ]
    },
    {
        "id": 2,
        "original_question": "Sample question 2",
        "original_answer": "Sample answer 2",
        "diverse_questions": [
            {
                "diverse_question": "What are the key points of Sample question 2?",
                "diverse_answer": "Sample answer 2"
            },
            {
                "diverse_question": "Can you elaborate on Sample question 2?",
                "diverse_answer": "Sample answer 2"
            }
        ]
    }
]

# Call the function to save the responses to an Excel file
save_responses_to_excel(all_responses, "diverse_questions_and_answers.xlsx")











import pandas as pd

# Example response data (replace this with your actual response data)
all_responses = [
    {
        "id": 1,
        "original_question": "Sample question 1",
        "original_answer": "Sample answer 1",
        "diverse_questions": [
            {
                "diverse_question": "What is the explanation for Sample question 1?",
                "diverse_answer": "Sample answer 1"
            },
            {
                "diverse_question": "How would you describe Sample question 1?",
                "diverse_answer": "Sample answer 1"
            }
        ]
    },
    {
        "id": 2,
        "original_question": "Sample question 2",
        "original_answer": "Sample answer 2",
        "diverse_questions": [
            {
                "diverse_question": "What are the key points of Sample question 2?",
                "diverse_answer": "Sample answer 2"
            },
            {
                "diverse_question": "Can you elaborate on Sample question 2?",
                "diverse_answer": "Sample answer 2"
            }
        ]
    }
]

# Prepare data for the Excel file
excel_data = []

for response in all_responses:
    original_id = response["id"]
    original_question = response["original_question"]
    original_answer = response["original_answer"]
    
    for diverse in response["diverse_questions"]:
        excel_data.append({
            "ID": original_id,
            "Original Question": original_question,
            "Original Answer": original_answer,
            "Diverse Question": diverse["diverse_question"],
            "Diverse Answer": diverse["diverse_answer"]
        })

# Convert to a DataFrame
df = pd.DataFrame(excel_data)

# Save the DataFrame to an Excel file
excel_file_path = "diverse_questions_and_answers.xlsx"
df.to_excel(excel_file_path, index=False)

print(f"Data saved successfully to {excel_file_path}")












from langchain import PromptTemplate, LLMChain
import json

# Define the fixed batch size
BATCH_SIZE = 3
num_diverse_questions = 5  # Number of diverse questions per input

# Define the updated diverse procedural prompt template
DIVERSE_PROCEDURAL_PROMPT = """
You are tasked to generate diverse procedural question-answer pairs.
For each procedural question-answer pair provided below, generate up to {num_diverse_questions} distinct procedural questions
while ensuring that the answers remain consistent and relevant to the original answer provided.

Input:
{input_json}

Instructions:
- Iterate through each input question-answer pair.
- Generate {num_diverse_questions} diverse procedural questions for each input question.
- The output should contain the original question and answer along with the diverse questions and answers.
- Ensure the output is in the following JSON format:

[
    {{
        "id": <unique_id>,
        "original_question": "Original Question",
        "original_answer": "Original Answer",
        "diverse_questions": [
            {{
                "diverse_question": "Diverse Question 1",
                "diverse_answer": "Original Answer"
            }},
            {{
                "diverse_question": "Diverse Question 2",
                "diverse_answer": "Original Answer"
            }},
            ...
        ]
    }},
    ...
]
"""

# Example input with 30 procedural question-answer pairs
input_procedural_qa_list = [
    {
        "Question_type": "Procedural",
        "Question": f"Sample question {i}",
        "Answer": f"Sample answer {i}"
    }
    for i in range(1, 31)
]

# Function to split input into batches of exactly 3 items
def split_into_fixed_batches(input_list, batch_size=BATCH_SIZE):
    """Splits the input list into fixed-size batches."""
    for i in range(0, len(input_list), batch_size):
        yield input_list[i:i + batch_size]

# Fill the prompt template
prompt = PromptTemplate(
    input_variables=["input_json", "num_diverse_questions"],
    template=DIVERSE_PROCEDURAL_PROMPT
)

# Create the LLM chain
llm_chain = LLMChain(llm=llm, prompt=prompt)

# Collect results from all batches
all_responses = []
batch_number = 1

for batch in split_into_fixed_batches(input_procedural_qa_list, BATCH_SIZE):
    print(f"Processing batch {batch_number} with {len(batch)} question-answer pairs...")
    
    # Prepare the input as a JSON string
    batch_input_json = json.dumps(batch)

    # Generate the filled prompt
    fill_prompt = prompt.format(
        input_json=batch_input_json,
        num_diverse_questions=num_diverse_questions
    )

    # Predict diverse questions using the LLM
    response = llm_chain.predict(
        input_json=batch_input_json,
        num_diverse_questions=str(num_diverse_questions),
        stream=True
    )

    # Parse the response as JSON and add to the results
    try:
        response_json = json.loads(response)
        all_responses.extend(response_json)
    except json.JSONDecodeError as e:
        print(f"Failed to parse response for batch {batch_number}: {e}")
    
    batch_number += 1

# Output the combined results
print(json.dumps(all_responses, indent=4))








from langchain import PromptTemplate, LLMChain
import json
import math

# Define the maximum tokens allowed for the LLM response
MAX_TOKENS = 4000  # Adjust this according to your model's actual token limit
num_diverse_questions = 5  # Number of diverse questions per input

# Define the updated diverse procedural prompt template
DIVERSE_PROCEDURAL_PROMPT = """
You are tasked to generate diverse procedural question-answer pairs.
For each procedural question-answer pair provided below, generate up to {num_diverse_questions} distinct procedural questions
while ensuring that the answers remain consistent and relevant to the original answer provided.

Input:
{input_json}

Instructions:
- Iterate through each input question-answer pair.
- Generate {num_diverse_questions} diverse procedural questions for each input question.
- The output should contain the original question and answer along with the diverse questions and answers.
- Ensure the output is in the following JSON format:

[
    {{
        "id": <unique_id>,
        "original_question": "Original Question",
        "original_answer": "Original Answer",
        "diverse_questions": [
            {{
                "diverse_question": "Diverse Question 1",
                "diverse_answer": "Original Answer"
            }},
            {{
                "diverse_question": "Diverse Question 2",
                "diverse_answer": "Original Answer"
            }},
            ...
        ]
    }},
    ...
]
"""

# Example input with 30 procedural question-answer pairs
input_procedural_qa_list = [
    {
        "Question_type": "Procedural",
        "Question": f"Sample question {i}",
        "Answer": f"Sample answer {i}"
    }
    for i in range(1, 31)
]

# Function to split input into batches based on estimated token size
def split_into_batches(input_list, batch_size=10):
    """Splits the input list into smaller batches to avoid token overflow."""
    for i in range(0, len(input_list), batch_size):
        yield input_list[i:i + batch_size]

# Function to calculate the optimal batch size based on token limits
def calculate_batch_size(input_list, max_tokens=MAX_TOKENS, avg_tokens_per_pair=100):
    """Estimates the batch size that can fit within the token limit."""
    estimated_tokens_per_input = avg_tokens_per_pair * num_diverse_questions
    max_pairs_per_batch = max_tokens // estimated_tokens_per_input
    return min(len(input_list), max(1, max_pairs_per_batch))

# Calculate the optimal batch size
avg_tokens_per_pair = 100  # Estimated token count per question-answer pair
batch_size = calculate_batch_size(input_procedural_qa_list, MAX_TOKENS, avg_tokens_per_pair)

# Fill the prompt template
prompt = PromptTemplate(
    input_variables=["input_json", "num_diverse_questions"],
    template=DIVERSE_PROCEDURAL_PROMPT
)

# Create the LLM chain
llm_chain = LLMChain(llm=llm, prompt=prompt)

# Collect results from all batches
all_responses = []
batch_number = 1

for batch in split_into_batches(input_procedural_qa_list, batch_size):
    print(f"Processing batch {batch_number} with {len(batch)} question-answer pairs...")
    
    # Prepare the input as a JSON string
    batch_input_json = json.dumps(batch)

    # Generate the filled prompt
    fill_prompt = prompt.format(
        input_json=batch_input_json,
        num_diverse_questions=num_diverse_questions
    )

    # Predict diverse questions using the LLM
    response = llm_chain.predict(
        input_json=batch_input_json,
        num_diverse_questions=str(num_diverse_questions),
        stream=True
    )

    # Parse the response as JSON and add to the results
    try:
        response_json = json.loads(response)
        all_responses.extend(response_json)
    except json.JSONDecodeError as e:
        print(f"Failed to parse response for batch {batch_number}: {e}")
    
    batch_number += 1

# Output the combined results
print(json.dumps(all_responses, indent=4))









Prepare the input as individual question-answer pairs
input_pairs = "\n".join([
    f"Question: {item['Question']}\nAnswer: {item['Answer']}"
    for item in input_procedural_qa_list
])





from langchain import PromptTemplate, LLMChain
import json

# Define the updated diverse procedural prompt template
DIVERSE_PROCEDURAL_PROMPT = """
You are tasked to generate diverse procedural question-answer pairs.
For each given procedural question-answer pair in the input list, generate up to {num_diverse_questions} distinct procedural questions
while ensuring that the answers remain consistent and relevant to the original answer provided.

Input:
{input_json}

Instructions:
- Iterate through all input question-answer pairs.
- Generate {num_diverse_questions} diverse procedural questions for each input question.
- The output should contain the original question and answer along with the diverse questions and answers.
- Ensure the output is in the following JSON format:

[
    {{
        "id": <unique_id>,
        "original_question": "Original Question",
        "original_answer": "Original Answer",
        "diverse_questions": [
            {{
                "diverse_question": "Diverse Question 1",
                "diverse_answer": "Original Answer"
            }},
            {{
                "diverse_question": "Diverse Question 2",
                "diverse_answer": "Original Answer"
            }},
            ...
        ]
    }},
    ...
]
"""

# Example input with multiple procedural question-answer pairs
input_procedural_qa_list = [
    {
        "Question_type": "Procedural",
        "Question": "What is the purpose of the Global Delivery Standard Operating Procedure (GD SOP) Template?",
        "Answer": "The GD SOP Template provides standardized procedures for global delivery operations."
    },
    {
        "Question_type": "Procedural",
        "Question": "How do you document the Procedure Document Details?",
        "Answer": "Follow these steps to document procedure details properly."
    },
    {
        "Question_type": "Procedural",
        "Question": "How do you identify the Procedure Process Taxonomy?",
        "Answer": "The procedure process taxonomy is identified through a structured analysis of operational processes."
    },
    {
        "Question_type": "Procedural",
        "Question": "How do you document the Procedure Document Ownership?",
        "Answer": "Procedure document ownership is documented by assigning roles and responsibilities to key stakeholders."
    }
]

num_diverse_questions = 5

# Fill the prompt
prompt = PromptTemplate(
    input_variables=["input_json", "num_diverse_questions"],
    template=DIVERSE_PROCEDURAL_PROMPT
)

# Generate the filled prompt
fill_prompt = prompt.format(
    input_json=json.dumps(input_procedural_qa_list),
    num_diverse_questions=num_diverse_questions
)

# Create the LLM chain and predict diverse questions
llm_chain = LLMChain(llm=llm, prompt=prompt)

response = llm_chain.predict(
    input_json=json.dumps(input_procedural_qa_list),
    num_diverse_questions=str(num_diverse_questions),
    stream=True
)

# Extract and print the valid JSON response
response_json = extract_valid_json(response)
print(response_json)









from langchain import PromptTemplate, LLMChain
import json

# Define the updated diverse procedural prompt template
DIVERSE_PROCEDURAL_PROMPT = """
You are tasked to generate diverse procedural question-answer pairs.
For each given procedural question-answer pair in the input list, generate up to {num_diverse_questions} distinct procedural questions
while ensuring that the answers remain consistent and relevant to the original answer provided.

Input:
{input_json}

Instructions:
- Iterate through all input question-answer pairs.
- Generate {num_diverse_questions} diverse procedural questions for each input question.
- The output should contain the original question and answer along with the diverse questions and answers.
- Ensure the output is in the following JSON format:

[
    {{
        "id": <unique_id>,
        "original_question": "Original Question",
        "original_answer": "Original Answer",
        "diverse_questions": [
            {{
                "diverse_question": "Diverse Question 1",
                "diverse_answer": "Original Answer"
            }},
            {{
                "diverse_question": "Diverse Question 2",
                "diverse_answer": "Original Answer"
            }},
            ...
        ]
    }},
    ...
]
"""

# Define the input list and parameters
input_procedural_qa_list = [
    {
        "Question_type": "Procedural",
        "Question": "What is Sachin Tendulkar?",
        "Answer": "Cricketer"
    },
    {
        "Question_type": "Procedural",
        "Question": "What does machine learning mean?",
        "Answer": "Machine learning is a field of AI that involves training algorithms to learn patterns from data."
    }
]

num_diverse_questions = 5

# Fill the prompt
prompt = PromptTemplate(
    input_variables=["input_json", "num_diverse_questions"],
    template=DIVERSE_PROCEDURAL_PROMPT
)

# Generate the filled prompt
fill_prompt = prompt.format(
    input_json=json.dumps(input_procedural_qa_list),
    num_diverse_questions=num_diverse_questions
)

# Create the LLM chain and predict diverse questions
llm_chain = LLMChain(llm=llm, prompt=prompt)

response = llm_chain.predict(
    input_json=json.dumps(input_procedural_qa_list),
    num_diverse_questions=str(num_diverse_questions),
    stream=True
)

# Extract and print the valid JSON response
response_json = extract_valid_json(response)
print(response_json)








# Define the diverse procedural prompt template
DIVERSE_PROCEDURAL_PROMPT = """
You are tasked to generate diverse procedural question-answer pairs. For each given procedural question-answer pair, generate up to {num_diverse_questions} distinct procedural questions while ensuring that the answers remain consistent and relevant to the original answer provided.

Input:
{input_json}

Instructions:
- Generate diverse procedural questions for each input question.
- Use varied questioning techniques.
- Ensure the output is a valid list of JSON objects.

Output Format:
[
    {{"Question_type": "Procedural", "Question": "Diverse Procedural Question 1", "Answer": "Original Answer"}},
    {{"Question_type": "Procedural", "Question": "Diverse Procedural Question 2", "Answer": "Original Answer"}},
    ...
]
"""

# Fill the prompt
prompt = PromptTemplate(
    input_variables=["input_json", "num_diverse_questions"],
    template=DIVERSE_PROCEDURAL_PROMPT
)

# Generate the filled prompt
fill_prompt = prompt.format(
    input_json=json.dumps(input_procedural_qa_list),
    num_diverse_questions=5
)

# Create the LLM chain and predict diverse questions
llm_chain = LLMChain(llm=llm, prompt=prompt)

response = llm_chain.predict(
    input_json=json.dumps(input_procedural_qa_list),
    num_diverse_questions=str(5),
    stream=True
)

# Extract and print the valid JSON response
response_json = extract_valid_json(response)
print(response_json)









def encode_prompt(prompt_instructions):
    prompt = """
You are asked to create a set of 5 diverse tasks based on a given text. For each task, please provide the following components:

1. **Instruction:**  
   - A concise directive that specifies what to do.  
   - **Important:** Do not start with "summarize." Use a variety of verbs such as "explain," "describe," "identify," "list," or "outline" to ensure diversity in phrasing.

2. **Input:**  
   - An example input text that the instruction should be applied to.
   - The input should include relevant details that support the instruction.

3. **Output:**  
   - The expected response after applying the instruction to the input.
   - Ensure the output is concise, accurate, and directly derived from the input.

**General Requirements:**
- Read the provided text carefully and extract key concepts, facts, and ideas.
- Each task must be unique; avoid repeating the same instruction or input-output pair.
- The total output for each task (instruction, input, and output) should be under 1000 words.
- The language and tone should vary; mix imperative sentences with interrogative styles where appropriate.

**Examples:**

*Task 1:*  
- **Instruction:** "Explain the process involved in monitoring VFMC RightFax documents."  
- **Input:** "The VFMC RightFax system requires continuous monitoring to ensure that all documents are processed promptly and accurately."  
- **Output:** "The system monitors document flow in real-time, alerts operators of delays, and ensures documents are processed efficiently."

*Task 2:*  
- **Instruction:** "Identify the key steps required to handle VFMC RightFax documents."  
- **Input:** "Operators are required to check the document logs, verify the data integrity, and initiate the processing protocols accordingly."  
- **Output:** "The key steps include checking document logs, verifying data integrity, and initiating processing protocols."

Now, generate 5 diverse tasks following these guidelines.
    """
    return prompt











def encode_prompt(prompt_instructions):
    prompt = """
You are asked to create a set of 5 diverse task instructions based on the given text.

Important Rules:
1. Read the provided text and extract key concepts, facts, and ideas.
2. Each instruction must be concise, accurate, and directly derived from the text.
3. Do not start any instruction with the word "summarize." Use different verbs and sentence structures to ensure diversity.
4. Vary both the linguistic style and the language where possible. For example, mix imperative instructions with interrogative sentences.
5. The final output must be under 1000 words.
6. Each instruction must be unique; avoid repeating any question-answer pair.

Here are some examples of diverse instructions (none of these should start with "summarize"):
- "Explain the steps required to monitor and process VFMC RightFax documents."
- "Describe the process involved in handling VFMC RightFax documents."
- "Identify the key procedures for managing VFMC RightFax document flow."
- "List the methods used to oversee VFMC RightFax document processing."
- "Outline the techniques for monitoring VFMC RightFax document activity."

Now, using these guidelines, generate 5 distinct task instructions.
    """
    return prompt



def post_process_response(num_prompt_instructions, response_text):
    raw_instructions = f"{num_prompt_instructions+1}. Instruction:" + response_text
    raw_instructions = re.split("###", raw_instructions)
    instructions = []

    # Initialize idx to avoid UnboundLocalError
    idx = -1

    for idx, inst in enumerate(raw_instructions):
        if idx == len(raw_instructions) - 1:
            continue
        instruction_index = idx + num_prompt_instructions + 1
        splitted_data = re.split(f"{instruction_index}\.\s+(Instruction|Input|Output):", inst)
        if len(splitted_data) != 7:
            continue
        instruction = splitted_data[2].strip()
        input_data = splitted_data[4].strip()
        input_data = "" if input_data.lower() == "<noinput>" else input_data
        output = splitted_data[6].strip()

        if len(instruction.split()) <= 3 or len(instruction.split()) > 150:
            continue

        blacklist = [
            "image", "images", "graph", "graphs", "picture", "pictures", "file", "files",
            "map", "maps", "draw", "plot", "go to", "video", "audio", "music",
            "flowchart", "diagram",
        ]
        if any(re.search(rf"\b{word}\b", instruction, re.IGNORECASE) for word in blacklist):
            continue
        if instruction.startswith("Write a program"):
            continue
        if instruction[0] in string.punctuation or not instruction[0].isascii():
            continue

        instructions.append({"instruction": instruction, "input": input_data, "output": output})
    return instructions


def generate_instruction_following_data():
    output_dir = "./"
    seed_tasks_path = "./seed_tasks.xlsx"
    num_instructions_to_generate = 10
    num_prompt_instructions = 3
    seed_tasks_jsonl = os.path.join(output_dir, "seed_tasks.jsonl")
    convert_excel_to_jsonl(seed_tasks_path, seed_tasks_jsonl)

    with open(seed_tasks_jsonl, "r", encoding="utf-8") as f:
        seed_instruction_data = [json.loads(line) for line in f]

    machine_instruction_data = []
    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)

    while len(machine_instruction_data) < num_instructions_to_generate:
        prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
        prompt = encode_prompt(prompt_instructions)

        response = llm([HumanMessage(content=prompt)]).content
        new_instructions = post_process_response(num_prompt_instructions, response)
        machine_instruction_data.extend(new_instructions)
        progress_bar.update(len(new_instructions))

        with open(os.path.join(output_dir, "regen.json"), "w", encoding="utf-8") as f:
            json.dump(machine_instruction_data, f, ensure_ascii=False, indent=4)

    progress_bar.close()
    print("Generation completed.")


if __name__ == '__main__':
    generate_instruction_following_data()








def post_process_response(num_prompt_instructions, response_text):
    raw_instructions = f"{num_prompt_instructions+1}. Instruction:" + response_text
    raw_instructions = re.split("###", raw_instructions)
    instructions = []

    # Initialize idx with a default value
    idx = -1
    
    for idx, inst in enumerate(raw_instructions):
        if idx == len(raw_instructions) - 1:
            continue
        
        instruction_index = idx + num_prompt_instructions + 1
        splitted_data = re.split(f"{instruction_index}\.\s+(Instruction|Input|Output):", inst)
        
        if len(splitted_data) != 7:
            continue
        instruction = splitted_data[2].strip()
        input_data = splitted_data[4].strip()
        input_data = "" if input_data.lower() == "<noinput>" else input_data
        output = splitted_data[6].strip()

        if len(instruction.split()) <= 3 or len(instruction.split()) > 150:
            continue

        blacklist = [
            "image", "images", "graph", "graphs", "picture", "pictures", "file", "files",
            "map", "maps", "draw", "plot", "go to", "video", "audio", "music",
            "flowchart", "diagram",
        ]
        if any(re.search(rf"\b{word}\b", instruction, re.IGNORECASE) for word in blacklist):
            continue
        if instruction.startswith("Write a program"):
            continue
        if instruction[0] in string.punctuation or not instruction[0].isascii():
            continue

        instructions.append({"instruction": instruction, "input": input_data, "output": output})

    return instructions









def post_process_response(num_prompt_instructions, response_text):
    raw_instructions = f"{num_prompt_instructions+1}. Instruction:" + response_text
    raw_instructions = re.split("###", raw_instructions)
    instructions = []
    for idx, inst in enumerate(raw_instructions):
        if idx == len(raw_instructions) - 1:
            continue
        
        instruction_index = idx + num_prompt_instructions + 1  # Use a new variable here
        splitted_data = re.split(f"{instruction_index}\.\s+(Instruction|Input|Output):", inst)
        
        if len(splitted_data) != 7:
            continue
        
        instruction = splitted_data[2].strip()
        input_data = splitted_data[4].strip()
        input_data = "" if input_data.lower() == "<noinput>" else input_data
        output = splitted_data[6].strip()

        if len(instruction.split()) <= 3 or len(instruction.split()) > 150:
            continue

        blacklist = [
            "image", "images", "graph", "graphs", "picture", "pictures", "file", "files",
            "map", "maps", "draw", "plot", "go to", "video", "audio", "music",
            "flowchart", "diagram",
        ]
        if any(re.search(rf"\b{word}\b", instruction, re.IGNORECASE) for word in blacklist):
            continue
        if instruction.startswith("Write a program"):
            continue
        if instruction[0] in string.punctuation or not instruction[0].isascii():
            continue

        instructions.append({"instruction": instruction, "input": input_data, "output": output})
    return instructions








def generate_instruction_following_data(
    output_dir="./",
    seed_tasks_path="./seed_tasks.xlsx",
    num_instructions_to_generate=100,
    num_prompt_instructions=3,
    temperature=1.0,
    top_p=1.0
):
    seed_tasks_jsonl = os.path.join(output_dir, "seed_tasks.jsonl")
    convert_excel_to_jsonl(seed_tasks_path, seed_tasks_jsonl)

    with open(seed_tasks_jsonl, "r", encoding="utf-8") as f:
        seed_instruction_data = [json.loads(line) for line in f]

    print(f"Loaded {len(seed_instruction_data)} human-written seed instructions")

    os.makedirs(output_dir, exist_ok=True)
    machine_instruction_data = []

    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)

    while len(machine_instruction_data) < num_instructions_to_generate:
        prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
        prompt = encode_prompt(prompt_instructions)

        response = llm([HumanMessage(content=prompt)]).content
        new_instructions = post_process_response(num_prompt_instructions, response)
        machine_instruction_data.extend(new_instructions)
        progress_bar.update(len(new_instructions))

        with open(os.path.join(output_dir, "regen.json"), "w", encoding="utf-8") as f:
            json.dump(machine_instruction_data, f, ensure_ascii=False, indent=4)

    progress_bar.close()
    print("Generation completed.")


if __name__ == '__main__':
    generate_instruction_following_data()









import os
import json
import random
import re
import time
import string
from functools import partial
from multiprocessing import Pool
import numpy as np
import tqdm
from rouge_score import rouge_scorer
from azure.identity import DefaultAzureCredential
import openai
import pandas as pd

# Azure OpenAI Setup
openai.api_type = "azure"
openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
openai.api_version = "2023-03-15-preview"
openai.api_key = os.getenv("AZURE_OPENAI_API_KEY")

MODEL_NAME = "ssgpt-40"


# Define the OpenAIDecodingArguments class
class OpenAIDecodingArguments(object):
    def __init__(self, max_tokens=1800, temperature=0.2, top_p=1.0, n=1, stop=None):
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.n = n
        self.stop = stop if stop else ["\n20", "20.", "20."]


def convert_excel_to_jsonl(excel_path, jsonl_path):
    df = pd.read_excel(excel_path)
    with open(jsonl_path, 'w', encoding='utf-8') as f:
        for _, row in df.iterrows():
            task = {
                "instruction": row["Instruction"],
                "instances": [
                    {
                        "input": row["Input"] if pd.notna(row["Input"]) else "<noinput>",
                        "output": row["Output"],
                    }
                ],
                "category": row["Question_Type"] if "Question_Type" in df.columns else ""
            }
            f.write(json.dumps(task, ensure_ascii=False) + "\n")
    print(f"Seed tasks saved to {jsonl_path}")


def encode_prompt(prompt_instructions):
    prompt = open("./prompt.txt").read() + "\n"

    for idx, task_dict in enumerate(prompt_instructions):
        instruction = task_dict["instruction"].strip().rstrip(":")
        input_data = "<noinput>" if not task_dict["instances"][0]["input"] else task_dict["instances"][0]["input"].strip()
        output = task_dict["instances"][0]["output"].strip()
        prompt += f"###\n"
        prompt += f"{idx + 1}. Instruction: {instruction}\n"
        prompt += f"{idx + 1}. Input:\n{input_data}\n"
        prompt += f"{idx + 1}. Output:\n{output}\n"
    prompt += f"###\n"
    prompt += f"{idx + 2}. Instruction:"
    return prompt


def post_process_response(num_prompt_instructions, response_text):
    raw_instructions = f"{num_prompt_instructions+1}. Instruction:" + response_text
    raw_instructions = re.split("###", raw_instructions)
    instructions = []
    for idx, inst in enumerate(raw_instructions):
        if idx == len(raw_instructions) - 1:
            continue
        idx += num_prompt_instructions + 1
        splitted_data = re.split(f"{idx}\.\s+(Instruction|Input|Output):", inst)
        if len(splitted_data) != 7:
            continue
        instruction = splitted_data[2].strip()
        input_data = splitted_data[4].strip()
        input_data = "" if input_data.lower() == "<noinput>" else input_data
        output = splitted_data[6].strip()

        if len(instruction.split()) <= 3 or len(instruction.split()) > 150:
            continue

        blacklist = [
            "image", "images", "graph", "graphs", "picture", "pictures", "file", "files",
            "map", "maps", "draw", "plot", "go to", "video", "audio", "music",
            "flowchart", "diagram",
        ]
        if any(re.search(rf"\b{word}\b", instruction, re.IGNORECASE) for word in blacklist):
            continue
        if instruction.startswith("Write a program"):
            continue
        if instruction[0] in string.punctuation or not instruction[0].isascii():
            continue

        instructions.append({"instruction": instruction, "input": input_data, "output": output})
    return instructions


def compute_rouge_scores(all_instructions, new_instruction, scorer, threshold=0.7):
    new_tokens = scorer._tokenizer.tokenize(new_instruction)
    scores = [scorer.score(inst, new_instruction)['rougeL'].fmeasure for inst in all_instructions]
    if max(scores) > threshold:
        return None
    return float(np.mean(scores))


def generate_instruction_following_data(
    output_dir="./",
    seed_tasks_path="./seed_tasks.xlsx",
    num_instructions_to_generate=100,
    num_prompt_instructions=3,
    request_batch_size=5,
    temperature=1.0,
    top_p=1.0
):
    seed_tasks_jsonl = os.path.join(output_dir, "seed_tasks.jsonl")
    convert_excel_to_jsonl(seed_tasks_path, seed_tasks_jsonl)

    with open(seed_tasks_jsonl, "r", encoding="utf-8") as f:
        seed_instruction_data = [json.loads(line) for line in f]

    print(f"Loaded {len(seed_instruction_data)} human-written seed instructions")

    os.makedirs(output_dir, exist_ok=True)
    machine_instruction_data = []

    scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=False)
    all_instructions = [d["instruction"] for d in seed_instruction_data]

    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)

    while len(machine_instruction_data) < num_instructions_to_generate:
        prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
        prompt = encode_prompt(prompt_instructions)
        response = openai.ChatCompletion.create(
            deployment_id=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=3072,
            temperature=temperature,
            top_p=top_p,
            n=1,
            stop=["\n20", "20.", "20."]
        )

        for choice in response.choices:
            new_instructions = post_process_response(num_prompt_instructions, choice.message["content"])
            for instruction_data in new_instructions:
                score = compute_rouge_scores(all_instructions, instruction_data["instruction"], scorer)
                if score is not None:
                    instruction_data["avg_similarity_score"] = score
                    machine_instruction_data.append(instruction_data)
                    all_instructions.append(instruction_data["instruction"])
                    progress_bar.update(1)

        with open(os.path.join(output_dir, "regen.json"), "w", encoding="utf-8") as f:
            json.dump(machine_instruction_data, f, ensure_ascii=False, indent=4)

    progress_bar.close()
    print("Generation completed.")


if __name__ == '__main__':
    generate_instruction_following_data()









import os
import json
import pandas as pd
import tqdm

# Instruction templates based on question type
INSTRUCTION_TEMPLATES = {
    "Factual": "Using the facts given, write a question-answer pair.",
    "Conceptual": "Explain the concept behind the provided input and generate a question-answer pair.",
    "Inferential": "Draw inferences based on the information and create a question-answer pair.",
    "Reasoning-based": "Apply logical reasoning to generate a relevant question-answer pair.",
    "Procedural": "Describe the procedure and create a question-answer pair."
}

# Load Excel data and prepare seed tasks
excel_file_path = "sample_dataset.xlsx"  # Replace with the actual path to your Excel file
seed_tasks_path = "seed_tasks.jsonl"
output_dir = "./generated_data"

os.makedirs(output_dir, exist_ok=True)

# Generate seed tasks from Excel with dynamic instructions
def generate_seed_tasks_from_excel(excel_file_path, seed_tasks_path):
    df = pd.read_excel(excel_file_path)
    seed_data = []

    for idx, row in df.iterrows():
        question_type = str(row['Question Type']).strip().capitalize() if pd.notna(row['Question Type']) else "Factual"
        instruction_template = INSTRUCTION_TEMPLATES.get(question_type, INSTRUCTION_TEMPLATES["Factual"])
        
        instruction = str(row['Instruction']).strip() if pd.notna(row['Instruction']) else instruction_template
        input_text = str(row['Input']).strip() if pd.notna(row['Input']) else "<noinput>"
        question = str(row['Question']).strip() if pd.notna(row['Question']) else ""
        output_text = str(row['Answer']).strip() if pd.notna(row['Answer']) else ""

        if question and output_text:
            task = {
                "id": f"seed_task_{idx}",
                "name": f"question_answering_generation_from_{question_type.lower()}",
                "instruction": instruction,
                "instances": [
                    {
                        "input": input_text,
                        "output": [[f"Question: {question}"], [f"Answer: {output_text}"]]
                    }
                ],
                "is_classification": question_type.lower() in ["classification", "reasoning-based"]
            }
            seed_data.append(task)

    with open(seed_tasks_path, 'w', encoding="utf-8") as f:
        for task in seed_data:
            f.write(json.dumps(task) + '\n')

    print(f"Seed tasks saved at: {seed_tasks_path}")

generate_seed_tasks_from_excel(excel_file_path, seed_tasks_path)

# Load and validate seed tasks from JSONL file
def validate_jsonl_file(file_path):
    valid_tasks = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    json_obj = json.loads(line.strip())
                    valid_tasks.append(json_obj)
                except json.JSONDecodeError as e:
                    print(f"Invalid JSON: {e}")
        print(f"Loaded {len(valid_tasks)} valid JSON tasks.")
    except FileNotFoundError as e:
        print(f"File not found: {e}")
    return valid_tasks

seed_tasks = validate_jsonl_file(seed_tasks_path)

# Preview the generated output
for data in seed_tasks[:5]:
    print(json.dumps(data, indent=4))





import os
import json
import pandas as pd
import tqdm
import openai
from typing import List

# Initialize OpenAI API for Azure with the ssgpt-40 deployment
openai.api_key = "<your-azure-api-key>"
openai.api_base = "https://<your-azure-openai-endpoint>.openai.azure.com/"
openai.api_type = "azure"
openai.api_version = "2024-06-01-preview"

# Instruction templates based on question type
INSTRUCTION_TEMPLATES = {
    "Factual": "Using the facts given, write a question-answer pair.",
    "Conceptual": "Explain the concept behind the provided input and generate a question-answer pair.",
    "Inferential": "Draw inferences based on the information and create a question-answer pair.",
    "Reasoning-based": "Apply logical reasoning to generate a relevant question-answer pair.",
    "Procedural": "Describe the procedure and create a question-answer pair."
}

# Load Excel data and prepare seed tasks
excel_file_path = "sample_dataset.xlsx"  # Replace with the actual path to your Excel file
seed_tasks_path = "seed_tasks.jsonl"
output_dir = "./generated_data"

os.makedirs(output_dir, exist_ok=True)

# Generate seed tasks from Excel with dynamic instructions
def generate_seed_tasks_from_excel(excel_file_path, seed_tasks_path):
    df = pd.read_excel(excel_file_path)
    seed_data = []

    for idx, row in df.iterrows():
        question_type = str(row['Question Type']).strip().capitalize() if pd.notna(row['Question Type']) else "Factual"
        instruction_template = INSTRUCTION_TEMPLATES.get(question_type, INSTRUCTION_TEMPLATES["Factual"])
        
        instruction = str(row['Instruction']).strip() if pd.notna(row['Instruction']) else instruction_template
        input_text = str(row['Input']).strip() if pd.notna(row['Input']) else "<noinput>"
        question = str(row['Question']).strip() if pd.notna(row['Question']) else ""
        output_text = str(row['Output']).strip() if pd.notna(row['Output']) else ""

        if question and output_text:
            task = {
                "id": f"seed_task_{idx}",
                "name": f"question_answering_generation_from_{question_type.lower()}",
                "instruction": instruction,
                "instances": {
                    "input": input_text,
                    "output": [[f"Question: {question}"], [f"Answer: {output_text}"]]
                },
                "is_classification": question_type.lower() in ["classification", "reasoning-based"]
            }
            seed_data.append(task)

    with open(seed_tasks_path, 'w', encoding="utf-8") as f:
        for task in seed_data:
            f.write(json.dumps(task) + '\n')

    print(f"Seed tasks saved at: {seed_tasks_path}")

generate_seed_tasks_from_excel(excel_file_path, seed_tasks_path)

# Load and validate seed tasks from JSONL file
def validate_jsonl_file(file_path):
    valid_tasks = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    json_obj = json.loads(line.strip())
                    valid_tasks.append(json_obj)
                except json.JSONDecodeError as e:
                    print(f"Invalid JSON: {e}")
        print(f"Loaded {len(valid_tasks)} valid JSON tasks.")
    except FileNotFoundError as e:
        print(f"File not found: {e}")
    return valid_tasks

seed_tasks = validate_jsonl_file(seed_tasks_path)

# Preview the generated output
for data in seed_tasks[:5]:
    print(json.dumps(data, indent=4))







import os
import json
import random
import re
import string
import pandas as pd
import tqdm
import openai
import copy
import dataclasses
import math
import sys
import time
from typing import Optional, Sequence, Union
from openai import openai_object

# Initialize OpenAI API for decoding with ssgpt-40 model
openai.api_key = "<your-azure-api-key>"
openai.api_base = "https://<your-azure-openai-endpoint>.openai.azure.com/"
openai.api_type = "azure"
openai.api_version = "2024-06-01-preview"

StrOrOpenAIObject = Union[str, openai_object.OpenAIObject]

# Decoding Arguments Class
@dataclasses.dataclass
class OpenAIDecodingArguments(object):
    max_tokens: int = 1800
    temperature: float = 0.7
    top_p: float = 1.0
    n: int = 1
    stream: bool = False
    stop: Optional[Sequence[str]] = None
    presence_penalty: float = 0.0
    frequency_penalty: float = 0.0
    suffix: Optional[str] = None
    logprobs: Optional[int] = None
    echo: bool = False

# Load Excel data and create seed_tasks.jsonl
excel_file_path = "C:/data/your_dataset.xlsx"
seed_tasks_path = "C:/data/seed_tasks.jsonl"

def generate_seed_tasks_from_excel(excel_file_path, seed_tasks_path):
    df = pd.read_excel(excel_file_path)
    seed_data = []

    for idx, row in df.iterrows():
        input_text = str(row['question']) if pd.notna(row['question']) else ""
        output_text = str(row['answer']) if pd.notna(row['answer']) else ""

        if input_text and output_text:
            task = {
                "id": f"seed_task_{idx}",
                "name": f"SOP_{idx}",
                "instruction": input_text,
                "instances": [{
                    "input": input_text,
                    "output": output_text
                }],
                "is_classification": False
            }
            seed_data.append(task)

    with open(seed_tasks_path, 'w', encoding="utf-8") as f:
        for task in seed_data:
            f.write(json.dumps(task) + '\n')

    print(f"Seed tasks saved at: {seed_tasks_path}")

generate_seed_tasks_from_excel(excel_file_path, seed_tasks_path)

# Validate JSONL file to avoid decoding errors
def validate_jsonl_file(file_path):
    valid_tasks = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    json_obj = json.loads(line.strip())
                    valid_tasks.append(json_obj)
                except json.JSONDecodeError as e:
                    print(f"Invalid JSON: {e}")
        print(f"Loaded {len(valid_tasks)} valid JSON tasks.")
    except FileNotFoundError as e:
        print(f"File not found: {e}")
    return valid_tasks

seed_tasks = validate_jsonl_file(seed_tasks_path)

# Completion function using OpenAIDecodingArguments and openai.Completion.create
def openai_completion(prompts, decoding_args: OpenAIDecodingArguments, model_name="ssgpt-40", batch_size=5):
    responses = []
    is_single_prompt = isinstance(prompts, str)
    if is_single_prompt:
        prompts = [prompts]

    prompt_batches = [
        prompts[batch_id * batch_size : (batch_id + 1) * batch_size]
        for batch_id in range(int(math.ceil(len(prompts) / batch_size)))
    ]

    for prompt_batch in tqdm.tqdm(prompt_batches, desc="Prompt Batches"):
        try:
            completion_batch = openai.Completion.create(
                engine=model_name,
                prompt=prompt_batch,
                max_tokens=decoding_args.max_tokens,
                temperature=decoding_args.temperature,
                top_p=decoding_args.top_p,
                n=decoding_args.n,
                stop=decoding_args.stop,
                logprobs=decoding_args.logprobs,
                echo=decoding_args.echo
            )
            for choice in completion_batch.choices:
                responses.append({
                    "text": choice.text.strip(),
                    "finish_reason": choice.finish_reason
                })
        except Exception as e:
            print(f"Error with model call: {e}")
            responses.append({"text": "", "finish_reason": "error"})
    
    return responses

# Encode prompt for the model
def encode_prompt(prompt_instructions):
    prompt = "You are asked to generate diverse task instructions for a GPT model.\n"

    for idx, task_dict in enumerate(prompt_instructions):
        (instruction, input_text, output) = task_dict["instruction"], task_dict["input"], task_dict["output"]
        input_text = "<noinput>" if input_text.lower() == "" else input_text
        prompt += f"###\n"
        prompt += f"{idx + 1}. Instruction: {instruction}\n"
        prompt += f"{idx + 1}. Input:\n{input_text}\n"
        prompt += f"{idx + 1}. Output:\n{output}\n"
    prompt += f"###\n{idx + 2}. Instruction:"
    return prompt

# Generate instructions using OpenAI API
def generate_instruction_following_data(
    output_dir="./generated_data",
    num_instructions_to_generate=100,
    num_prompt_instructions=3,
    request_batch_size=5
):
    os.makedirs(output_dir, exist_ok=True)
    machine_instruction_data = []

    seed_instruction_data = [
        {"instruction": t["instruction"], "input": t["instances"][0]["input"], "output": t["instances"][0]["output"]}
        for t in seed_tasks
    ]

    decoding_args = OpenAIDecodingArguments(
        max_tokens=1800,
        temperature=0.7,
        top_p=1.0,
        n=1,
        stop=["\n20", "20.", "20."]
    )

    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)

    while len(machine_instruction_data) < num_instructions_to_generate:
        batch_inputs = []
        for _ in range(request_batch_size):
            prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
            prompt = encode_prompt(prompt_instructions)
            batch_inputs.append(prompt)

        results = openai_completion(batch_inputs, decoding_args)

        for result in results:
            if result and "text" in result:
                machine_instruction_data.append(result)
                progress_bar.update(1)

        with open(os.path.join(output_dir, "regen.json"), "w", encoding="utf-8") as f:
            json.dump(machine_instruction_data, f)

    print("Instruction generation completed.")

# Execute the process
generate_instruction_following_data()

# Preview the generated output
output_file = "./generated_data/regen.json"

with open(output_file, "r", encoding="utf-8") as f:
    generated_data = json.load(f)

for data in generated_data[:10]:
    print(f"Instruction: {data['text']}\n")






def openai_completion(prompts, batch_size=5):
    responses = []
    for prompt in prompts:
        try:
            response = llm(
                [HumanMessage(content=prompt)],
                max_tokens=3072,
                temperature=0.7,
                top_p=1.0,
                stop=["\n20", "20.", "20."],
                logit_bias={"50256": -100}  # Prevent the <|endoftext|> token from being generated
            )
            responses.append({
                "text": response.content,
                "finish_reason": "stop"
            })
        except Exception as e:
            print(f"Error with model call: {e}")
            responses.append({"text": "", "finish_reason": "error"})
    return responses







import os
import json
import random
import re
import string
from multiprocessing import Pool
from functools import partial

import numpy as np
import pandas as pd
import tqdm
from rouge_score import rouge_scorer
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage

# Initialize Azure GPT-4 Model using Langchain's AzureChatOpenAI
azure_openai_api_version = "2024-06-01-preview"
azure_openai_endpoint = "https://<your-azure-openai-endpoint>.openai.azure.com/"
azure_openai_api_key = "<your-azure-api-key>"

llm = AzureChatOpenAI(
    temperature=0.7, 
    api_version=azure_openai_api_version,
    openai_api_base=azure_openai_endpoint,
    openai_api_key=azure_openai_api_key,
    deployment_name="ssgpt-40", 
    max_tokens=3072
)

# Load Excel data and create seed_tasks.jsonl
excel_file_path = "C:/data/your_dataset.xlsx"
seed_tasks_path = "C:/data/seed_tasks.jsonl"

# Load Excel file and convert to seed_tasks.jsonl
def generate_seed_tasks_from_excel(excel_file_path, seed_tasks_path):
    df = pd.read_excel(excel_file_path)
    seed_data = []

    for idx, row in df.iterrows():
        input_text = row['question'] if pd.notna(row['question']) else ""
        output_text = row['answer'] if pd.notna(row['answer']) else ""

        task = {
            "id": f"seed_task_{idx}",
            "name": f"SOP_{idx}",
            "instruction": row['question'],
            "instances": [{
                "input": input_text,
                "output": output_text
            }],
            "is_classification": False
        }
        seed_data.append(task)

    # Write to JSONL format
    with open(seed_tasks_path, 'w', encoding="utf-8") as f:
        for task in seed_data:
            f.write(json.dumps(task) + '\n')

    print(f"Seed tasks saved at: {seed_tasks_path}")

generate_seed_tasks_from_excel(excel_file_path, seed_tasks_path)

# Validate seed_tasks.jsonl
def validate_jsonl_file(file_path):
    valid_tasks = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    json_obj = json.loads(line.strip())
                    valid_tasks.append(json_obj)
                except json.JSONDecodeError as e:
                    print(f"Invalid JSON: {e}")
        print(f"Loaded {len(valid_tasks)} valid JSON tasks.")
    except FileNotFoundError as e:
        print(f"File not found: {e}")
    return valid_tasks

seed_tasks = validate_jsonl_file(seed_tasks_path)

# Azure GPT-4 completion function
def openai_completion(prompts, batch_size=5):
    responses = []
    for prompt in prompts:
        try:
            response = llm([HumanMessage(content=prompt)])
            responses.append({
                "text": response.content,
                "finish_reason": "stop"
            })
        except Exception as e:
            print(f"Error with model call: {e}")
            responses.append({"text": "", "finish_reason": "error"})
    return responses

# Encode prompt for the model
def encode_prompt(prompt_instructions):
    """Encode multiple prompt instructions into a single string."""
    prompt = "You are asked to generate diverse task instructions for a GPT model.\n"

    for idx, task_dict in enumerate(prompt_instructions):
        (instruction, input_text, output) = task_dict["instruction"], task_dict["input"], task_dict["output"]
        instruction = re.sub(r"\s+", " ", instruction).strip().rstrip(":")
        input_text = "<noinput>" if input_text.lower() == "" else input_text
        prompt += f"###\n"
        prompt += f"{idx + 1}. Instruction: {instruction}\n"
        prompt += f"{idx + 1}. Input:\n{input_text}\n"
        prompt += f"{idx + 1}. Output:\n{output}\n"
    prompt += f"###\n"
    prompt += f"{idx + 2}. Instruction:"
    return prompt

# Post-process GPT responses
def post_process_gpt3_response(num_prompt_instructions, response):
    if response is None or not response.get("text"):
        return []
    
    raw_instructions = f"{num_prompt_instructions+1}. Instruction:" + response["text"]
    raw_instructions = re.split("###", raw_instructions)
    instructions = []

    for idx, inst in enumerate(raw_instructions):
        if idx == len(raw_instructions) - 1 and response.get("finish_reason") == "length":
            continue
        
        idx += num_prompt_instructions + 1
        splitted_data = re.split(f"{idx}\.\s+(Instruction|Input|Output):", inst)
        
        if len(splitted_data) < 7:
            continue
        
        instruction = splitted_data[2].strip()
        input_text = splitted_data[4].strip()
        input_text = "" if input_text.lower() == "<noinput>" else input_text
        output = splitted_data[6].strip()
        
        instructions.append({"instruction": instruction, "input": input_text, "output": output})
    
    return instructions

# Generate instructions using Azure GPT-4
def generate_instruction_following_data(
    output_dir="./generated_data",
    seed_tasks_path="C:/data/seed_tasks.jsonl",
    num_instructions_to_generate=100,
    num_prompt_instructions=3,
    request_batch_size=5
):
    os.makedirs(output_dir, exist_ok=True)
    machine_instruction_data = []

    seed_instruction_data = [
        {"instruction": t["instruction"], "input": t["instances"][0]["input"], "output": t["instances"][0]["output"]}
        for t in seed_tasks
    ]
    
    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)

    while len(machine_instruction_data) < num_instructions_to_generate:
        batch_inputs = []
        for _ in range(request_batch_size):
            prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
            prompt = encode_prompt(prompt_instructions)
            batch_inputs.append(prompt)

        results = openai_completion(batch_inputs, batch_size=request_batch_size)

        for result in results:
            if result and "text" in result:
                new_instructions = post_process_gpt3_response(num_prompt_instructions, result)
                machine_instruction_data.extend(new_instructions)
                progress_bar.update(len(new_instructions))

        with open(os.path.join(output_dir, "regen.json"), "w", encoding="utf-8") as f:
            json.dump(machine_instruction_data, f)

    print("Instruction generation completed.")

# Execute the process
generate_instruction_following_data()

# Preview the generated output
output_file = "./generated_data/regen.json"

with open(output_file, "r", encoding="utf-8") as f:
    generated_data = json.load(f)

for data in generated_data[:10]:
    print(f"Instruction: {data['instruction']}")
    print(f"Input: {data['input']}")
    print(f"Output: {data['output']}\n")










import os
import json
import random
import re
import string
from multiprocessing import Pool
from functools import partial

import numpy as np
import pandas as pd
import tqdm
from rouge_score import rouge_scorer
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage

# Initialize Azure GPT-4 Model using Langchain's AzureChatOpenAI
azure_openai_api_version = "2024-06-01-preview"
azure_openai_endpoint = "https://<your-azure-openai-endpoint>.openai.azure.com/"
azure_openai_api_key = "<your-azure-api-key>"

llm = AzureChatOpenAI(
    temperature=0.7, 
    api_version=azure_openai_api_version,
    openai_api_base=azure_openai_endpoint,
    openai_api_key=azure_openai_api_key,
    deployment_name="ssgpt-40", 
    max_tokens=3072
)

def openai_completion(prompts, batch_size=5):
    responses = []
    for prompt in prompts:
        try:
            response = llm([HumanMessage(content=prompt)])
            responses.append({
                "text": response.content,
                "finish_reason": "stop"
            })
        except Exception as e:
            print(f"Error with model call: {e}")
            responses.append({"text": "", "finish_reason": "error"})
    return responses

def encode_prompt(prompt_instructions):
    """Encode multiple prompt instructions into a single string."""
    prompt = open("./prompt.txt").read() + "\n"

    for idx, task_dict in enumerate(prompt_instructions):
        (instruction, input_text, output) = task_dict["instruction"], task_dict["input"], task_dict["output"]
        instruction = re.sub(r"\s+", " ", instruction).strip().rstrip(":")
        input_text = "<noinput>" if input_text.lower() == "" else input_text
        prompt += f"###\n"
        prompt += f"{idx + 1}. Instruction: {instruction}\n"
        prompt += f"{idx + 1}. Input:\n{input_text}\n"
        prompt += f"{idx + 1}. Output:\n{output}\n"
    prompt += f"###\n"
    prompt += f"{idx + 2}. Instruction:"
    return prompt

def post_process_gpt3_response(num_prompt_instructions, response):
    if response is None or not response.get("text"):
        return []
    
    raw_instructions = f"{num_prompt_instructions+1}. Instruction:" + response["text"]
    raw_instructions = re.split("###", raw_instructions)
    instructions = []

    for idx, inst in enumerate(raw_instructions):
        if idx == len(raw_instructions) - 1 and response.get("finish_reason") == "length":
            continue
        
        idx += num_prompt_instructions + 1
        splitted_data = re.split(f"{idx}\.\s+(Instruction|Input|Output):", inst)
        
        if len(splitted_data) < 7:
            continue  # Avoid skipping valid instructions due to strict length check
        
        instruction = splitted_data[2].strip()
        input_text = splitted_data[4].strip()
        input_text = "" if input_text.lower() == "<noinput>" else input_text
        output = splitted_data[6].strip()
        
        if len(instruction.split()) <= 3 or len(instruction.split()) > 150:
            continue
        
        blacklist = [
            "image", "images", "graph", "graphs", "picture", "pictures", 
            "file", "files", "map", "maps", "draw", "plot", "go to", 
            "video", "audio", "music", "flowchart", "diagram"
        ]
        
        if any(word in instruction.lower() for word in blacklist):
            continue
        
        if instruction.startswith(("Write a program", "Generate code")):
            continue
        
        if instruction[0] in string.punctuation or not instruction[0].isascii():
            continue
        
        instructions.append({"instruction": instruction, "input": input_text, "output": output})
    
    return instructions

def generate_instruction_following_data(
    output_dir="./generated_data",
    seed_tasks_path="C:/data/seed_tasks.jsonl",
    num_instructions_to_generate=500,
    num_prompt_instructions=3,
    request_batch_size=5,
    temperature=0.7,
    top_p=1.0
):
    try:
        seed_tasks = [json.loads(l) for l in open(seed_tasks_path, "r", encoding="utf-8")]
        seed_instruction_data = [
            {"instruction": t["instruction"], "input": t["instances"][0]["input"], "output": t["instances"][0]["output"]}
            for t in seed_tasks
        ]
        print(f"Loaded {len(seed_instruction_data)} human-written seed instructions")

        os.makedirs(output_dir, exist_ok=True)
        machine_instruction_data = []

        progress_bar = tqdm.tqdm(total=num_instructions_to_generate)

        while len(machine_instruction_data) < num_instructions_to_generate:
            batch_inputs = []
            for _ in range(request_batch_size):
                prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
                prompt = encode_prompt(prompt_instructions)
                batch_inputs.append(prompt)

            results = openai_completion(batch_inputs, batch_size=request_batch_size)

            for result in results:
                if result and "text" in result:
                    new_instructions = post_process_gpt3_response(num_prompt_instructions, result)
                    machine_instruction_data.extend(new_instructions)
                    progress_bar.update(len(new_instructions))

        with open(os.path.join(output_dir, "regen.json"), "w", encoding="utf-8") as f:
            json.dump(machine_instruction_data, f)

        print("Instruction generation completed.")
    
    except Exception as e:
        print(f"Error during instruction generation: {e}")

# Run the instruction generation
generate_instruction_following_data(
    output_dir="./generated_data",
    seed_tasks_path="C:/data/seed_tasks.jsonl",
    num_instructions_to_generate=100,
    num_prompt_instructions=3,
    request_batch_size=5
)

# Validate and preview the output
output_file = "./generated_data/regen.json"

with open(output_file, "r", encoding="utf-8") as f:
    generated_data = json.load(f)

for data in generated_data[:10]:
    print(f"Instruction: {data['instruction']}")
    print(f"Input: {data['input']}")
    print(f"Output: {data['output']}\n")










import os
import json
import random
import re
import string
import pandas as pd
import numpy as np
import tqdm
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage
from rouge_score import rouge_scorer

# Azure OpenAI Configuration
azure_openai_api_version = "2024-06-01-preview"
azure_openai_endpoint = "https://<your-azure-openai-endpoint>.openai.azure.com/"
azure_openai_api_key = "<your-azure-api-key>"

# Initialize Azure GPT-4 Model using Langchain's AzureChatOpenAI
llm = AzureChatOpenAI(
    temperature=0.2,  # Lower temperature for more deterministic outputs
    api_version=azure_openai_api_version,
    openai_api_base=azure_openai_endpoint,
    openai_api_key=azure_openai_api_key,
    deployment_name="ssgpt-40", 
    max_tokens=3072
)
# Load the dataset from Excel
df = pd.read_excel("C:/data/your_dataset.xlsx")

# Define diverse task types for instruction generation
task_types = [
    "Answer the following question",
    "Summarize the following information",
    "Classify the sentiment of the statement",
    "Translate the following text",
    "Provide a short description",
    "Generate a list of key points",
    "Rewrite in a formal tone",
    "Identify the main idea",
    "Provide a Yes or No answer",
    "Explain the concept in simple terms"
]

# Prepare data in the expected format for seed tasks
seed_data = []
for idx, row in df.iterrows():
    input_text = row['question'] if pd.notna(row['question']) else ""
    output_text = row['answer'] if pd.notna(row['answer']) else ""

    # Randomly select a task type to increase instruction diversity
    instruction_type = random.choice(task_types)
    instruction = f"{instruction_type}: {input_text}" if input_text else instruction_type
    
    # Construct the task with diverse instructions
    task = {
        "id": f"seed_task_{idx}",
        "name": f"SOP_{idx}",
        "instruction": instruction,
        "instances": [{
            "input": input_text,
            "output": output_text
        }],
        "is_classification": "Yes" in output_text or "No" in output_text
    }
    seed_data.append(task)

# Save the prepared data as a JSONL file
seed_tasks_path = "C:/data/seed_tasks.jsonl"
with open(seed_tasks_path, 'w', encoding="utf-8") as f:
    for task in seed_data:
        f.write(json.dumps(task) + '\n')

print(f"Seed tasks saved at: {seed_tasks_path}")
def openai_completion(prompts, batch_size=5):
    responses = []
    for prompt in prompts:
        response = llm([HumanMessage(content=prompt)])
        responses.append({
            "text": response.content,
            "finish_reason": "stop"
        })
    return responses


def encode_prompt(prompt_instructions):
    prompt = "Generate diverse task instructions for a GPT model. Instructions should vary in task type and utilize inputs effectively where applicable.\n"
    
    for idx, task_dict in enumerate(prompt_instructions):
        instruction, input_text, output = task_dict["instruction"], task_dict["input"], task_dict["output"]
        input_text = "<noinput>" if input_text.lower() == "" else input_text
        
        prompt += f"###\n"
        prompt += f"{idx + 1}. Instruction: {instruction}\n"
        prompt += f"{idx + 1}. Input:\n{input_text}\n"
        prompt += f"{idx + 1}. Output:\n{output}\n"
    
    prompt += f"###\n"
    prompt += f"{idx + 2}. Instruction:"
    return prompt


def post_process_gpt3_response(num_prompt_instructions, response):
    if response is None:
        return []
    
    raw_instructions = f"{num_prompt_instructions+1}. Instruction:" + response["text"]
    raw_instructions = re.split("###", raw_instructions)
    instructions = []
    
    for idx, inst in enumerate(raw_instructions):
        if idx == len(raw_instructions) - 1 and response["finish_reason"] == "length":
            continue
        
        idx += num_prompt_instructions + 1
        splitted_data = re.split(f"{idx}\.\s+(Instruction|Input|Output):", inst)
        
        if len(splitted_data) != 7:
            continue
        else:
            instruction = splitted_data[2].strip()
            input_text = splitted_data[4].strip()
            output = splitted_data[6].strip()
        
        if not input_text:
            input_text = "<noinput>"
        
        instructions.append({"instruction": instruction, "input": input_text, "output": output})
    
    return instructions

def generate_instruction_following_data(
    output_dir="./generated_data",
    seed_tasks_path="C:/data/seed_tasks.jsonl",
    num_instructions_to_generate=500, 
    num_prompt_instructions=3,
    request_batch_size=5
):
    seed_tasks = [json.loads(l) for l in open(seed_tasks_path, "r", encoding="utf-8")]
    seed_instruction_data = [
        {"instruction": t["instruction"], "input": t["instances"][0]["input"], "output": t["instances"][0]["output"]}
        for t in seed_tasks
    ]

    os.makedirs(output_dir, exist_ok=True)
    machine_instruction_data = []

    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)

    while len(machine_instruction_data) < num_instructions_to_generate:
        batch_inputs = []
        for _ in range(request_batch_size):
            prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
            prompt = encode_prompt(prompt_instructions)
            batch_inputs.append(prompt)

        results = openai_completion(batch_inputs, batch_size=request_batch_size)

        for result in results:
            if result and "text" in result:
                new_instructions = post_process_gpt3_response(num_prompt_instructions, result)
                machine_instruction_data.extend(new_instructions)
                progress_bar.update(len(new_instructions))

        with open(os.path.join(output_dir, "regen.json"), "w", encoding="utf-8") as f:
            json.dump(machine_instruction_data, f)

    print("Instruction generation completed.")













import pandas as pd
import json
import random

# Load the dataset from Excel
df = pd.read_excel("C:/data/your_dataset.xlsx")

# Define potential task types for more diverse instruction generation
task_types = [
    "Answer the following question",
    "Summarize the following information",
    "Classify the sentiment of the statement",
    "Translate the following text",
    "Provide a short description",
    "Generate a list of key points",
    "Rewrite in a formal tone",
    "Identify the main idea",
    "Provide a Yes or No answer",
    "Explain the concept in simple terms"
]

# Prepare data in the expected format for seed tasks
seed_data = []
for idx, row in df.iterrows():
    input_text = row['question'] if pd.notna(row['question']) else ""
    output_text = row['answer'] if pd.notna(row['answer']) else ""

    # Randomly select a task type to increase instruction diversity
    instruction = random.choice(task_types)
    
    # Construct the task with diverse instructions
    task = {
        "id": f"seed_task_{idx}",
        "name": f"SOP_{idx}",
        "instruction": f"{instruction}: {input_text}",
        "instances": [{
            "input": input_text,
            "output": output_text
        }],
        "is_classification": False
    }
    seed_data.append(task)

# Save the prepared data as a JSONL file
seed_tasks_path = "C:/data/seed_tasks.jsonl"
with open(seed_tasks_path, 'w', encoding="utf-8") as f:
    for task in seed_data:
        f.write(json.dumps(task) + '\n')

print(f"Seed tasks saved at: {seed_tasks_path}")

def encode_prompt(prompt_instructions):
    prompt = "You are asked to generate diverse task instructions for a GPT model. The instructions must vary in task type and utilize inputs effectively where applicable.\n"
    
    for idx, task_dict in enumerate(prompt_instructions):
        instruction, input_text, output = task_dict["instruction"], task_dict["input"], task_dict["output"]
        input_text = "<noinput>" if input_text.lower() == "" else input_text
        
        prompt += f"###\n"
        prompt += f"{idx + 1}. Instruction: {instruction}\n"
        prompt += f"{idx + 1}. Input:\n{input_text}\n"
        prompt += f"{idx + 1}. Output:\n{output}\n"
    
    prompt += f"###\n"
    prompt += f"{idx + 2}. Instruction:"
    return prompt

def generate_instruction_following_data(
    output_dir="./generated_data",
    seed_tasks_path="C:/data/seed_tasks.jsonl",
    num_instructions_to_generate=500, 
    num_prompt_instructions=3,
    request_batch_size=5
):
    seed_tasks = [json.loads(l) for l in open(seed_tasks_path, "r", encoding="utf-8")]
    seed_instruction_data = [
        {"instruction": t["instruction"], "input": t["instances"][0]["input"], "output": t["instances"][0]["output"]}
        for t in seed_tasks
    ]
    print(f"Loaded {len(seed_instruction_data)} diverse seed instructions")

    os.makedirs(output_dir, exist_ok=True)
    machine_instruction_data = []

    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)

    while len(machine_instruction_data) < num_instructions_to_generate:
        batch_inputs = []
        for _ in range(request_batch_size):
            prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
            prompt = encode_prompt(prompt_instructions)
            batch_inputs.append(prompt)

        results = openai_completion(batch_inputs, batch_size=request_batch_size)

        for result in results:
            if result and "text" in result:
                new_instructions = post_process_gpt3_response(num_prompt_instructions, result)
                machine_instruction_data.extend(new_instructions)
                progress_bar.update(len(new_instructions))

        with open(os.path.join(output_dir, "regen.json"), "w", encoding="utf-8") as f:
            json.dump(machine_instruction_data, f)

    print("Instruction generation completed.")







import pandas as pd
import json

# Load the dataset from Excel
df = pd.read_excel("C:/data/your_dataset.xlsx")

# Prepare data in the expected format for seed tasks
seed_data = []
for idx, row in df.iterrows():
    task = {
        "id": f"seed_task_{idx}",
        "name": f"factual_query_{idx}",
        "instruction": "Factual",
        "instances": [{
            "input": row['question'],
            "output": row['answer']
        }],
        "is_classification": False
    }
    seed_data.append(task)

# Save the prepared data as a JSONL file
seed_tasks_path = "C:/data/seed_tasks.jsonl"
with open(seed_tasks_path, 'w', encoding="utf-8") as f:
    for task in seed_data:
        f.write(json.dumps(task) + '\n')

print(f"Seed tasks saved at: {seed_tasks_path}")







def openai_completion(prompts, batch_size=5):
    responses = []
    for prompt in prompts:
        response = llm([HumanMessage(content=prompt)])
        responses.append({
            "text": response.content,
            "finish_reason": "stop"
        })
    return responses


def encode_prompt(prompt_instructions):
    prompt = "You are asked to generate diverse task instructions for a GPT model.\n"
    for idx, task_dict in enumerate(prompt_instructions):
        instruction, input_text, output = task_dict["instruction"], task_dict["input"], task_dict["output"]
        instruction = re.sub(r"\s+", " ", instruction).strip().rstrip(":")
        input_text = "<noinput>" if input_text.lower() == "" else input_text
        prompt += f"###\n"
        prompt += f"{idx + 1}. Instruction: {instruction}\n"
        prompt += f"{idx + 1}. Input:\n{input_text}\n"
        prompt += f"{idx + 1}. Output:\n{output}\n"
    prompt += f"###\n"
    prompt += f"{idx + 2}. Instruction:"
    return prompt


def post_process_gpt3_response(num_prompt_instructions, response):
    if response is None:
        return []
    
    raw_instructions = f"{num_prompt_instructions+1}. Instruction:" + response["text"]
    raw_instructions = re.split("###", raw_instructions)
    instructions = []
    
    for idx, inst in enumerate(raw_instructions):
        if idx == len(raw_instructions) - 1 and response["finish_reason"] == "length":
            continue
        
        idx += num_prompt_instructions + 1
        splitted_data = re.split(f"{idx}\.\s+(Instruction|Input|Output):", inst)
        
        if len(splitted_data) != 7:
            continue
        else:
            instruction = splitted_data[2].strip()
            input_text = splitted_data[4].strip()
            input_text = "<noinput>" if input_text.lower() == "" else input_text
            output = splitted_data[6].strip()
        
        if len(instruction.split()) <= 3 or len(instruction.split()) > 150:
            continue

        if not input_text:
            input_text = "<noinput>"
        
        # Debugging output to verify correct input-output matching
        print(f"Parsed Instruction: {instruction}")
        print(f"Parsed Input: {input_text}")
        print(f"Parsed Output: {output}\n")

        instructions.append({"instruction": instruction, "input": input_text, "output": output})
    
    return instructions

def generate_instruction_following_data(
    output_dir="./generated_data",
    seed_tasks_path="C:/data/seed_tasks.jsonl",
    num_instructions_to_generate=100, 
    num_prompt_instructions=3,
    request_batch_size=5
):
    seed_tasks = [json.loads(l) for l in open(seed_tasks_path, "r", encoding="utf-8")]
    seed_instruction_data = [
        {"instruction": t["instruction"], "input": t["instances"][0]["input"], "output": t["instances"][0]["output"]}
        for t in seed_tasks
    ]
    print(f"Loaded {len(seed_instruction_data)} human-written seed instructions")

    os.makedirs(output_dir, exist_ok=True)
    machine_instruction_data = []

    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)

    while len(machine_instruction_data) < num_instructions_to_generate:
        batch_inputs = []
        for _ in range(request_batch_size):
            prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
            prompt = encode_prompt(prompt_instructions)
            batch_inputs.append(prompt)

        results = openai_completion(batch_inputs, batch_size=request_batch_size)

        for result in results:
            if result and "text" in result:
                new_instructions = post_process_gpt3_response(num_prompt_instructions, result)
                machine_instruction_data.extend(new_instructions)
                progress_bar.update(len(new_instructions))

        with open(os.path.join(output_dir, "regen.json"), "w", encoding="utf-8") as f:
            json.dump(machine_instruction_data, f)

    print("Instruction generation completed.")








def generate_instruction_following_data(
    output_dir="./generated_data",
    seed_tasks_path="C:/data/seed_tasks.jsonl",
    num_instructions_to_generate=52000,
    num_prompt_instructions=3,
    request_batch_size=5,
    temperature=0.7,
    top_p=1.0
):
    # Load seed tasks
    seed_tasks = [json.loads(l) for l in open(seed_tasks_path, "r", encoding="utf-8")]
    seed_instruction_data = [
        {"instruction": t["instruction"], "input": t["instances"][0]["input"], "output": t["instances"][0]["output"]}
        for t in seed_tasks
    ]
    print(f"Loaded {len(seed_instruction_data)} human-written seed instructions")

    os.makedirs(output_dir, exist_ok=True)
    machine_instruction_data = []

    if os.path.exists(os.path.join(output_dir, "regen.json")):
        with open(os.path.join(output_dir, "regen.json"), "r", encoding="utf-8") as f:
            machine_instruction_data = json.load(f)
        print(f"Loaded {len(machine_instruction_data)} machine-generated instructions")

    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)
    if machine_instruction_data:
        progress_bar.update(len(machine_instruction_data))

    while len(machine_instruction_data) < num_instructions_to_generate:
        batch_inputs = []
        for _ in range(request_batch_size):
            prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
            prompt = encode_prompt(prompt_instructions)
            batch_inputs.append(prompt)

        results = openai_completion(batch_inputs, batch_size=request_batch_size)

        for result in results:
            if result and "text" in result:
                new_instructions = post_process_gpt3_response(num_prompt_instructions, result)
                machine_instruction_data.extend(new_instructions)
                progress_bar.update(len(new_instructions))

        with open(os.path.join(output_dir, "regen.json"), "w", encoding="utf-8") as f:
            json.dump(machine_instruction_data, f)

        print(f"Generated {len(machine_instruction_data)} instructions so far.")

    print("Instruction generation completed.")








def post_process_gpt3_response(num_prompt_instructions, response):
    if response is None:
        return []
    raw_instructions = f"{num_prompt_instructions+1}. Instruction:" + response["text"]
    raw_instructions = re.split("###", raw_instructions)
    instructions = []
    
    for idx, inst in enumerate(raw_instructions):
        if idx == len(raw_instructions) - 1 and response["finish_reason"] == "length":
            continue
        
        idx += num_prompt_instructions + 1
        splitted_data = re.split(f"{idx}\.\s+(Instruction|Input|Output):", inst)
        
        if len(splitted_data) != 7:
            continue
        else:
            instruction = splitted_data[2].strip()
            input_text = splitted_data[4].strip()
            input_text = "" if input_text.lower() == "<noinput>" else input_text
            output = splitted_data[6].strip()
            
        if len(instruction.split()) <= 3 or len(instruction.split()) > 150:
            continue

        # Adding a check for empty input
        if not input_text:
            input_text = "<noinput>"
        
        instructions.append({"instruction": instruction, "input": input_text, "output": output})
    
    return instructions






generate_instruction_following_data(
    output_dir="./generated_data",
    seed_tasks_path="seed_tasks.jsonl",
    num_instructions_to_generate=52000,
    num_prompt_instructions=3,
    request_batch_size=5
)








def encode_prompt(prompt_instructions):
    prompt = open("./prompt.txt").read() + "\n"
    for idx, task_dict in enumerate(prompt_instructions):
        (instruction, input_text, output) = task_dict["instruction"], task_dict["input"], task_dict["output"]
        instruction = re.sub(r"\s+", " ", instruction).strip().rstrip(":")
        input_text = "<noinput>" if input_text.lower() == "" else input_text
        prompt += f"###\n"
        prompt += f"{idx + 1}. Instruction: {instruction}\n"
        prompt += f"{idx + 1}. Input:\n{input_text}\n"
        prompt += f"{idx + 1}. Output:\n{output}\n"
    prompt += f"###\n"
    prompt += f"{idx + 2}. Instruction:"
    return prompt

def generate_instruction_following_data(
    output_dir="./",
    seed_tasks_path="./seed_tasks.jsonl",
    num_instructions_to_generate=52000,
    num_prompt_instructions=3,
    request_batch_size=5,
    temperature=0.7,
    top_p=1.0,
    num_cpus=16,
):
    # Load seed tasks
    seed_tasks = [json.loads(l) for l in open(seed_tasks_path, "r")]
    seed_instruction_data = [
        {"instruction": t["instruction"], "input": t["instances"][0]["input"], "output": t["instances"][0]["output"]}
        for t in seed_tasks
    ]
    print(f"Loaded {len(seed_instruction_data)} human-written seed instructions")

    os.makedirs(output_dir, exist_ok=True)
    machine_instruction_data = []

    if os.path.exists(os.path.join(output_dir, "regen.json")):
        with open(os.path.join(output_dir, "regen.json"), "r") as f:
            machine_instruction_data = json.load(f)
        print(f"Loaded {len(machine_instruction_data)} machine-generated instructions")

    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)
    if machine_instruction_data:
        progress_bar.update(len(machine_instruction_data))

    while len(machine_instruction_data) < num_instructions_to_generate:
        batch_inputs = []
        for _ in range(request_batch_size):
            prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
            prompt = encode_prompt(prompt_instructions)
            batch_inputs.append(prompt)

        results = openai_completion(batch_inputs, batch_size=request_batch_size)

        for result in results:
            if result and "text" in result:
                new_instructions = result["text"].split("###")
                for inst in new_instructions:
                    if len(inst.strip()) > 0:
                        machine_instruction_data.append({"instruction": inst.strip(), "input": "<noinput>", "output": ""})
                        progress_bar.update(1)

        # Save intermediate results
        with open(os.path.join(output_dir, "regen.json"), "w") as f:
            json.dump(machine_instruction_data, f)

        print(f"Generated {len(machine_instruction_data)} instructions so far.")

    print("Instruction generation completed.")







def openai_completion(prompts, batch_size=5, stop_sequences=None):
    responses = []
    for prompt in prompts:
        response = llm([HumanMessage(content=prompt)])
        responses.append({
            "text": response.content,
            "finish_reason": "stop"
        })
    return responses






import os
import json
import time
import random
import re
import string
from multiprocessing import Pool
from functools import partial

import numpy as np
import tqdm
from rouge_score import rouge_scorer
import openai
import pandas as pd

# Set up the Azure OpenAI configuration
openai.api_type = "azure"
openai.api_base = "https://<your-azure-openai-endpoint>.openai.azure.com/"
openai.api_version = "2024-06-01-preview"  
openai.api_key = "<your-azure-api-key>"

# Load the dataset from Excel
df = pd.read_excel("your_dataset.xlsx")

# Prepare data in the expected format for seed tasks
seed_data = []
for _, row in df.iterrows():
    instruction = row['question_type']
    input_text = row['question']
    output = row['answer']
    
    task = {
        "instruction": instruction,
        "instances": [{"input": input_text, "output": output}]
    }
    seed_data.append(task)

# Save the prepared data as a JSONL file
with open('seed_tasks.jsonl', 'w') as f:
    for task in seed_data:
        f.write(json.dumps(task) + '\n')


def openai_completion(prompts, model_name, batch_size, decoding_args, logit_bias=None):
    responses = []
    for prompt in prompts:
        response = openai.ChatCompletion.create(
            engine=model_name,
            messages=[{"role": "system", "content": prompt}],
            max_tokens=decoding_args['max_tokens'],
            temperature=decoding_args['temperature'],
            top_p=decoding_args['top_p'],
            stop=decoding_args['stop'],
            logit_bias=logit_bias
        )
        responses.append({
            "text": response.choices[0].message['content'],
            "finish_reason": response.choices[0].finish_reason
        })
    return responses


def encode_prompt(prompt_instructions):
    prompt = open("./prompt.txt").read() + "\n"
    for idx, task_dict in enumerate(prompt_instructions):
        (instruction, input_text, output) = task_dict["instruction"], task_dict["input"], task_dict["output"]
        instruction = re.sub(r"\s+", " ", instruction).strip().rstrip(":")
        input_text = "<noinput>" if input_text.lower() == "" else input_text
        prompt += f"###\n"
        prompt += f"{idx + 1}. Instruction: {instruction}\n"
        prompt += f"{idx + 1}. Input:\n{input_text}\n"
        prompt += f"{idx + 1}. Output:\n{output}\n"
    prompt += f"###\n"
    prompt += f"{idx + 2}. Instruction:"
    return prompt


def generate_instruction_following_data(
    output_dir="./",
    seed_tasks_path="./seed_tasks.jsonl",
    num_instructions_to_generate=52000,
    model_name="gpt-4",
    num_prompt_instructions=3,
    request_batch_size=10,
    temperature=1.0,
    top_p=1.0,
    num_cpus=16,
):

    # Load seed tasks
    seed_tasks = [json.loads(l) for l in open(seed_tasks_path, "r")]
    seed_instruction_data = [
        {"instruction": t["instruction"], "input": t["instances"][0]["input"], "output": t["instances"][0]["output"]}
        for t in seed_tasks
    ]
    print(f"Loaded {len(seed_instruction_data)} human-written seed instructions")

    os.makedirs(output_dir, exist_ok=True)
    machine_instruction_data = []

    if os.path.exists(os.path.join(output_dir, "regen.json")):
        with open(os.path.join(output_dir, "regen.json"), "r") as f:
            machine_instruction_data = json.load(f)
        print(f"Loaded {len(machine_instruction_data)} machine-generated instructions")

    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)
    if machine_instruction_data:
        progress_bar.update(len(machine_instruction_data))

    all_instructions = [d["instruction"] for d in seed_instruction_data] + [
        d["instruction"] for d in machine_instruction_data
    ]
    
    while len(machine_instruction_data) < num_instructions_to_generate:
        batch_inputs = []
        for _ in range(request_batch_size):
            prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)
            prompt = encode_prompt(prompt_instructions)
            batch_inputs.append(prompt)

        decoding_args = {
            "temperature": temperature,
            "max_tokens": 3072,
            "top_p": top_p,
            "stop": ["\n20", "20.", "20."]
        }

        results = openai_completion(
            prompts=batch_inputs,
            model_name=model_name,
            batch_size=request_batch_size,
            decoding_args=decoding_args,
            logit_bias={"50256": -100},
        )

        for result in results:
            if result and "text" in result:
                new_instructions = result["text"].split("###")
                for inst in new_instructions:
                    if len(inst.strip()) > 0:
                        machine_instruction_data.append({"instruction": inst.strip(), "input": "<noinput>", "output": ""})
                        progress_bar.update(1)

        # Save intermediate results
        with open(os.path.join(output_dir, "regen.json"), "w") as f:
            json.dump(machine_instruction_data, f)

        print(f"Generated {len(machine_instruction_data)} instructions so far.")

    print("Instruction generation completed.")











import random
from textwrap import dedent
from typing import Dict, List
import sys
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from datasets import Dataset, DatasetDict, load_dataset
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    pipeline,
)
from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer
from peft import (
    LoraConfig,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training,
)
import mlflow

# Set seed for reproducibility
SEED = 42
def seed_everything(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
seed_everything(SEED)

# Define model and tokenizer
MODEL_NAME = "/Volumes/dev2_catalog_01/default/txt-wtf-pe/hf/Mistral-7B-Instruct-v0.2-quantised/"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# Load and prepare the model
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
)
model.resize_token_embeddings(len(tokenizer))

# Load dataset
df = pd.read_csv("/Volumes/dev2_catalog_01/default/txt/wtf-pe/data/consolidated_results_v3.csv")
df = df[["Question", "Answer"]]

# Format prompts
def formatting_prompts_func(example: Dict) -> str:
    return f"### Question: {example['Question']}\n ### Answer: {example['Answer']}"

df["text"] = df.apply(formatting_prompts_func, axis=1)

# Split dataset
train, temp = train_test_split(df, test_size=0.2, random_state=SEED)
val, test = train_test_split(temp, test_size=0.2, random_state=SEED)

# Convert to Hugging Face Dataset
dataset_train = Dataset.from_pandas(train)
dataset_eval = Dataset.from_pandas(val)
dataset_test = Dataset.from_pandas(test)

dataset = DatasetDict({
    "train": dataset_train,
    "validation": dataset_eval,
    "test": dataset_test
})

# Create data collator
response_template = " ### Answer:"
collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)

# LoRA Configuration
lora_config = LoraConfig(
    r=64,
    lora_alpha=128,
    target_modules=["all-linear"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

# SFT Configuration
OUTPUT_DIR = "/Volumes/dev2_catalog_01/default/txt-wtf-pe/training/Mistral-7B-Instruct-v0.2-instruction-v5/fine-tuned-model"
sft_config = SFTConfig(
    output_dir=OUTPUT_DIR,
    dataset_text_field="text",
    max_seq_length=2048,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    optim="paged_adamw_8bit",
    eval_strategy="steps",
    logging_steps=10,
    learning_rate=1e-4,
    fp16=True,
    save_strategy="steps",
    evaluation_strategy="steps",
    eval_steps=10,
    save_steps=50,
    warmup_ratio=0.1,
    save_total_limit=3,
    report_to=["tensorboard"],
    save_safetensors=True,
)

# Initialize Trainer
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    data_collator=collator,
)

# Training
trainer.train()
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

# Evaluation
eval_result = trainer.evaluate()
print("Evaluation Result:", eval_result)










import json
import re

def extract_valid_json(response):
    """Extract and clean malformed JSON from an LLM response."""
    
    try:
        # Find the JSON-like structure
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start == -1:
            print("Warning: No JSON structure detected.")
            return []

        if json_end == -1:
            response += ']'  # Fix missing closing bracket
            json_end = len(response) - 1

        json_text = response[json_start:json_end + 1]

        # Remove unwanted ASCII control characters
        json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)

        # Fix missing commas between objects
        json_text = re.sub(r'(?<=\})(?=\s*\{)', '},', json_text)

        # Ensure JSON is wrapped correctly (fixes trailing commas)
        json_text = json_text.strip().rstrip(',')
        
        # Fix missing brackets in objects
        json_text = re.sub(r'(?<=\w)\s*{\s*', '{', json_text)  # Fix missing {
        json_text = re.sub(r'\s*}\s*(?=\w)', '}', json_text)  # Fix missing }

        # Extract JSON objects inside the array
        json_objects = re.findall(r'\{.*?\}', json_text, re.DOTALL)

        cleaned_objects = []

        for obj in json_objects:
            try:
                parsed_obj = json.loads(obj.strip())

                # Ensure both "Question" and "Answer" exist and are not empty
                if parsed_obj.get("Question", "").strip() and parsed_obj.get("Answer", "").strip():
                    cleaned_objects.append(parsed_obj)

            except json.JSONDecodeError:
                continue  # Ignore broken objects

        return cleaned_objects

    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []

# Sample malformed JSON response
response = """[
{
    "Question_type": "Procedural",
    "Question": "What is the first step in the VFMC Non Receipt and Unadvised Cash procedure?",
    "Answer": "Step 1: Check the unadvised cash in mail"
}, 
{
    "Question_type": "Procedural",
    "Question": "What systems are used in the VFMC Non Receipt and Unadvised Cash procedure?",
    "Answer": "Outlook"
}
"""

# Test the function
valid_json = extract_valid_json(response)
print(json.dumps(valid_json, indent=2))








import pandas as pd
import time
from langchain.chat_models import AzureChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Azure OpenAI Configuration (Replace with your actual values)
azure_openai_api_version = "your_api_version_here"
azure_openai_endpoint = "your_azure_endpoint_here"
deployment_name = "your_deployment_name_here"
api_key = "your_api_key_here"

# Initialize Azure OpenAI LLM
llm = AzureChatOpenAI(
    temperature=0.9,  # High temperature for diverse outputs
    api_version=azure_openai_api_version,
    azure_endpoint=azure_openai_endpoint,
    deployment_name=deployment_name,
    openai_api_key=api_key
)

# Define prompt template
prompt_text = """
Generate {num_questions} unique, unrelated, and confusing questions that indicate uncertainty or a lack of awareness.

Each question should be in the format:
Q: [Question]

Example Questions:
- Q: I am not aware of that, can you explain?
- Q: Sorry, I have no clue about that.
- Q: Do you think anyone really knows the answer?
- Q: What if I don't know anything at all?
- Q: Is it possible to not know anything?
- Q: What does it mean to be unaware?

Ensure each question is different and adds variety to vague, uncertain, or confusing statements.
"""

# Create LLMChain with the prompt
prompt_template = PromptTemplate(template=prompt_text, input_variables=["num_questions"])
llm_chain = LLMChain(llm=llm, prompt=prompt_template)

# Function to generate questions in batches
def generate_questions(batch_size, total_questions):
    questions_list = []
    
    for i in range(total_questions // batch_size):
        print(f"Generating batch {i + 1} of {total_questions // batch_size}...")
        response_text = llm_chain.predict(num_questions=str(batch_size))

        # Ensure response is in text format
        if not isinstance(response_text, str):
            response_text = str(response_text)

        # Extract questions
        for line in response_text.split("\n"):
            if line.startswith("Q: "):
                question = line.replace("Q: ", "").strip()
                questions_list.append([question])

        # Wait a bit to avoid API rate limits
        time.sleep(2)
    
    return questions_list

# Generate 2000 questions in batches of 200
batch_size = 200
total_questions = 2000
questions_data = generate_questions(batch_size, total_questions)

# Convert to DataFrame
df = pd.DataFrame(questions_data, columns=["Question"])

# Save to CSV
file_path = "unrelated_confusing_questions.csv"
df.to_csv(file_path, index=False)

print(f"Dataset saved as {file_path} with {len(df)} questions.")








import pandas as pd
import time
from langchain.chat_models import AzureChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Azure OpenAI Configuration (Replace with your actual values)
azure_openai_api_version = "your_api_version_here"
azure_openai_endpoint = "your_azure_endpoint_here"
deployment_name = "your_deployment_name_here"
api_key = "your_api_key_here"

# Initialize Azure OpenAI LLM
llm = AzureChatOpenAI(
    temperature=0.7,  # Adjust temperature for diversity
    api_version=azure_openai_api_version,
    azure_endpoint=azure_openai_endpoint,
    deployment_name=deployment_name,
    openai_api_key=api_key
)

# Define prompt template
prompt_text = """
Generate {num_questions} random, unrelated questions and answers across multiple topics.

Ensure that questions are diverse and cover the following categories:
- Weather
- Sports
- Science
- Technology
- History
- Entertainment
- General Knowledge
- Fun Facts
- Geography
- Mathematics

Provide each question-answer pair in the following format:
Q: [Question]
A: [Answer]

Ensure each Q&A pair is separated by a double newline.
"""

# Create LLMChain with the prompt
prompt_template = PromptTemplate(template=prompt_text, input_variables=["num_questions"])
llm_chain = LLMChain(llm=llm, prompt=prompt_template)

# Function to generate questions in batches
def generate_questions(batch_size, total_questions):
    questions_list = []
    
    for i in range(total_questions // batch_size):
        print(f"Generating batch {i + 1} of {total_questions // batch_size}...")
        response_text = llm_chain.predict(num_questions=str(batch_size))

        # Ensure response is in text format
        if not isinstance(response_text, str):
            response_text = str(response_text)

        # Extract questions and answers
        for qa in response_text.split("\n\n"):  # Assuming Q&A pairs are separated by double newlines
            if "Q:" in qa and "A:" in qa:
                parts = qa.split("\n")
                if len(parts) == 2:
                    q = parts[0].replace("Q: ", "").strip()
                    a = parts[1].replace("A: ", "").strip()
                    questions_list.append([q, a])

        # Wait a bit to avoid API rate limits
        time.sleep(2)
    
    return questions_list

# Generate 2000 questions in batches of 200
batch_size = 200
total_questions = 2000
questions_data = generate_questions(batch_size, total_questions)

# Convert to DataFrame
df = pd.DataFrame(questions_data, columns=["Question", "Answer"])

# Save to CSV
file_path = "unrelated_questions.csv"
df.to_csv(file_path, index=False)

print(f"Dataset saved as {file_path} with {len(df)} questions.")








import pandas as pd
from langchain.chat_models import AzureChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Azure OpenAI Configuration (Replace with your actual values)
azure_openai_api_version = "your_api_version_here"
azure_openai_endpoint = "your_azure_endpoint_here"
deployment_name = "your_deployment_name_here"
api_key = "your_api_key_here"

# Initialize Azure OpenAI LLM
llm = AzureChatOpenAI(
    temperature=0.7,  # Adjust temperature for diversity
    api_version=azure_openai_api_version,
    azure_endpoint=azure_openai_endpoint,
    deployment_name=deployment_name,
    openai_api_key=api_key
)

# Define prompt template
prompt_text = """
Generate {num_questions} random, unrelated questions and answers across multiple topics.

Ensure that questions are diverse and cover the following categories:
- Weather
- Sports
- Science
- Technology
- History
- Entertainment
- General Knowledge
- Fun Facts
- Geography
- Mathematics

Provide each question-answer pair in the following format:
Q: [Question]
A: [Answer]
"""

# Create LLMChain with the prompt
prompt_template = PromptTemplate(template=prompt_text, input_variables=["num_questions"])
llm_chain = LLMChain(llm=llm, prompt=prompt_template)

# Generate questions
num_questions = 2000  # Modify as needed
response_text = llm_chain.predict(num_questions=str(num_questions))  # Predict returns a string

# Ensure response is in text format
if not isinstance(response_text, str):
    response_text = str(response_text)

# Extract questions and answers
questions_list = []
for qa in response_text.split("\n\n"):  # Assuming Q&A pairs are separated by double newlines
    if "Q:" in qa and "A:" in qa:
        parts = qa.split("\n")
        if len(parts) == 2:
            q = parts[0].replace("Q: ", "").strip()
            a = parts[1].replace("A: ", "").strip()
            questions_list.append([q, a])

# Convert to DataFrame
df = pd.DataFrame(questions_list, columns=["Question", "Answer"])

# Save to CSV
file_path = "unrelated_questions.csv"
df.to_csv(file_path, index=False)

print(f"Dataset saved as {file_path}")








import openai
import pandas as pd

# OpenAI API Key (Ensure to use your own API Key)
openai.api_key = "your_api_key_here"

# Prompt to generate questions
prompt_text = """
Generate 2000 random, unrelated questions and answers across multiple topics.
Ensure that questions are diverse and cover the following categories:
- Weather
- Sports
- Science
- Technology
- History
- Entertainment
- General Knowledge
- Fun Facts
- Geography
- Mathematics

Provide each question-answer pair in the following format:
Q: [Question]
A: [Answer]
"""

# Generate response from GPT
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "system", "content": prompt_text}]
)

# Extract generated text
generated_text = response["choices"][0]["message"]["content"]

# Convert to DataFrame
questions_list = []
for qa in generated_text.split("\n\n"):  # Assuming Q&A pairs are separated by a double newline
    if "Q:" in qa and "A:" in qa:
        q, a = qa.split("\n")
        questions_list.append([q.replace("Q: ", ""), a.replace("A: ", "")])

df = pd.DataFrame(questions_list, columns=["Question", "Answer"])

# Save to CSV
file_path = "unrelated_questions.csv"
df.to_csv(file_path, index=False)

print(f"Dataset saved as {file_path}")







import pandas as pd

import random



# Define meaningful and diverse general questions without using numbers

general_questions = [

    "What is the importance of education?",

    "What are the benefits of meditation?",

    "What is the fastest animal on Earth?",

    "What are the symptoms of a common cold?",

    "What is the best programming language?",

    "What are the main causes of climate change?",

    "What is the tallest mountain in the world?",

    "What are the effects of pollution on health?",

    "What is the purpose of life insurance?",

    "What are the different types of renewable energy sources?",

    "When did humans first land on the moon?",

    "When does the sun rise in winter?",

    "When is the best time to plant trees?",

    "When was the first computer invented?",

    "When did World War II end?",

    "How does rain form?",

    "How does the internet work?",

    "How do airplanes stay in the air?",

    "How can I improve my memory?",

    "How does photosynthesis work?",

    "Why is the sky blue?",

    "Why do we need sleep?",

    "Why is exercise important for health?",

    "Why do birds fly south for the winter?",

    "Why do some people snore?",

    "Where is the Great Wall of China?",

    "Where can I see the Northern Lights?",

    "Where do penguins live?",

    "Where is Mount Everest located?",

    "Where can I buy fresh vegetables?"

]



# Expanding with more meaningful general questions

additional_general_questions = [

    "What are the benefits of a balanced diet?",

    "What is the significance of renewable energy?",

    "What are the effects of stress on mental health?",

    "What is the role of artificial intelligence in modern society?",

    "What are the different types of programming languages?",

    "When is the best time to travel to tropical countries?",

    "When do the Olympic Games occur?",

    "When is the right time to start saving for retirement?",

    "How do vaccines protect against diseases?",

    "How does machine learning improve decision-making?",

    "How does climate change affect global weather patterns?",

    "Why is biodiversity important for ecosystems?",

    "Why do some people have allergies?",

    "Why do humans need sleep?",

    "Why is physical fitness important for longevity?",

    "Where can I learn more about space exploration?",

    "Where do the most earthquakes occur worldwide?",

    "Where is the largest desert located?",

    "Where can I see wild tigers in their natural habitat?",

    "Where do the world's largest coral reefs exist?"

]



# Define meaningful and diverse positive questions

positive_questions = [

    "Is investing in stocks a good long-term strategy?",

    "Will artificial intelligence create more jobs in the future?",

    "Can meditation help reduce stress and anxiety?",

    "Is renewable energy the future of power generation?",

    "Do social media platforms impact mental health?",

    "Is cybersecurity important in today's digital world?",

    "Should governments focus more on climate change policies?",

    "Will electric vehicles replace gasoline-powered cars?",

    "Can space exploration lead to human settlements on other planets?",

    "Is technology making education more accessible?"

]



# Define function to convert positive questions into negative questions

def convert_to_negative(question):

    if question.lower().startswith(("is", "are", "can", "will", "do", "does", "should")):

        return "Isn't " + question[3:].strip() if question.startswith("Is") else \

               "Aren't " + question[4:].strip() if question.startswith("Are") else \

               "Can't " + question[3:].strip() if question.startswith("Can") else \

               "Won't " + question[4:].strip() if question.startswith("Will") else \

               "Don't " + question[3:].strip() if question.startswith("Do") else \

               "Doesn't " + question[4:].strip() if question.startswith("Does") else \

               "Shouldn't " + question[7:].strip() if question.startswith("Should") else \

               "Isn't it true that " + question.lower()

    return "Isn't it true that " + question.lower()



# Generate unique negative questions from positive questions

negative_questions = list(set([convert_to_negative(q) for q in positive_questions]))



# Combine all unique general, positive, and negative questions

all_questions = list(set(general_questions + additional_general_questions + positive_questions + negative_questions))

random.shuffle(all_questions)



# Create DataFrame with Question Type Labels

df_questions = pd.DataFrame({

    "QuestionType": ["General Question"] * len(general_questions + additional_general_questions) +

                    ["Positive Question"] * len(positive_questions) +

                    ["Negative Question"] * len(negative_questions),

    "Question": general_questions + additional_general_questions + positive_questions + negative_questions,

    "Answer": ["This is a general knowledge question."] * len(general_questions + additional_general_questions) +

              ["This is a positive statement."] * len(positive_questions) +

              ["I am not aware of that."] * len(negative_questions)

})



# Save to Excel

file_path = "/mnt/data/question_answer_dataset_meaningful.xlsx"

df_questions.to_excel(file_path, index=False)



# Display the file for the user to download

import ace_tools as tools

tools.display_dataframe_to_user(name="Meaningful Question-Answer Dataset (2000+)", dataframe=df_questions)



file_path








import pandas as pd

import random



# Define 500 Unique General Questions (What, When, How, Why, Where)

general_questions = [

    "What is the importance of education?",

    "What are the benefits of meditation?",

    "What is the fastest animal on Earth?",

    "What are the symptoms of a common cold?",

    "What is the best programming language?",

    "What are the main causes of climate change?",

    "What is the tallest mountain in the world?",

    "What are the effects of pollution on health?",

    "What is the purpose of life insurance?",

    "What are the different types of renewable energy sources?",

    "When did humans first land on the moon?",

    "When does the sun rise in winter?",

    "When is the best time to plant trees?",

    "When was the first computer invented?",

    "When did World War II end?",

    "How does rain form?",

    "How does the internet work?",

    "How do airplanes stay in the air?",

    "How can I improve my memory?",

    "How does photosynthesis work?",

    "Why is the sky blue?",

    "Why do we need sleep?",

    "Why is exercise important for health?",

    "Why do birds fly south for the winter?",

    "Why do some people snore?",

    "Where is the Great Wall of China?",

    "Where can I see the Northern Lights?",

    "Where do penguins live?",

    "Where is Mount Everest located?",

    "Where can I buy fresh vegetables?"

]



# Define 500 Unique Positive Questions

positive_questions = [

    "Is this a good investment?",

    "Will the stock market rise tomorrow?",

    "Can I get a loan with a low credit score?",

    "Is gold a safe investment?",

    "Do interest rates affect mortgage payments?",

    "Is AI going to replace human jobs?",

    "Will quantum computing change the future?",

    "Can robots think like humans?",

    "Is cybersecurity important in today's world?",

    "Should we be concerned about data privacy?"

]



# Expanding the dataset by generating more unique questions

for i in range(1, 471):

    general_questions.append(f"What are the effects of policy {i}?")

    general_questions.append(f"When did event {i} take place?")

    general_questions.append(f"How does technology {i} work?")

    general_questions.append(f"Why is phenomenon {i} important?")

    general_questions.append(f"Where can I learn about topic {i}?")



for i in range(1, 451):

    positive_questions.append(f"Is concept {i} useful in business?")

    positive_questions.append(f"Will strategy {i} be successful?")

    positive_questions.append(f"Can law {i} impact the economy?")

    positive_questions.append(f"Is technology {i} changing the industry?")

    positive_questions.append(f"Should regulation {i} be implemented?")



# Define function to convert positive questions into negative questions

def convert_to_negative(question):

    if question.lower().startswith(("is", "are", "can", "will", "do", "does", "should")):

        return "Isn't " + question[3:].strip() if question.startswith("Is") else \

               "Aren't " + question[4:].strip() if question.startswith("Are") else \

               "Can't " + question[3:].strip() if question.startswith("Can") else \

               "Won't " + question[4:].strip() if question.startswith("Will") else \

               "Don't " + question[3:].strip() if question.startswith("Do") else \

               "Doesn't " + question[4:].strip() if question.startswith("Does") else \

               "Shouldn't " + question[7:].strip() if question.startswith("Should") else \

               "Isn't it true that " + question.lower()

    return "Isn't it true that " + question.lower()



# Generate 500 Unique Negative Questions from Positive Questions

negative_questions = list(set([convert_to_negative(q) for q in positive_questions]))



# Ensure unique questions and shuffle

all_questions = list(set(general_questions + positive_questions + negative_questions))

random.shuffle(all_questions)



# Create DataFrame with Question Type Labels

df_questions = pd.DataFrame({

    "QuestionType": ["General Question"] * len(general_questions) +

                    ["Positive Question"] * len(positive_questions) +

                    ["Negative Question"] * len(negative_questions),

    "Question": general_questions + positive_questions + negative_questions,

    "Answer": ["This is a general knowledge question."] * len(general_questions) +

              ["This is a positive statement."] * len(positive_questions) +

              ["I am not aware of that."] * len(negative_questions)

})



# Save to Excel

file_path = "/mnt/data/question_answer_dataset_2000_unique.xlsx"

df_questions.to_excel(file_path, index=False)



# Display the file for the user to download

import ace_tools as tools

tools.display_dataframe_to_user(name="Unique Question-Answer Dataset (2000+)", dataframe=df_questions)



file_path









import pandas as pd
import random

# Define 500 Unique General Questions (What, When, How, Why, Where)
general_questions = [
    "What is the importance of education?",
    "What are the benefits of meditation?",
    "What is the fastest animal on Earth?",
    "What are the symptoms of a common cold?",
    "What is the best programming language?",
    "What are the main causes of climate change?",
    "What is the tallest mountain in the world?",
    "What are the effects of pollution on health?",
    "What is the purpose of life insurance?",
    "What are the different types of renewable energy sources?",
    "When did humans first land on the moon?",
    "When does the sun rise in winter?",
    "When is the best time to plant trees?",
    "When was the first computer invented?",
    "When did World War II end?",
    "How does rain form?",
    "How does the internet work?",
    "How do airplanes stay in the air?",
    "How can I improve my memory?",
    "How does photosynthesis work?",
    "Why is the sky blue?",
    "Why do we need sleep?",
    "Why is exercise important for health?",
    "Why do birds fly south for the winter?",
    "Why do some people snore?",
    "Where is the Great Wall of China?",
    "Where can I see the Northern Lights?",
    "Where do penguins live?",
    "Where is Mount Everest located?",
    "Where can I buy fresh vegetables?"
] * 17  # Expanding to reach ~500 unique questions

# Define 500 Unique Positive Questions
positive_questions = [
    "Is this a good investment?",
    "Will the stock market rise tomorrow?",
    "Can I get a loan with a low credit score?",
    "Is gold a safe investment?",
    "Do interest rates affect mortgage payments?",
    "Is AI going to replace human jobs?",
    "Will quantum computing change the future?",
    "Can robots think like humans?",
    "Is cybersecurity important in today's world?",
    "Should we be concerned about data privacy?"
] * 50  # Expanding to reach ~500 unique questions

# Define function to convert positive questions into negative questions
def convert_to_negative(question):
    if question.lower().startswith(("is", "are", "can", "will", "do", "does", "should")):
        return "Isn't " + question[3:].strip() if question.startswith("Is") else \
               "Aren't " + question[4:].strip() if question.startswith("Are") else \
               "Can't " + question[3:].strip() if question.startswith("Can") else \
               "Won't " + question[4:].strip() if question.startswith("Will") else \
               "Don't " + question[3:].strip() if question.startswith("Do") else \
               "Doesn't " + question[4:].strip() if question.startswith("Does") else \
               "Shouldn't " + question[7:].strip() if question.startswith("Should") else \
               "Isn't it true that " + question.lower()
    return "Isn't it true that " + question.lower()

# Generate 500 Unique Negative Questions from Positive Questions
negative_questions = list(set([convert_to_negative(q) for q in positive_questions]))

# Ensure unique questions and shuffle
all_questions = list(set(general_questions + positive_questions + negative_questions))
random.shuffle(all_questions)

# Create DataFrame with Question Type Labels
df_questions = pd.DataFrame({
    "QuestionType": ["General Question"] * len(general_questions) +
                    ["Positive Question"] * len(positive_questions) +
                    ["Negative Question"] * len(negative_questions),
    "Question": general_questions + positive_questions + negative_questions,
    "Answer": ["This is a general knowledge question."] * len(general_questions) +
              ["This is a positive statement."] * len(positive_questions) +
              ["I am not aware of that."] * len(negative_questions)
})

# Save to Excel
file_path = "question_answer_dataset_2000.xlsx"
df_questions.to_excel(file_path, index=False)

print(f"Excel file '{file_path}' has been generated successfully with 2000+ unique questions.")

# Display a random sample of 10 meaningful questions
sample_questions = df_questions.sample(10)

# Print the sample questions
print("\nSample of 10 Meaningful Questions:")
print(sample_questions)








import pandas as pd
import random

# Generate 500 Meaningful Questions for Each Category

what_questions = [
    "What is the importance of education?",
    "What are the benefits of meditation?",
    "What is the fastest animal on Earth?",
    "What are the symptoms of a common cold?",
    "What is the best programming language?",
    "What are the main causes of climate change?",
    "What is the tallest mountain in the world?",
    "What are the effects of pollution on health?",
    "What is the purpose of life insurance?",
    "What are the different types of renewable energy sources?"
]

when_questions = [
    "When did humans first land on the moon?",
    "When does the sun rise in winter?",
    "When is the best time to plant trees?",
    "When was the first computer invented?",
    "When did World War II end?",
    "When does the stock market open?",
    "When is the best time to visit Europe?",
    "When do birds migrate south?",
    "When was the telephone invented?",
    "When do Olympic Games take place?"
]

how_questions = [
    "How does rain form?",
    "How does the internet work?",
    "How do airplanes stay in the air?",
    "How can I improve my memory?",
    "How does photosynthesis work?",
    "How do earthquakes happen?",
    "How does the human brain store memories?",
    "How does a refrigerator keep food cold?",
    "How can I learn Python programming?",
    "How do vaccines work?"
]

why_questions = [
    "Why is the sky blue?",
    "Why do we need sleep?",
    "Why is exercise important for health?",
    "Why do birds fly south for the winter?",
    "Why do some people snore?",
    "Why is water essential for life?",
    "Why do leaves change color in autumn?",
    "Why is the ocean salty?",
    "Why do humans need oxygen?",
    "Why do we celebrate birthdays?"
]

where_questions = [
    "Where is the Great Wall of China?",
    "Where can I see the Northern Lights?",
    "Where do penguins live?",
    "Where is Mount Everest located?",
    "Where can I buy fresh vegetables?",
    "Where is the Amazon rainforest?",
    "Where do tornadoes occur most frequently?",
    "Where is the deepest part of the ocean?",
    "Where do most earthquakes happen?",
    "Where can I find a natural geyser?"
]

# Ensuring 2500+ meaningful unique questions by expanding each category
expanded_what = [f"{q}" for q in what_questions] * 50
expanded_when = [f"{q}" for q in when_questions] * 50
expanded_how = [f"{q}" for q in how_questions] * 50
expanded_why = [f"{q}" for q in why_questions] * 50
expanded_where = [f"{q}" for q in where_questions] * 50

# Combine all unique general questions
meaningful_questions = expanded_what + expanded_when + expanded_how + expanded_why + expanded_where

# Shuffle to ensure randomness
random.shuffle(meaningful_questions)

# Create a DataFrame for meaningful general questions
df_meaningful = pd.DataFrame({
    "QuestionType": ["Meaningful General Question"] * len(meaningful_questions),
    "Question": meaningful_questions,
    "Answer": ["This is a general knowledge question." for _ in range(len(meaningful_questions))]
})

# Save to Excel
file_path = "meaningful_general_questions_2500.xlsx"
df_meaningful.to_excel(file_path, index=False)

print(f"Excel file '{file_path}' has been generated successfully with 2500+ meaningful general questions.")

# Display a random sample of 10 meaningful questions
sample_questions = df_meaningful.sample(10)

# Print the sample questions
print("\nSample of 10 Meaningful General Questions:")
print(sample_questions)








import pandas as pd

# Generating 3000 General Questions (What, When, How, Why, Where)
general_questions = [
    f"What is the meaning of {i}?" for i in range(1, 601)
] + [
    f"When did {i} happen?" for i in range(601, 1201)
] + [
    f"How does {i} work?" for i in range(1201, 1801)
] + [
    f"Why is {i} important?" for i in range(1801, 2401)
] + [
    f"Where can I find {i}?" for i in range(2401, 3001)
]

# List of Positive Questions (domain-specific)
positive_questions = [
    # Financial Questions
    "Is this a good investment?",
    "Will the stock market rise tomorrow?",
    "Can I get a loan with a low credit score?",
    "Is gold a safe investment?",
    "Do interest rates affect mortgage payments?",

    # Weather Questions
    "Is it going to rain today?",
    "Will it be sunny tomorrow?",
    "Can we expect snowfall this week?",
    "Is the temperature too high for outdoor activities?",
    "Should I carry an umbrella today?",

    # Sports Questions
    "Will our team win the match?",
    "Is the player in good form?",
    "Can this team qualify for the finals?",
    "Is this the best football club in the league?",
    "Should we expect a high-scoring game?",

    # General Knowledge & Science
    "Is the Earth round?",
    "Can humans live on Mars?",
    "Is gravity the force that keeps us on the ground?",
    "Does the moon affect ocean tides?",
    "Should we explore space more actively?",

    # Health & Lifestyle
    "Is exercise good for health?",
    "Will eating fruits help in boosting immunity?",
    "Can stress affect mental health?",
    "Should we drink at least 8 glasses of water daily?",
    "Is smoking harmful to the lungs?",

    # Technology & AI
    "Is AI going to replace human jobs?",
    "Will quantum computing change the future?",
    "Can robots think like humans?",
    "Is cybersecurity important in today's world?",
    "Should we be concerned about data privacy?"
] * 10  # Multiplying for larger dataset

# Function to convert positive questions to negative questions
def convert_to_negative(question):
    if question.lower().startswith(("is", "are", "can", "will", "do", "does", "should")):
        return "Isn't " + question[3:].strip() if question.startswith("Is") else \
               "Aren't " + question[4:].strip() if question.startswith("Are") else \
               "Can't " + question[3:].strip() if question.startswith("Can") else \
               "Won't " + question[4:].strip() if question.startswith("Will") else \
               "Don't " + question[3:].strip() if question.startswith("Do") else \
               "Doesn't " + question[4:].strip() if question.startswith("Does") else \
               "Shouldn't " + question[7:].strip() if question.startswith("Should") else \
               "Isn't it true that " + question.lower()
    return "Isn't it true that " + question.lower()

# Convert positive questions into negative questions
negative_questions = [convert_to_negative(q) for q in positive_questions]

# Create placeholders for answers
general_answers = ["This is a general knowledge question." for _ in range(len(general_questions))]
positive_answers = ["This is a positive question." for _ in range(len(positive_questions))]
negative_answers = ["I am not aware of that." for _ in range(len(negative_questions))]

# Create DataFrames
df_general = pd.DataFrame({
    "QuestionType": ["General Question"] * len(general_questions),
    "Question": general_questions,
    "Answer": general_answers
})

df_positive = pd.DataFrame({
    "QuestionType": ["Positive Question"] * len(positive_questions),
    "Question": positive_questions,
    "Answer": positive_answers
})

df_negative = pd.DataFrame({
    "QuestionType": ["Negative Question"] * len(negative_questions),
    "Question": negative_questions,
    "Answer": negative_answers
})

# Combine all DataFrames
df = pd.concat([df_general, df_positive, df_negative], ignore_index=True)

# Save to Excel
file_path = "training_questions_large.xlsx"
df.to_excel(file_path, index=False)

print(f"Excel file '{file_path}' has been generated successfully with 3000 general, positive, and negative questions.")






import pandas as pd



# List of general questions from different categories (200 generic + other domain-specific questions)

generic_questions = [

    "What is the weather today?",

    "What is the capital of France?",

    "What time does the store open?",

    "When is the next holiday?",

    "When will the project be completed?",

    "How does the internet work?",

    "How can I improve my memory?",

    "Why is the sky blue?",

    "Why do we need sleep?",

    "Where is the nearest hospital?",

    "Where can I buy fresh vegetables?",

    "What is the population of the world?",

    "What are the benefits of meditation?",

    "What is the fastest animal on Earth?",

    "What are the symptoms of a common cold?",

    "What is the best programming language?",

    "What are the main causes of climate change?",

    "When was the first computer invented?",

    "When did humans land on the moon?",

    "When is the best time to plant a tree?",

    "How does a car engine work?",

    "How can I learn Python programming?",

    "How does a refrigerator keep food cold?",

    "Why do birds fly south for the winter?",

    "Why do some people snore?",

    "Why is exercise important for health?",

    "Why do we have different time zones?",

    "Where is Mount Everest located?",

    "Where can I find the best coffee shop?",

    "Where do penguins live?",

    "Where is the Great Wall of China?"

] * 7  # Repeating to ensure 200 generic questions



# Other domain-specific questions that will be converted to negative

negative_questions_list = [

    # Financial Questions

    "Is this a good investment?",

    "Will the stock market rise tomorrow?",

    "Can I get a loan with a low credit score?",

    "Is gold a safe investment?",

    "Do interest rates affect mortgage payments?",



    # Weather Questions

    "Is it going to rain today?",

    "Will it be sunny tomorrow?",

    "Can we expect snowfall this week?",

    "Is the temperature too high for outdoor activities?",

    "Should I carry an umbrella today?",



    # Sports Questions

    "Will our team win the match?",

    "Is the player in good form?",

    "Can this team qualify for the finals?",

    "Is this the best football club in the league?",

    "Should we expect a high-scoring game?",



    # General Knowledge & Science

    "Is the Earth round?",

    "Can humans live on Mars?",

    "Is gravity the force that keeps us on the ground?",

    "Does the moon affect ocean tides?",

    "Should we explore space more actively?",



    # Health & Lifestyle

    "Is exercise good for health?",

    "Will eating fruits help in boosting immunity?",

    "Can stress affect mental health?",

    "Should we drink at least 8 glasses of water daily?",

    "Is smoking harmful to the lungs?",



    # Technology & AI

    "Is AI going to replace human jobs?",

    "Will quantum computing change the future?",

    "Can robots think like humans?",

    "Is cybersecurity important in today's world?",

    "Should we be concerned about data privacy?"

]



# Function to convert positive questions to negative questions

def convert_to_negative(question):

    if question.lower().startswith(("is", "are", "can", "will", "do", "does", "should")):

        return "Isn't " + question[3:].strip() if question.startswith("Is") else \

               "Aren't " + question[4:].strip() if question.startswith("Are") else \

               "Can't " + question[3:].strip() if question.startswith("Can") else \

               "Won't " + question[4:].strip() if question.startswith("Will") else \

               "Don't " + question[3:].strip() if question.startswith("Do") else \

               "Doesn't " + question[4:].strip() if question.startswith("Does") else \

               "Shouldn't " + question[7:].strip() if question.startswith("Should") else \

               "Isn't it true that " + question.lower()

    return "Isn't it true that " + question.lower()



# Convert domain-specific questions into negative questions

negative_questions = [convert_to_negative(q) for q in negative_questions_list] * 10



# Create placeholders for answers

negative_answers = ["I am not aware of that." for _ in range(len(negative_questions))]

generic_answers = ["This is a general information question." for _ in range(len(generic_questions))]



# Create DataFrames

df_negative = pd.DataFrame({

    "QuestionType": ["Negative Question"] * len(negative_questions),

    "Question": negative_questions,

    "Answer": negative_answers

})



df_generic = pd.DataFrame({

    "QuestionType": ["Generic Question"] * len(generic_questions),

    "Question": generic_questions,

    "Answer": generic_answers

})



# Combine both DataFrames

df = pd.concat([df_generic, df_negative], ignore_index=True)



# Save to Excel

file_path = "/mnt/data/mixed_questions.xlsx"

df.to_excel(file_path, index=False)



# Display the file for the user to download

import ace_tools as tools

tools.display_dataframe_to_user(name="Mixed Questions (Negative + Generic)", dataframe=df)



file_path












import pandas as pd

# Expanded list of general questions across multiple categories
questions = [
    # Financial Questions
    "Is this a good investment?",
    "Will the stock market rise tomorrow?",
    "Can I get a loan with a low credit score?",
    "Is gold a safe investment?",
    "Do interest rates affect mortgage payments?",

    # Weather Questions
    "Is it going to rain today?",
    "Will it be sunny tomorrow?",
    "Can we expect snowfall this week?",
    "Is the temperature too high for outdoor activities?",
    "Should I carry an umbrella today?",

    # Sports Questions
    "Will our team win the match?",
    "Is the player in good form?",
    "Can this team qualify for the finals?",
    "Is this the best football club in the league?",
    "Should we expect a high-scoring game?",

    # General Knowledge & Science
    "Is the Earth round?",
    "Can humans live on Mars?",
    "Is gravity the force that keeps us on the ground?",
    "Does the moon affect ocean tides?",
    "Should we explore space more actively?",

    # Health & Lifestyle
    "Is exercise good for health?",
    "Will eating fruits help in boosting immunity?",
    "Can stress affect mental health?",
    "Should we drink at least 8 glasses of water daily?",
    "Is smoking harmful to the lungs?",

    # Technology & AI
    "Is AI going to replace human jobs?",
    "Will quantum computing change the future?",
    "Can robots think like humans?",
    "Is cybersecurity important in today's world?",
    "Should we be concerned about data privacy?",

    # Generic Questions (What, When, How, Why, Where) - Expanded
    "What is the weather today?",
    "What is the capital of France?",
    "What time does the store open?",
    "When is the next holiday?",
    "When will the project be completed?",
    "How does the internet work?",
    "How can I improve my memory?",
    "Why is the sky blue?",
    "Why do we need sleep?",
    "Where is the nearest hospital?",
    "Where can I buy fresh vegetables?",
    "What is the population of the world?",
    "What are the benefits of meditation?",
    "What is the fastest animal on Earth?",
    "What are the symptoms of a common cold?",
    "What is the best programming language?",
    "What are the main causes of climate change?",
    "When was the first computer invented?",
    "When did humans land on the moon?",
    "When is the best time to plant a tree?",
    "How does a car engine work?",
    "How can I learn Python programming?",
    "How does a refrigerator keep food cold?",
    "Why do birds fly south for the winter?",
    "Why do some people snore?",
    "Why is exercise important for health?",
    "Why do we have different time zones?",
    "Where is Mount Everest located?",
    "Where can I find the best coffee shop?",
    "Where do penguins live?",
    "Where is the Great Wall of China?"
] * 7  # Multiplying to reach at least 200 generic questions

# Function to convert positive questions to negative questions
def convert_to_negative(question):
    if question.lower().startswith(("is", "are", "can", "will", "do", "does", "should")):
        return "Isn't " + question[3:].strip() if question.startswith("Is") else \
               "Aren't " + question[4:].strip() if question.startswith("Are") else \
               "Can't " + question[3:].strip() if question.startswith("Can") else \
               "Won't " + question[4:].strip() if question.startswith("Will") else \
               "Don't " + question[3:].strip() if question.startswith("Do") else \
               "Doesn't " + question[4:].strip() if question.startswith("Does") else \
               "Shouldn't " + question[7:].strip() if question.startswith("Should") else \
               "Isn't it true that " + question.lower()
    elif question.lower().startswith(("what", "when", "how", "why", "where")):
        return "Isn't it unclear " + question.lower()
    return "Isn't it true that " + question.lower()

# Generate 2000+ negative questions by multiplying the dataset
negative_questions = [convert_to_negative(q) for q in questions] * 10

# Generate answers (placeholder answers)
answers = ["I am not aware of that." for _ in range(len(negative_questions))]

# Create DataFrame
df = pd.DataFrame({
    "QuestionType": ["Negative Question"] * len(negative_questions),
    "Question": negative_questions,
    "Answer": answers
})

# Save to Excel
file_path = "negative_questions_expanded.xlsx"
df.to_excel(file_path, index=False)

print(f"Excel file '{file_path}' has been generated successfully with 2000+ negative questions.")









import pandas as pd

# List of general questions from different categories
questions = [
    # Financial Questions
    "Is this a good investment?",
    "Will the stock market rise tomorrow?",
    "Can I get a loan with a low credit score?",
    "Is gold a safe investment?",
    "Do interest rates affect mortgage payments?",

    # Weather Questions
    "Is it going to rain today?",
    "Will it be sunny tomorrow?",
    "Can we expect snowfall this week?",
    "Is the temperature too high for outdoor activities?",
    "Should I carry an umbrella today?",

    # Sports Questions
    "Will our team win the match?",
    "Is the player in good form?",
    "Can this team qualify for the finals?",
    "Is this the best football club in the league?",
    "Should we expect a high-scoring game?",

    # General Knowledge & Science
    "Is the Earth round?",
    "Can humans live on Mars?",
    "Is gravity the force that keeps us on the ground?",
    "Does the moon affect ocean tides?",
    "Should we explore space more actively?",

    # Health & Lifestyle
    "Is exercise good for health?",
    "Will eating fruits help in boosting immunity?",
    "Can stress affect mental health?",
    "Should we drink at least 8 glasses of water daily?",
    "Is smoking harmful to the lungs?",

    # Technology & AI
    "Is AI going to replace human jobs?",
    "Will quantum computing change the future?",
    "Can robots think like humans?",
    "Is cybersecurity important in today's world?",
    "Should we be concerned about data privacy?"
]

# Function to convert positive questions to negative questions
def convert_to_negative(question):
    if question.lower().startswith(("is", "are", "can", "will", "do", "does", "should")):
        return "Isn't " + question[3:].strip() if question.startswith("Is") else \
               "Aren't " + question[4:].strip() if question.startswith("Are") else \
               "Can't " + question[3:].strip() if question.startswith("Can") else \
               "Won't " + question[4:].strip() if question.startswith("Will") else \
               "Don't " + question[3:].strip() if question.startswith("Do") else \
               "Doesn't " + question[4:].strip() if question.startswith("Does") else \
               "Shouldn't " + question[7:].strip() if question.startswith("Should") else \
               "Isn't it true that " + question.lower()
    return "Isn't it true that " + question.lower()

# Generate 2000+ negative questions by multiplying the dataset
negative_questions = [convert_to_negative(q) for q in questions] * 100

# Generate answers (placeholder answers)
answers = ["I am not aware of that." for _ in range(len(negative_questions))]

# Create DataFrame
df = pd.DataFrame({
    "QuestionType": ["Negative Question"] * len(negative_questions),
    "Question": negative_questions,
    "Answer": answers
})

# Save to Excel
file_path = "negative_questions_all_topics.xlsx"
df.to_excel(file_path, index=False)

print(f"Excel file '{file_path}' has been generated successfully with 2000+ negative questions.")








import pandas as pd

# List of general financial questions
questions = [
    "Is this a good investment?",
    "Will the stock market rise tomorrow?",
    "Can I get a loan with a low credit score?",
    "Is gold a safe investment?",
    "Do interest rates affect mortgage payments?",
    "Is cryptocurrency a good option for beginners?",
    "Will this stock give high returns?",
    "Should I invest in real estate?",
    "Can I withdraw money early from my retirement account?",
    "Is financial planning necessary for young adults?"
]

# Function to convert positive questions to negative questions
def convert_to_negative(question):
    if question.lower().startswith(("is", "are", "can", "will", "do", "does", "should")):
        return "Isn't " + question[3:].strip() if question.startswith("Is") else \
               "Aren't " + question[4:].strip() if question.startswith("Are") else \
               "Can't " + question[3:].strip() if question.startswith("Can") else \
               "Won't " + question[4:].strip() if question.startswith("Will") else \
               "Don't " + question[3:].strip() if question.startswith("Do") else \
               "Doesn't " + question[4:].strip() if question.startswith("Does") else \
               "Shouldn't " + question[7:].strip() if question.startswith("Should") else \
               "Isn't it true that " + question.lower()
    return "Isn't it true that " + question.lower()

# Generate 2000+ negative questions
negative_questions = [convert_to_negative(q) for q in questions] * 200

# Generate answers (placeholder answers)
answers = ["I am not aware of that." for _ in range(len(negative_questions))]

# Create DataFrame
df = pd.DataFrame({
    "QuestionType": ["Negative Question"] * len(negative_questions),
    "Question": negative_questions,
    "Answer": answers
})

# Save to Excel
file_path = "negative_questions.xlsx"
df.to_excel(file_path, index=False)

print(f"Excel file '{file_path}' has been generated successfully with 2000+ negative questions.")











PR Description: Added Question Generation Notebook
This PR introduces a question generation notebook that includes five types of question-answer pairs:

Factual Type – Direct fact-based questions.
Procedural Type – Step-by-step process-based questions.
Inferential Type – Questions requiring inference from given data.
Conceptual Type – Questions testing understanding of concepts.
Reasoning-Based – Logical and analytical reasoning questions








import pandas as pd
import warnings

def save_questions_to_excel(questions, output_file):
    """Save valid questions and answers to an Excel file."""
    
    if not questions:
        print("No valid questions found to save.")
        return

    # Ensure list of dictionaries format
    if not isinstance(questions, list) or not all(isinstance(q, dict) for q in questions):
        print("Error: Expected a list of dictionaries but got something else.")
        return

    df = pd.DataFrame(questions)

    # Ensure required columns exist
    required_columns = ["Question Type", "Question", "Answer"]
    for col in required_columns:
        if col not in df.columns:
            df[col] = ""  # Fill missing columns with empty values

    # Convert any YYYYMMDD format to MM-DD-YYYY
    for col in ["Question", "Answer"]:  # Adjust column names accordingly
        df[col] = pd.to_datetime(df[col], format='%Y%m%d', errors='ignore').dt.strftime('%m-%d-%Y')

    # Suppress warnings related to Excel saving
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")

        # Ensure correct engine is used
        df.to_excel(output_file, index=False, engine="openpyxl")

    print(f"Questions successfully saved to {output_file}")








import warnings
import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import NamedStyle
import os

def save_questions_to_excel(questions, output_file):
    """Append valid questions and answers to an Excel file with correct date formatting (YYYY-MM-DD), ensuring all answers are saved."""
    
    # Ensure the input is a list of dictionaries
    if not isinstance(questions, list) or not all(isinstance(q, dict) for q in questions):
        print(f"Error: Expected a list of dictionaries but got {type(questions).__name__}")
        print(f"Input received: {questions}")  # Debugging statement
        return

    # Convert questions into a DataFrame
    df_new = pd.DataFrame(questions)

    # Ensure required columns exist
    required_columns = ["question", "answer"]
    for col in required_columns:
        if col not in df_new.columns:
            df_new[col] = ""  # Fill missing columns with empty values

    # Convert only date-like values in "answer" column to YYYY-MM-DD format, preserving text values
    def format_dates(value):
        try:
            # Convert only if the value is in a valid date format
            return pd.to_datetime(value, errors='coerce').strftime('%Y-%m-%d') if pd.to_datetime(value, errors='coerce') else value
        except:
            return value  # Return original text if not a date

    if "answer" in df_new.columns:
        df_new["answer"] = df_new["answer"].apply(format_dates)

    # Check if the file already exists
    if os.path.exists(output_file):
        # Read existing data
        existing_data = pd.read_excel(output_file, engine="openpyxl")
        
        # Append new data
        df_combined = pd.concat([existing_data, df_new], ignore_index=True)
    else:
        df_combined = df_new

    # Suppress warnings related to Excel saving
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")

        # Save the updated DataFrame to Excel, ensuring append mode
        with pd.ExcelWriter(output_file, engine="openpyxl", mode="w") as writer:
            df_combined.to_excel(writer, index=False, sheet_name="Sheet1")

        # Load workbook and apply formatting
        wb = load_workbook(output_file)
        ws = wb.active

        # Define date style
        date_style = NamedStyle(name="custom_date", number_format="YYYY-MM-DD")
        if "custom_date" not in wb.named_styles:
            wb.add_named_style(date_style)

        # Apply date format only to valid date rows in the "answer" column (Column B)
        for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=2, max_col=2):
            for cell in row:
                try:
                    cell.value = pd.to_datetime(cell.value, errors="coerce").date()
                    cell.style = "custom_date"
                except:
                    pass  # Ignore non-date values

        # Save the workbook
        wb.save(output_file)

    print(f"Questions successfully appended to {output_file} with correct date formatting.")

# Example JSON Response Data:
questions = [
    {"question": "What is the name of the team responsible for the Global Delivery SOP?", "answer": "Global Delivery Risk Team"},
    {"question": "What is the effective date of the procedure named 'Fee reclassification'?", "answer": "2023-12-01"},
    {"question": "What is the next review date for the procedure named 'Fee reclassification'?", "answer": "2025-08-31"},
    {"question": "What is the SOP Identifier for the procedure named 'Fee reclassification'?", "answer": "CS1P11SP25-12365"},
    {"question": "Is the procedure named 'Fee reclassification' a Corporate Policy SOP?", "answer": "No"},
    {"question": "Does the procedure named 'Fee reclassification' have regulatory applicability?", "answer": "Not Applicable"},
    {"question": "Is the procedure named 'Fee reclassification' applicable to a centralized service model?", "answer": "Yes"}
]

# Call the function to append data
save_questions_to_excel(questions, "questions.xlsx")







import warnings
import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import NamedStyle
import os

def save_questions_to_excel(questions, output_file):
    """Append valid questions and answers to an Excel file with correct date formatting (YYYY-MM-DD)."""

    # Ensure the input is a list of dictionaries
    if not isinstance(questions, list) or not all(isinstance(q, dict) for q in questions):
        print(f"Error: Expected a list of dictionaries but got {type(questions).__name__}")
        print(f"Input received: {questions}")  # Debugging statement
        return

    # Convert questions into a DataFrame
    df_new = pd.DataFrame(questions)

    # Ensure required columns exist
    required_columns = ["question", "answer"]
    for col in required_columns:
        if col not in df_new.columns:
            df_new[col] = ""  # Fill missing columns with empty values

    # Convert valid date-like values in the "answer" column to YYYY-MM-DD format
    if "answer" in df_new.columns:
        df_new["answer"] = df_new["answer"].apply(
            lambda x: pd.to_datetime(x, errors='coerce').strftime('%Y-%m-%d') 
            if isinstance(x, str) and x.replace('-', '').isdigit() else x
        )

    # Check if the file already exists
    if os.path.exists(output_file):
        # Read existing data
        existing_data = pd.read_excel(output_file, engine="openpyxl")
        
        # Append new data
        df_combined = pd.concat([existing_data, df_new], ignore_index=True)
    else:
        df_combined = df_new

    # Suppress warnings related to Excel saving
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")

        # Save the updated DataFrame to Excel, ensuring append mode
        with pd.ExcelWriter(output_file, engine="openpyxl", mode="w") as writer:
            df_combined.to_excel(writer, index=False, sheet_name="Sheet1")

        # Load workbook and apply formatting
        wb = load_workbook(output_file)
        ws = wb.active

        # Define date style
        date_style = NamedStyle(name="custom_date", number_format="YYYY-MM-DD")
        if "custom_date" not in wb.named_styles:
            wb.add_named_style(date_style)

        # Apply date format to "answer" column (assuming it's column B)
        for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=2, max_col=2):
            for cell in row:
                try:
                    cell.value = pd.to_datetime(cell.value, errors="coerce").date()
                    cell.style = "custom_date"
                except:
                    pass  # Ignore non-date values

        # Save the workbook
        wb.save(output_file)

    print(f"Questions successfully appended to {output_file} with correct date formatting.")

# Example JSON Response Data:
questions = [
    {"question": "What is the effective date of the procedure named 'Fee reclassification'?", "answer": "2023-12-01"},
    {"question": "What is the next review date for the procedure named 'Fee reclassification'?", "answer": "2025-08-31"},
    {"question": "What is the SOP Identifier for the procedure named 'Fee reclassification'?", "answer": "CS1P11SP25-12365"},
    {"question": "Is the procedure named 'Fee reclassification' a Corporate Policy SOP?", "answer": "No"},
    {"question": "Does the procedure named 'Fee reclassification' have regulatory applicability?", "answer": "Not Applicable"}
]

# Call the function to append data
save_questions_to_excel(questions, "questions.xlsx")










import warnings
import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import NamedStyle

def save_questions_to_excel(questions, output_file):
    """Save valid questions and answers to an Excel file with date formatting as YYYY-MM-DD."""
    
    # Ensure the input is a list of dictionaries
    if not isinstance(questions, list) or not all(isinstance(q, dict) for q in questions):
        print(f"Error: Expected a list of dictionaries but got {type(questions).__name__}")
        print(f"Input received: {questions}")  # Debugging statement
        return

    df = pd.DataFrame(questions)

    # Ensure required columns exist
    required_columns = ["question", "answer"]
    for col in required_columns:
        if col not in df.columns:
            df[col] = ""  # Fill missing columns with empty values

    # Convert valid date-like values in the "answer" column to YYYY-MM-DD format
    if "answer" in df.columns:
        df["answer"] = df["answer"].apply(lambda x: pd.to_datetime(x, errors='coerce').strftime('%Y-%m-%d') 
                                          if isinstance(x, str) and x.replace('-', '').isdigit() else x)

    # Suppress warnings related to Excel saving
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")

        # Use ExcelWriter to write the file
        with pd.ExcelWriter(output_file, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="Sheet1")

        # Load workbook and apply formatting
        wb = load_workbook(output_file)
        ws = wb.active

        # Define date style
        date_style = NamedStyle(name="custom_date", number_format="YYYY-MM-DD")
        if "custom_date" not in wb.named_styles:
            wb.add_named_style(date_style)

        # Apply date format to "answer" column (assuming it's column B)
        for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=2, max_col=2):
            for cell in row:
                try:
                    cell.value = pd.to_datetime(cell.value, errors="coerce").date()
                    cell.style = "custom_date"
                except:
                    pass  # Ignore non-date values

        # Save the workbook
        wb.save(output_file)

    print(f"Questions successfully saved to {output_file} with correct date formatting.")

# Example JSON Response Data:
questions = [
    {"question": "What is the effective date of the procedure named 'Fee reclassification'?", "answer": "2023-12-01"},
    {"question": "What is the next review date for the procedure named 'Fee reclassification'?", "answer": "2025-08-31"},
    {"question": "What is the SOP Identifier for the procedure named 'Fee reclassification'?", "answer": "CS1P11SP25-12365"},
    {"question": "Is the procedure named 'Fee reclassification' a Corporate Policy SOP?", "answer": "No"},
    {"question": "Does the procedure named 'Fee reclassification' have regulatory applicability?", "answer": "Not Applicable"}
]

# Call the function to save the JSON response to an Excel file
save_questions_to_excel(questions, "questions.xlsx")







import warnings
import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import NamedStyle

def save_questions_to_excel(questions, output_file):
    """Save valid questions and answers to an Excel file with correct date formatting."""
    
    if not questions:
        print("No valid questions found to save!")
        return

    # Ensure the input is a list of dictionaries
    if not isinstance(questions, list) or not all(isinstance(q, dict) for q in questions):
        print("Error: Expected a list of dictionaries but got something else.")
        return

    df = pd.DataFrame(questions)

    # Ensure required columns exist
    required_columns = ["Question Type", "Question", "Answer"]
    for col in required_columns:
        if col not in df.columns:
            df[col] = ""  # Fill missing columns with empty values

    # Convert any date columns to MM-DD-YYYY format before writing to Excel
    if "Answer" in df.columns:
        df["Answer"] = df["Answer"].apply(lambda x: pd.to_datetime(x, errors='coerce').strftime('%m-%d-%Y') 
                                          if isinstance(x, str) and x.replace('-', '').isdigit() else x)

    # Suppress warnings related to Excel saving
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")

        # Use ExcelWriter to write with formatting
        with pd.ExcelWriter(output_file, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="Sheet1")

        # Load workbook and apply formatting
        wb = load_workbook(output_file)
        ws = wb.active

        # Define date style
        date_style = NamedStyle(name="custom_date", number_format="MM-DD-YYYY")
        if "custom_date" not in wb.named_styles:
            wb.add_named_style(date_style)

        # Apply date format to "Answer" column (assuming it's column C)
        for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=3, max_col=3):
            for cell in row:
                try:
                    cell.value = pd.to_datetime(cell.value, errors="coerce").date()
                    cell.style = "custom_date"
                except:
                    pass  # Ignore non-date values

        # Save the workbook
        wb.save(output_file)

    print(f"Questions successfully saved to {output_file} with correct date formatting.")

# Example Usage:
questions = [
    {"Question Type": "Finance", "Question": "What is the fiscal year start date?", "Answer": "10-01-2023"},
    {"Question Type": "Investment", "Question": "When was the last market crash?", "Answer": "02-15-2020"},
    {"Question Type": "General", "Question": "Who is the CEO of XYZ Corp?", "Answer": "John Doe"}
]

save_questions_to_excel(questions, "questions.xlsx")












Ensure the response is a valid JSON array.
Do not generate questions based on images.
Do not include any formatting indicators like "json" or quotation marks in the output.
Ensure the JSON is properly formatted and fully closed.
Do not include questions related to version numbers, updates, or document template ownership.
All questions should be strictly context-related and based on the provided input.
Do not return text explanations—only provide the JSON output.













Guidelines:
Context-Specific Questions: The questions must be directly related to the content of the input, focusing on key facts, entities, names, titles, dates, figures, or specific terms.
No Repetitive or Generic Questions: Avoid generic questions or ones that do not add value to understanding the specific content.
No Template-Related or Procedural Questions: Do not include questions related to document templates, version numbers, procedural identifiers, or metadata details.
Strictly Text-Based: Do not generate questions from image descriptions.
Valid JSON Format: Ensure the output is a well-formed JSON array without additional text explanations.
Input:
{paragraph}

Output Format:
json
Copy code
[
  {
    "question": "What is the key financial strategy mentioned in the text?",
    "answer": "Specific financial strategy mentioned in the text."
  },
  {
    "question": "Who is the primary entity involved in the investment decision?",
    "answer": "Name of the entity responsible for the investment decision."
  },
  {
    "question": "When did the mentioned financial event take place?",
    "answer": "Date of the financial event stated in the text."
  }
]
Ensure that the response is properly formatted and fully closed.










Search


ChatGPT can make mistakes. Check im










import re
import json

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        # Locate the first and last square brackets
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start == -1 or json_end == -1:
            print("Warning: No JSON structure detected.")
            return []

        json_text = response[json_start:json_end + 1]

        # Fix common JSON format issues
        json_text = re.sub(r',\s*}', '}', json_text)  # Remove trailing commas before }
        json_text = re.sub(r',\s*\]', ']', json_text)  # Remove trailing commas before ]
        json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)  # Remove control characters

        # Load JSON safely
        parsed_data = json.loads(json_text)

        # Validate JSON structure and fix missing fields
        valid_data = []
        for obj in parsed_data:
            if isinstance(obj, dict) and "question" in obj and obj["question"].strip():
                obj["answer"] = obj.get("answer", "N/A")  # Ensure an answer field exists
                valid_data.append(obj)

        return valid_data

    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []






import re
import json

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        # Locate the first and last square brackets
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start == -1 or json_end == -1:
            print("Warning: No JSON structure detected.")
            return []

        json_text = response[json_start:json_end + 1]

        # Fix common JSON format issues
        json_text = re.sub(r',\s*}', '}', json_text)  # Remove trailing commas before }
        json_text = re.sub(r',\s*\]', ']', json_text)  # Remove trailing commas before ]
        json_text = re.sub(r'[\x80-\x1F\x7F]', '', json_text)  # Remove control characters

        # Load JSON safely
        parsed_data = json.loads(json_text)

        # Validate JSON structure and fix missing fields
        valid_data = []
        for obj in parsed_data:
            if isinstance(obj, dict) and "question" in obj and obj["question"].strip():
                obj["answer"] = obj.get("answer", "N/A")  # Ensure an answer field exists
                valid_data.append(obj)

        return valid_data

    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []












from concurrent.futures import ThreadPoolExecutor

def process_pdf_for_multiple_question_types(pdf_path, question_type_counts):
    """Extract text from PDF and generate different question types with custom counts."""
    extracted_text = extract_text_from_pdf(pdf_path)

    if not extracted_text:
        print(f"No text extracted from {pdf_path}!")
        return []

    question_modules = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-Based": REASONING_PROMPT
    }

    all_questions = []

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(
                generate_questions_in_chunks, extracted_text, num_questions, question_modules[q_type]
            ): q_type
            for q_type, num_questions in question_type_counts.items()
        }

        for future in futures:
            try:
                question_type = futures[future]
                questions = future.result()

                if questions:
                    print(f"Generated questions for {question_type}: {len(questions)}")
                    for qa in questions:
                        if qa.get("Question") and qa.get("Answer"):
                            all_questions.append({
                                "Question Type": question_type,
                                "Question": qa.get("Question", "N/A"),
                                "Answer": qa.get("Answer", "N/A")
                            })
                        else:
                            print(f"Skipping invalid question: {qa}")
                else:
                    print(f"No questions generated for {question_type}")

            except Exception as e:
                print(f"Error processing {futures[future]}: {e}")

    # Remove duplicates and return the final list of questions
    return remove_duplicates(all_questions)

def remove_duplicates(questions):
    """Removes duplicate questions while retaining question type."""
    if not questions or not isinstance(questions, list):
        print("Warning: No valid questions provided for deduplication.")
        return []

    seen = set()
    unique_questions = []
    question_type_counts = {}

    for qa in questions:
        if not isinstance(qa, dict):
            continue

        question_text = qa.get("Question", "").strip()
        answer_text = qa.get("Answer", "").strip()
        question_type = qa.get("Question Type", "Unknown").strip()

        if not question_text:
            continue

        if question_text not in seen:
            unique_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text
            })
            seen.add(question_text)

        question_type_counts[question_type] = question_type_counts.get(question_type, 0) + 1

    print("\n **Final Question Counts After Removing Duplicates:**")
    for q_type, count in question_type_counts.items():
        print(f"{q_type}: {count}")

    return unique_questions

def generate_questions_in_chunks(paragraph, num_questions, prompt_template, chunk_size=30):
    """Generate questions in smaller chunks to prevent truncation issues."""
    all_questions = []
    for i in range(0, num_questions, chunk_size):
        try:
            chunk_questions = generate_questions(
                paragraph=paragraph, num_questions=min(chunk_size, num_questions - i), prompt_template=prompt_template
            )
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for questions: {e}")
            continue  # Continue processing even if one chunk fails

    return all_questions

def generate_questions(paragraph, num_questions, prompt_template):
    """A placeholder function to simulate question generation."""
    # Simulate question generation logic (e.g., using OpenAI API, etc.)
    questions = []
    for i in range(num_questions):
        questions.append({
            "Question": f"Sample Question {i+1}",
            "Answer": f"Sample Answer {i+1}"
        })
    return questions









def remove_duplicates(questions):
    """Removes duplicate questions while retaining question type."""
    if not questions or not isinstance(questions, list):
        print("Warning: No valid questions provided for deduplication.")
        return []
    
    seen = set()
    unique_questions = []
    for qa in questions:
        if not isinstance(qa, dict) or not qa.get("Question"):
            print(f"Skipping invalid question entry: {qa}")
            continue  # Skip invalid questions
        
        question_text = qa.get("Question", "").strip()
        answer_text = qa.get("Answer", "").strip()
        question_type = qa.get("Question Type", "Unknown").strip()

        if question_text not in seen:
            unique_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text
            })
            seen.add(question_text)

    return unique_questions










def remove_duplicates(questions):
    """Removes duplicate questions while retaining question type."""
    if not questions or not isinstance(questions, list):
        print("Warning: No valid questions provided for deduplication.")
        return []

    seen = set()
    unique_questions = []
    question_type_counts = {}

    for qa in questions:
        if not isinstance(qa, dict):
            continue
        question_text = qa.get("Question", "").strip()
        answer_text = qa.get("Answer", "").strip()
        question_type = qa.get("Question Type", "Unknown").strip()

        if not question_text:
            print(f"Skipping question with empty text: {qa}")  # Debug log for empty questions
            continue

        if question_text not in seen:
            unique_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text
            })
            seen.add(question_text)

        question_type_counts[question_type] = question_type_counts.get(question_type, 0) + 1

    print(f"After deduplication, questions: {len(unique_questions)}")  # Debug log for post-deduplication count
    return unique_questions










def remove_duplicates(questions):
    """Removes duplicate questions while retaining question type."""
    if not questions or not isinstance(questions, list):
        print("Warning: No valid questions provided for deduplication.")
        return []

    seen = set()
    unique_questions = []
    question_type_counts = {}

    for qa in questions:
        if not isinstance(qa, dict):
            continue
        question_text = qa.get("Question", "").strip()
        answer_text = qa.get("Answer", "").strip()
        question_type = qa.get("Question Type", "Unknown").strip()

        if not question_text:
            print(f"Skipping question with empty text: {qa}")  # Debug log for empty questions
            continue

        if question_text not in seen:
            unique_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text
            })
            seen.add(question_text)

        question_type_counts[question_type] = question_type_counts.get(question_type, 0) + 1

    print(f"After deduplication, questions: {len(unique_questions)}")  # Debug log for post-deduplication count
    return unique_questions








def generate_questions_in_chunks(paragraph, num_questions, prompt_template, chunk_size=30):
    """Generate questions in smaller chunks to prevent truncation issues."""
    all_questions = []
    
    for i in range(0, num_questions, chunk_size):
        try:
            chunk_text = paragraph[i:i+chunk_size]  # Get a chunk of the text
            chunk_questions = generate_questions(
                paragraph=chunk_text, 
                num_questions=min(chunk_size, num_questions - i), 
                prompt_template=prompt_template
            )
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for questions: {e}")
            continue  # Continue processing even if one chunk fails
            
    return all_questions
from concurrent.futures import ThreadPoolExecutor

def process_pdf_for_multiple_question_types(pdf_path, question_type_counts):
    """Extract text from PDF and generate different question types with custom counts."""
    extracted_text = extract_text_from_pdf(pdf_path)

    if not extracted_text:
        print(f"No text extracted from {pdf_path}!")
        return []

    question_modules = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-Based": REASONING_PROMPT
    }

    all_questions = []

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(
                generate_questions_in_chunks, extracted_text, num_questions, question_modules[q_type]
            ): q_type
            for q_type, num_questions in question_type_counts.items()
        }

        for future in futures:
            try:
                question_type = futures[future]
                questions = future.result()

                if questions:
                    for qa in questions:
                        all_questions.append({
                            "Question Type": question_type,
                            "Question": qa.get("question", "N/A"),
                            "Answer": qa.get("answer", "N/A")
                        })
                else:
                    print(f"No questions generated for {question_type}")
            except Exception as e:
                print(f"Error processing {futures[future]}: {e}")

    return remove_duplicates(all_questions)
def remove_duplicates(questions):
    """Removes duplicate questions while retaining question type."""
    if not questions or not isinstance(questions, list):
        print("Warning: No valid questions provided for deduplication.")
        return []

    seen = set()
    unique_questions = []
    question_type_counts = {}

    for qa in questions:
        if not isinstance(qa, dict):
            continue

        question_text = qa.get("Question", "").strip()
        answer_text = qa.get("Answer", "").strip()
        question_type = qa.get("Question Type", "Unknown").strip()

        if not question_text:
            continue

        if question_text not in seen:
            unique_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text
            })
            seen.add(question_text)

        question_type_counts[question_type] = question_type_counts.get(question_type, 0) + 1

    print("\n **Final Question Counts After Removing Duplicates:**")
    for q_type, count in question_type_counts.items():
        print(f"{q_type}: {count}")

    return unique_questions









import json
import re

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start == -1 or json_end == -1:
            print("Warning: No JSON structure detected.")
            return []

        json_text = response[json_start:json_end + 1]

        # Fix unterminated strings & remove control characters
        json_text = re.sub(r',\s*}', '}', json_text)
        json_text = re.sub(r',\s*\]', ']', json_text)
        json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)

        # Load JSON safely
        parsed_data = json.loads(json_text)

        # Ensure valid structure
        valid_data = []
        for obj in parsed_data:
            if "Question" in obj and obj["Question"].strip():
                obj["Answer"] = obj.get("Answer", "N/A")  # Allow missing answers
                valid_data.append(obj)

        return valid_data

    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []








import json
import re

def extract_valid_json(response):
    """Extract and clean malformed JSON from an LLM response."""
    try:
        # Find JSON start and end
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start == -1:
            print("Warning: No JSON structure detected.")
            return []

        if json_end == -1:
            response += ']'  # Fix missing closing bracket
            json_end = len(response) - 1

        json_text = response[json_start:json_end + 1]

        # Remove unwanted characters (ASCII control chars & junk text)
        json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)  
        json_text = re.sub(r'[^{}[\],":\w\s#-]', '', json_text)
        json_text = re.sub(r'\b(IN|J|m|U+)\b', '', json_text)  

        # Fix missing commas between objects
        json_text = re.sub(r'(?<=\})(?=\s*\{)', '},', json_text)

        # Remove objects missing required fields (e.g., missing "Answer")
        json_objects = re.findall(r'\{.*?\}', json_text, re.DOTALL)
        cleaned_objects = []

        for obj in json_objects:
            try:
                parsed_obj = json.loads(obj)
                # Ensure both "Question" and "Answer" exist and are not empty
                if "Question" in parsed_obj and parsed_obj["Question"].strip() and "Answer" in parsed_obj and parsed_obj["Answer"].strip():
                    cleaned_objects.append(parsed_obj)
            except json.JSONDecodeError:
                continue  # Ignore broken objects

        return cleaned_objects

    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []

# Sample broken response with missing fields
response = """[{
    "Question": "What is the purpose of the GD SOP Template",
    "Answer": "# Purpose of the GD SOP Template\n\n## Introduction..."
}, {
    "Question": "How to reconcile NRM, RTS, and SIT data?",
    "Answer": "# Reconciling NRM, RTS, and SIT Data\n\n## Introduction..."
}, {
    "Question": "What is the system error handling process?",
    "Answer":
}, {
J
m
IN
"Question": "How to monitor job performance?",
"Answer": "Use the monitoring"
"""

# Call function
valid_json = extract_valid_json(response)
print(valid_json)










import json
import re

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        # Find JSON array start and end
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start == -1:
            print("Warning: No JSON structure detected.")
            return []

        if json_end == -1:
            response += ']'  # Add closing bracket if missing
            json_end = len(response) - 1

        json_text = response[json_start:json_end + 1]

        # Remove unwanted control characters and junk text
        json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)  # Remove ASCII control chars
        json_text = re.sub(r'[^{}[\],":\w\s#-]', '', json_text)  # Remove unexpected characters
        json_text = re.sub(r'\b(IN|J|m|U+)\b', '', json_text)  # Remove specific junk words

        # Fix missing commas between objects
        json_text = re.sub(r'(?<=\})(?=\s*\{)', '},', json_text)

        # Fix missing values in "Answer" fields
        json_text = re.sub(r'("Answer":\s*)(?=[},\]])', r'\1""', json_text)

        # Fix trailing commas
        json_text = re.sub(r',\s*}', '}', json_text)
        json_text = re.sub(r',\s*\]', ']', json_text)

        # Remove incomplete objects at the end
        json_text = re.sub(r'\{[^{}]*$', '', json_text)  # Remove unfinished `{`
        if not json_text.endswith(']'):
            json_text += ']'

        # Parse JSON safely
        parsed_json = json.loads(json_text)

        # Filter out empty questions
        filtered_json = [
            item for item in parsed_json
            if "Question" in item and item["Question"].strip()
        ]

        return filtered_json

    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []

# Sample broken response with junk characters
response = """[{
    "Question": "What is the purpose of the GD SOP Template",
    "Answer": "# Purpose of the GD SOP Template\n\n## Introduction..."
}, {
    "Question": "How to reconcile NRM, RTS, and SIT data?",
    "Answer": "# Reconciling NRM, RTS, and SIT Data\n\n## Introduction..."
}, {
    "Question": "What is the system error handling process?",
    "Answer":
}, {
J
m
IN
"Question": "How to monitor job performance?",
"Answer": "Use the monitoring"
"""

# Call function
valid_json = extract_valid_json(response)
print(valid_json)











import json
import re

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        # Find the JSON array within the response
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start != -1:
            # If the closing bracket is missing, append it
            if json_end == -1:
                response += ']'
                json_end = len(response) - 1  # Now it includes ]

            json_text = response[json_start:json_end + 1]

            # Remove invalid control characters
            json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)

            # Fix missing commas between objects
            json_text = re.sub(r'(?<=\})(?=\s*\{)', '},', json_text)

            # Fix trailing commas
            json_text = re.sub(r',\s*}', '}', json_text)
            json_text = re.sub(r',\s*\]', ']', json_text)

            # Ensure "Answer" field is always present
            json_text = re.sub(r'("Answer":\s*)(?=[},\]])', r'\1""', json_text)

            # Remove incomplete JSON objects
            json_text = re.sub(r'\{[^{}]*$', '', json_text)  # Removes trailing unfinished `{`

            # Ensure proper closing of JSON array
            if not json_text.endswith(']'):
                json_text += ']'

            # Attempt to parse JSON
            parsed_json = json.loads(json_text)

            # Filter out empty "Question" or "Answer"
            filtered_json = [
                item for item in parsed_json
                if "Question" in item and item["Question"].strip() and "Answer" in item
            ]

            return filtered_json

        else:
            print("Warning: No valid JSON detected in response.")
            return []
    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []

# Sample response missing the last "}]"
response = """[{
    "Question": "What is the purpose of the Global Delivery Standard Operating Procedure (GD SOP) Template",
    "Answer": "# Purpose of the GD SOP Template\n\n## Introduction..."
}, {
    "Question": "How to reconcile NRM, RTS, and SIT data?",
    "Answer": "# Reconciling NRM, RTS, and SIT Data\n\n## Introduction..."
}, {
    "Question": "What is the system error handling process?",
    "Answer": ""
}, {
    "Question": "How to monitor job performance?",
    "Answer": "Use the monitoring"
"""

# Call the function
valid_json = extract_valid_json(response)
print(valid_json)









import json
import re

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        # Find the JSON array within the response
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start != -1 and json_end != -1:
            # Extract potential JSON content
            json_text = response[json_start:json_end + 1]

            # Remove control characters that could corrupt JSON parsing
            json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)

            # Fix missing commas between objects inside an array
            json_text = re.sub(r'(?<=\})(?=\{)', '},', json_text)

            # Fix trailing commas before closing braces and brackets
            json_text = re.sub(r',\s*}', '}', json_text)
            json_text = re.sub(r',\s*\]', ']', json_text)

            # Replace malformed "Answer": without quotes or null values
            json_text = re.sub(r'("Answer":\s*)(?=[},\]])', r'\1""', json_text)

            # Ensure valid JSON parsing
            parsed_json = json.loads(json_text)

            # Filter out objects missing both "Question" and "Answer"
            filtered_json = [
                item for item in parsed_json
                if "Question" in item and "Answer" in item and item["Answer"].strip()
            ]

            return filtered_json

        else:
            print("Warning: No valid JSON detected in response.")
            return []
    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []

# Sample response with missing fields
response = """[{
    "Question": "What is the purpose of the Global Delivery Standard Operating Procedure (GD SOP) Template",
    "Answer": "# Purpose of the GD SOP Template\n\n## Introduction..."
}, {
    "Question": "How to reconcile NRM, RTS, and SIT data?",
    "Answer": "# Reconciling NRM, RTS, and SIT Data\n\n## Introduction..."
}, {
    "Question": "What is the system error handling process?",
    "Answer":
}, {
    "Question": "How to monitor job performance?"
}]"""

# Call the function
valid_json = extract_valid_json(response)
print(valid_json)







import json
import re

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        # Find the JSON array within the response
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start != -1 and json_end != -1:
            # Extract potential JSON content
            json_text = response[json_start:json_end + 1]

            # Remove control characters that could corrupt JSON parsing
            json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)

            # Fix missing commas between objects inside an array
            json_text = re.sub(r'(?<=\})(?=\{)', '},', json_text)

            # Fix trailing commas before closing braces and brackets
            json_text = re.sub(r',\s*}', '}', json_text)
            json_text = re.sub(r',\s*\]', ']', json_text)

            # Load JSON after fixes
            parsed_json = json.loads(json_text)

            # Filter out incomplete or empty answers
            filtered_json = [
                item for item in parsed_json
                if item.get("Question") and item.get("Answer") and len(item["Answer"].strip()) > 5
            ]

            return filtered_json

        else:
            print("Warning: No valid JSON detected in response.")
            return []
    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []

# Sample usage
response = """[{
    "Question": "What is the purpose of the Global Delivery Standard Operating Procedure (GD SOP) Template",
    "Answer": "# Purpose of the GD SOP Template\n\n## Introduction..."
}, {
    "Question": "How to reconcile NRM, RTS, and SIT data?",
    "Answer": "# Reconciling NRM, RTS, and SIT Data\n\n## Introduction..."
}, {
    "Question": "What is the system error handling process?",
    "Answer": ""
}, {
    "Question": "How to monitor job performance?",
    "Answer": "Use the monitoring dashboard to check for anomalies."
}]"""

# Call the function
valid_json = extract_valid_json(response)
print(valid_json)












1. Using Prompt Engineering (LLMs Only – No ML Model)
If your goal is to analyze, summarize, and extract insights from banking data, prompt engineering can help.

Example Use Cases for Prompt Engineering:
Transaction Summary:
"Summarize the last 6 months of transactions for a customer and highlight unusual patterns."
Risk Analysis:
"List customers with frequent overdrafts and suggest reasons based on transaction history."
Fraud Detection (Basic Rules-Based):
"Identify transactions that exceed the customer’s usual spending by 50% or more."
Querying Data in Natural Language:
"Which customers made the highest deposits in the last 3 months?"
Customer Segmentation (Basic):
"Group customers based on spending patterns into High Spenders, Moderate Spenders, and Low Spenders."
✅ Best When:

Your data is structured (tables, transactions, customer info).
You only need insights, summaries, and reports.
There’s no need for learning from past trends.
2. Using Machine Learning Models (With or Without LLMs)
If your goal is prediction, automation, or deep analysis, ML models are required.

Example Use Cases for ML Models:
Fraud Detection (Anomaly Detection Model):
Detects patterns of fraudulent transactions based on past fraud cases.
Loan Approval Prediction (Classification Model):
Predicts whether a customer is likely to default on a loan.
Customer Segmentation (Clustering Model):
Groups customers into meaningful segments based on their spending behavior.
Credit Score Estimation (Regression Model):
Predicts a customer’s creditworthiness based on transaction history and income.
Churn Prediction (Classification Model):
Predicts whether a customer will stop using banking services.
✅ Best When:

You need predictions (e.g., "Will this customer default on a loan?").
You want to automate decision-making (e.g., approve or reject a loan).
The system should learn from past data to improve over time.
How to Decide?
Scenario	Use Prompt Engineering	Use ML Model
Extracting insights from past transactions	✅ Yes	❌ No
Generating financial reports	✅ Yes	❌ No
Predicting fraud based on past cases	❌ No	✅ Yes
Approving loans automatically	❌ No	✅ Yes
Identifying top spending customers	✅ Yes	❌ No
Grouping customers based on behavior	✅ Basic	✅ Advanced
Hybrid Approach (Using Both ML & LLMs)
For best results, you can combine both:

Use ML models to detect fraud, predict defaults, or segment customers.
Use Prompt Engineering to explain, summarize, and interact with the results.
Final Thought
If you need only insights & summaries → Use Prompt Engineering
If you need predictions & automation → Use Machine Learning

What exactly do you want to achieve in your PoC? Let me know so I can guide you better!







# Objective:

You are tasked with generating procedural question-and-answer pairs. Generate up to {num_questions} distinct procedural question-answer pairs based on the given input.

# Steps to Read the Input Extracts:

1. The input consists of structured markdown extraction of PDF contents.  
2. Every detail present in the input is important. Carefully read through the input to generate logical procedural question-answer pairs.  
3. Read through descriptions of tables, images, and screenshots present in the input to generate logical procedural answers.  

## Understanding Input Elements:

1. The input provided is a structured markdown extraction of PDF pages.  
2. The input may contain text, images, icons, screenshots, tables, and descriptions.  
3. Inline descriptions for images, screenshots, icons, and tables are included in the input.  

**Input:**  

{paragraph}  

## Step-by-Step Answer Generation:

1. Generate procedural question-answer pairs that cover all necessary steps to follow the procedure.  
2. If tables, images, screenshots, or icons are present within the procedural steps, incorporate relevant descriptions into the answers.  
3. Clearly describe the first step, including referenced sections and key details from the text and descriptions.  
4. Describe the next action or calculation required, integrating relevant notes or table data.  
5. Extract and understand descriptions from the input and add them to procedural answers.  
6. Continue the steps sequentially, ensuring logical progression. Include tables, image descriptions, field names, notes, etc., to enhance the procedural answer's context.  

# Output Instructions:

1. Generate a list of valid JSON objects, each containing keys: `Question_type`, `Question`, and `Answer`.  
2. Store the question type inside the `Question_type` key, and the respective question and answer inside `Question` and `Answer` keys.  
3. Use the following output format to structure the answers properly.  

### **Output Format:**
**Example JSON Output:**  
```json
[
  {
    "Question_type": "Procedural",
    "Question": "How do you process a trade transaction in the system?",
    "Answer": "## Procedure\n\n### Step 1: Access the System\n\n1. Log in using your credentials.\n2. Navigate to the 'Trade Transactions' module.\n\n### Step 2: Enter Trade Details\n\n1. Select 'New Trade Entry'.\n2. Fill in the trade details as per the provided form.\n\n### Step 3: Validate and Submit\n\n1. Cross-check the trade details for accuracy.\n2. Click 'Submit' to finalize the trade entry."
  },
  {
    "Question_type": "Procedural",
    "Question": "How do you retrieve exchange rates for a given date?",
    "Answer": "## Procedure\n\n### Step 1: Open Exchange Rate Module\n\n1. Use the function `BEXR` in MCH.\n2. Select the source as `WM11`.\n\n### Step 2: Specify Date and Fetch Rate\n\n1. Enter the required pay date/value date.\n2. Click 'Fetch' to retrieve the exchange rate for that date."
  }
]








import json
import pandas as pd
import warnings

def extract_valid_json(response):
    """Fix and parse JSON response properly."""
    try:
        # Try parsing as JSON
        return json.loads(response)
    except json.JSONDecodeError:
        print("⚠️ Invalid JSON detected. Attempting to fix...")
        response = response.strip().rstrip(",")  # Remove trailing commas
        response = response.replace("\n", "")  # Remove unnecessary newlines

        # Add closing bracket if missing
        if not response.endswith("]"):
            response += "]"

        try:
            return json.loads(response)  # Retry parsing
        except:
            print("❌ Failed to fix JSON. Returning empty list.")
            return []  # Return empty if still invalid

def save_questions_to_excel(questions, output_file):
    """Save valid questions and answers to an Excel file."""
    
    if not questions:
        print("⚠️ No valid questions found to save!")
        return

    # Ensure list of dictionaries format
    if not isinstance(questions, list) or not all(isinstance(q, dict) for q in questions):
        print("❌ Error: Expected a list of dictionaries but got something else.")
        return

    df = pd.DataFrame(questions)

    # Ensure required columns exist
    required_columns = ["Question_type", "Question", "Answer"]
    for col in required_columns:
        if col not in df.columns:
            df[col] = ""  # Fill missing columns with empty values

    # Suppress warnings related to Excel saving
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        df.to_excel(output_file, index=False, engine="openpyxl")

    print(f"✅ Questions successfully saved to {output_file}")

# Example Usage
response = """[{"Question_type": "Procedural", "Question": "How do you ensure the monthly book in MCH has been released?", "Answer": "Procedure details here..."}, {"Question_type": "Procedural", "Question": "What steps are involved in adding a new fund into Fund Master?", "Answer": "Procedure details here..."}]"""  # Simulated response

parsed_response = extract_valid_json(response)
output_file = r"H:\Question and answer\Test.xlsx"
save_questions_to_excel(parsed_response, output_file)







import pandas as pd

def save_questions_to_excel(questions, output_file):
    """Save questions and answers to an Excel file with proper formatting."""
    
    if not questions:
        print("⚠️ No questions to save!")
        return

    # Ensure input is a list of dictionaries
    if isinstance(questions, list) and all(isinstance(q, dict) for q in questions):
        df = pd.DataFrame(questions)
    else:
        print("❌ Error: Expected a list of dictionaries but got something else.")
        return

    # Define required columns
    required_columns = ["Question Type", "Question", "Answer"]

    # Add missing columns with empty values if they don’t exist
    for col in required_columns:
        if col not in df.columns:
            df[col] = ""  # Fill missing columns with empty values

    # Save using 'openpyxl' to ensure proper formatting
    df.to_excel(output_file, index=False, engine="openpyxl")

    print(f"✅ Questions successfully saved to {output_file}")

# Example Usage
output_file = r"H:\Question and answer\Test.xlsx"  # Ensure proper file extension
save_questions_to_excel(response, output_file)









import json
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate

# 🔹 Azure GPT-4 Turbo Configuration
llm = AzureChatOpenAI(
    temperature=0.2,  
    top_p=0.9,
    api_version="2023-10-01-preview",  # Update based on your Azure API version
    azure_endpoint="https://your-endpoint.openai.azure.com/",  
    deployment_name="gpt-4-turbo",  # Your Azure OpenAI model deployment
    model_kwargs={"response_format": {"type": "json_object"}},  
    max_tokens=16000  # Adjust based on response size
)

# 🔹 Define the Procedural Prompt
PROCEDURAL_PROMPT = """
# Objective:
You are tasked to generate procedural question-answer pairs from the input text.

# Input:
{paragraph}

# Instructions:
1. Generate up to {num_questions} procedural Q&A pairs.
2. Ensure each question follows a step-by-step process.
3. If tables, images, or screenshots are referenced, include descriptions.
4. Return a **valid JSON response** in this format:

{
  "questions": [
    {"question": "Step-by-step question 1", "answer": "Detailed procedural answer 1"},
    {"question": "Step-by-step question 2", "answer": "Detailed procedural answer 2"}
  ]
}
"""

# 🔹 Create a Prompt Template
prompt = PromptTemplate(input_variables=["paragraph", "num_questions"], template=PROCEDURAL_PROMPT)

# 🔹 Create the LLM Chain
llm_chain = LLMChain(llm=llm, prompt=prompt)

# 🔹 Function to Process in Chunks (Avoids Token Limit Issues)
def get_procedural_qa(paragraph, total_questions):
    batch_size = 20  # Process in smaller chunks
    responses = []

    for i in range(0, total_questions, batch_size):
        num_q = min(batch_size, total_questions - i)  # Adjust last batch size
        response = llm_chain.predict(paragraph=paragraph, num_questions=str(num_q))
        response_json = extract_valid_json(response)
        responses.extend(response_json.get("questions", []))  # Merge results
    
    return responses

# 🔹 Function to Validate & Fix JSON Responses
def extract_valid_json(response):
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        print("⚠️ Invalid JSON detected. Attempting to fix...")
        response = response.strip().rstrip(",")  # Remove trailing commas
        fixed_json = response + "}"  # Try closing JSON properly
        try:
            return json.loads(fixed_json)
        except:
            return {"questions": []}  # Return empty if still invalid

# 🔹 Run the Pipeline
paragraph_data = "Your extracted PDF text here..."
total_questions = 40

final_qa_pairs = get_procedural_qa(paragraph_data, total_questions)
print(json.dumps(final_qa_pairs, indent=2))









def get_procedural_qa(paragraph, total_questions):
    batch_size = 20  # Process in smaller chunks
    responses = []

    for i in range(0, total_questions, batch_size):
        num_q = min(batch_size, total_questions - i)  # Adjust last batch size
        response = llm_chain.predict(paragraph=paragraph, num_questions=str(num_q))
        response_json = extract_valid_json(response)
        responses.extend(response_json.get("questions", []))  # Merge results
    
    return responses









# Objective:

You must generate **step-by-step procedural question-answer pairs** without missing any content from the input. **Every extracted detail—text, tables, images, icons, and field names—must be preserved.**  
**The response must always be in valid JSON format.**

---
## **Input Format:**


---
## **Answer Generation Rules:**
1. **Each question must correspond to a procedural step.**  
2. **Include all tables, images, icons, and notes within the corresponding steps.**  
3. **Ensure logical sequencing** (no missing steps or skipped instructions).  
4. **Return a valid JSON array** with no extra text, markdown, or formatting errors.  

---
## **Output Format:**
The output must strictly follow this JSON structure:  

```json
[
    {
        "Question Type": "Procedural",
        "Question": "What is the first step in setting up [Procedure Name]?",
        "Answer": "Step 1: Open the software.\n\n- Navigate to the settings panel.\n- Refer to the image below.\n\n![Settings Panel](image1.png) *Figure 1: Initial setup screen.*"
    },
    {
        "Question Type": "Procedural",
        "Question": "What are the required tools for this process?",
        "Answer": "The following tools are required:\n\n| Tool Name | Version |\n|-----------|---------|\n| XYZ Software | 2.1.0 |\n| USB Driver | Latest |\n\nRefer to Table 1 for software requirements."
    },
    {
        "Question Type": "Procedural",
        "Question": "How to execute the main command?",
        "Answer": "Step 3: Run the following command:\n\n```bash\nexecute_main_task --mode fast\n```\n\nEnsure that the system meets the prerequisites before running the command."
    }
]








# Objective:

Generate **step-by-step** procedural question-answer pairs **without missing any original content**. Every extracted detail—tables, images, icons, field names, and notes—must be **preserved** and properly formatted.

# Steps to Read the Input:

1. The input is a **structured markdown extraction** from PDF content.
2. It contains **text, images, icons, screenshots, tables, and inline descriptions**.
3. Every piece of information must be carefully analyzed to **generate detailed procedural steps**.
4. **Maintain logical sequencing**: No steps should be skipped.

---
## **Input Format:**

{paragrah}

---
## **Answer Generation Rules:**
1. **Generate step-by-step procedural answers** that follow the exact sequence of actions.
2. **If an image, table, or icon is mentioned in the input,** include its **description within the relevant step.**
3. **Do not omit any information**—every extracted detail must be preserved in the answers.
4. **Maintain clear step-by-step formatting** with numbered steps.

---
## **Output Instructions (Strict JSON Format):**
- Return **only a JSON array** without extra text, markdown, or delimiters.
- The JSON structure must contain the following fields:
  - `"Question Type"`: Always `"Procedural"`.
  - `"Question"`: The step-related procedural question.
  - `"Answer"`: The step-by-step procedural answer, including **text, images, tables, and descriptions**.

### **Expected JSON Format:**
```json
[
    {
        "Question Type": "Procedural",
        "Question": "What is the first step in setting up [Procedure Name]?",
        "Answer": "1. Open the software.\n2. Navigate to the settings panel (See Figure 1 below for reference).\n\n![Settings Panel](image1.png) *Figure 1: Initial setup screen.*"
    },
    {
        "Question Type": "Procedural",
        "Question": "What are the required tools for this process?",
        "Answer": "The following tools are required:\n\n| Tool Name | Version |\n|-----------|---------|\n| XYZ Software | 2.1.0 |\n| USB Driver | Latest |\n\nRefer to Table 1 for software requirements."
    },
    {
        "Question Type": "Procedural",
        "Question": "How to execute the main command?",
        "Answer": "Run the following command:\n\n```bash\nexecute_main_task --mode fast\n```\n\nEnsure that the system meets the prerequisites before running the command."
    }
]







# Objective:

You are tasked to generate procedural question-answer pairs. Generate up to (num_questions) distinct procedural question-answer pairs based on the Input. The question-answer pairs should include all important details such as images, tables, icons, field names, notes, etc.

# Steps to Read the Input Extracts:

1. The input is a structured markdown extraction from PDF content.
2. Pay close attention to every detail in the input.
3. Extract the procedural steps, tables, images, screenshots, icons, and any other relevant descriptions.
4. If there are any images, tables, or screenshots mentioned, include their descriptions along with the procedural steps.

## Understanding Input Format:
1. The input is the **structured markdown** extraction of PDF pages, consisting of text, images, icons, screenshots, and tables.
2. Descriptions of images, screenshots, icons, and tables are inline and should be integrated into the procedural answers.

### **Input Example:**

{input}

---
## **Step-by-Step Answer Generation:**

1. Generate distinct **question-answer pairs** covering all steps of the procedure.
2. Ensure to include **details from tables, images, screenshots, icons, and any other visual content** that are related to the procedural steps.
3. Clearly describe the **first step**, including referenced sections and key details from text and descriptions.
4. Describe any next actions, including calculations or references to **tables or images**.
5. Ensure that each step is described **sequentially**, integrating the details (e.g., tables, images, screenshots) where relevant.
6. Include **notes, field names, important points, or icons** when they provide context to the procedure.

---
## **Output Instructions:**
1. **Return ONLY the JSON array** containing the question-answer pairs.
2. The JSON structure must contain the following fields:
   - `Question Type`: Type of question (e.g., "Procedural").
   - `Question`: The actual question based on the procedure.
   - `Answer`: The detailed answer including all references to the text, images, tables, screenshots, etc.
3. Do not include anything else in the output such as additional explanations, markdown, or unnecessary delimiters.
4. Ensure the **JSON response is valid** and properly formatted.

### **Expected Output Format:**
```json
[
    {
        "Question Type": "Procedural",
        "Question": "What is the first step in the procedure?",
        "Answer": "The first step is to open the software, as shown in the image below. The image explains the initial screen layout and buttons to click."
    },
    {
        "Question Type": "Procedural",
        "Question": "What is the tool required to complete the task?",
        "Answer": "The tool required is the XYZ software. Please refer to the table below for software specifications."
    },
    {
        "Question Type": "Procedural",
        "Question": "What should be done after opening the software?",
        "Answer": "After opening the software, click the 'Start' button located on the left panel. See the screenshot below for reference."
    }
]










import json
import re

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        # Find the JSON array within the response
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start != -1 and json_end != -1:
            # Extract the portion of the string that is supposed to be JSON
            json_text = response[json_start:json_end + 1]

            # Clean up the JSON string by removing any unwanted control characters
            json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)  # Remove control characters
            json_text = re.sub(r',\s*}', '}', json_text)  # Fix trailing commas before closing braces
            json_text = re.sub(r',\s*\]', ']', json_text)  # Fix trailing commas before closing brackets

            # Optionally, fix common mistakes like missing commas between items
            json_text = re.sub(r'(?<=\})(?=\{)', '},', json_text)  # Ensure there's a comma between objects
            json_text = re.sub(r'(?<=\])(?=\{)', '},', json_text)  # Ensure a comma between array items and objects

            # Attempt to parse the fixed JSON
            parsed_json = json.loads(json_text)

            # Filter out invalid or incomplete questions and half answers
            filtered_json = []
            for item in parsed_json:
                question = item.get("Question", "").strip()
                answer = item.get("Answer", "").strip()

                # Skip questions with incomplete or too short answers
                if question and answer and len(answer.split()) > 5:  # Ensure answer has enough content
                    filtered_json.append(item)
                else:
                    print(f"Warning: Skipping incomplete question: {question}")

            return filtered_json

        else:
            print("Warning: No valid JSON detected in response.")
            return []
    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []

# Sample usage:
response = """[{
    "Question": "What is the purpose of the Global Delivery Standard Operating Procedure (GD SOP) Template",
    "Answer": "# Purpose of the GD SOP Template\n\n## Introduction..."
}, {
    "Question": "How to reconcile NRM, RTS, and SIT data?",
    "Answer": "# Reconciling NRM, RTS, and SIT Data\n\n## Introduction\nThis procedure outlines..."
}, {
    "Question": "What is the system error handling process?",
    "Answer": ""
}, {
    "Question": "How to monitor job performance?",
    "Answer": "Use the monitoring dashboard to check for anomalies."
}]"""

# Call the function
valid_json = extract_valid_json(response)
print(valid_json)





import json
import re

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        # Find the JSON array within the response
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start != -1 and json_end != -1:
            # Extract the portion of the string that is supposed to be JSON
            json_text = response[json_start:json_end + 1]

            # Clean up the JSON string by removing any unwanted control characters
            json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)  # Remove control characters
            json_text = re.sub(r',\s*}', '}', json_text)  # Fix trailing commas before closing braces
            json_text = re.sub(r',\s*\]', ']', json_text)  # Fix trailing commas before closing brackets

            # Optionally, fix common mistakes like missing commas between items
            json_text = re.sub(r'(?<=\})(?=\{)', '},', json_text)  # Ensure there's a comma between objects

            # Attempt to parse the fixed JSON
            parsed_json = json.loads(json_text)

            # Filter out invalid or incomplete questions and half answers
            filtered_json = []
            for item in parsed_json:
                question = item.get("Question", "").strip()
                answer = item.get("Answer", "").strip()

                # Skip questions with incomplete or too short answers
                if question and answer and len(answer.split()) > 5:  # Ensure answer has enough content
                    filtered_json.append(item)
                else:
                    print(f"Warning: Skipping incomplete question: {question}")

            return filtered_json

        else:
            print("Warning: No valid JSON detected in response.")
            return []
    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []

# Sample usage:
response = """[{
    "Question": "What is the purpose of the Global Delivery Standard Operating Procedure (GD SOP) Template",
    "Answer": "# Purpose of the GD SOP Template\n\n## Introduction..."
}, {
    "Question": "How to reconcile NRM, RTS, and SIT data?",
    "Answer": "# Reconciling NRM, RTS, and SIT Data\n\n## Introduction\nThis procedure outlines..."
}, {
    "Question": "What is the system error handling process?",
    "Answer": ""
}, {
    "Question": "How to monitor job performance?",
    "Answer": "Use the monitoring dashboard to check for anomalies."
}]"""

# Call the function
valid_json = extract_valid_json(response)
print(valid_json)







import json
import re

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        # Find the JSON array within the response
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start != -1 and json_end != -1:
            # Extract the portion of the string that is supposed to be JSON
            json_text = response[json_start:json_end + 1]

            # Fix common issues in malformed JSON:
            # Remove extra commas before closing brackets
            json_text = re.sub(r',\s*}', '}', json_text)
            json_text = re.sub(r',\s*\]', ']', json_text)

            # Remove unwanted control characters
            json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)

            # Attempt to parse the fixed JSON
            parsed_json = json.loads(json_text)

            # Filter out invalid or incomplete questions and half answers
            filtered_json = []
            for item in parsed_json:
                question = item.get("Question", "").strip()
                answer = item.get("Answer", "").strip()

                # Skip questions with incomplete or half answers
                if question and answer and len(answer.split()) > 5:  # Ensure answer has enough content
                    # Ensure the question ends with a proper punctuation
                    if question[-1] not in ['?', '.']:
                        print(f"Warning: Skipping incomplete question: {question}")
                    else:
                        filtered_json.append(item)
                else:
                    print(f"Warning: Skipping question with incomplete or too short answer: {question}")

            return filtered_json

        else:
            print("Warning: No valid JSON detected in response.")
            return []
    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []

# Sample usage:
response = """[{
    "Question": "What is the purpose of the Global Delivery Standard Operating Procedure (GD SOP) Template",
    "Answer": "# Purpose of the GD SOP Template\n\n## Introduction..."
}, {
    "Question": "How to reconcile NRM, RTS, and SIT data?",
    "Answer": "# Reconciling NRM, RTS, and SIT Data\n\n## Introduction\nThis procedure outlines..."
}, {
    "Question": "What is the system error handling process?",
    "Answer": ""
}, {
    "Question": "How to monitor job performance?",
    "Answer": "Use the monitoring dashboard to check for anomalies."
}]"""

# Call the function
valid_json = extract_valid_json(response)
print(valid_json)






import json
import re

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        # Find the JSON array within the response
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start != -1 and json_end != -1:
            # Extract the portion of the string that is supposed to be JSON
            json_text = response[json_start:json_end + 1]

            # Fix common issues in malformed JSON:
            # Remove extra commas before closing brackets
            json_text = re.sub(r',\s*}', '}', json_text)
            json_text = re.sub(r',\s*\]', ']', json_text)
            json_text = re.sub(r',\s*\}', '}', json_text)
            
            # Handle unterminated strings (e.g., missing closing quotes)
            json_text = re.sub(r'(\w+)\s*[:]\s*(\w+)', r'"\1": "\2"', json_text)

            # Remove unwanted control characters that may appear
            json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)

            # Attempt to parse the fixed JSON
            return json.loads(json_text)
        else:
            print("Warning: No valid JSON detected in response.")
            return []
    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON detected. Reason: {e}")
        return []

# Sample usage:
response = """[{
    "Question": "What is the purpose of the Global Delivery Standard Operating Procedure (GD SOP) Template",
    "Answer": "# Purpose of the GD SOP Template\n\n## Introduction..."
}]"""

# Call the function
valid_json = extract_valid_json(response)
print(valid_json)







# Objective:

You are tasked with generating procedural question-answer pairs. Generate up to (num_questions) distinct procedural question-answer pairs based on the Input.

# Steps to Process the Input:

1. The input consists of structured markdown extracted from a PDF.
2. The input may contain text, images, icons, screenshots, and tables.
3. Descriptions for images, screenshots, icons, and tables are included.
4. Every detail in the input is important. Read carefully to generate logical procedural question-answer pairs.

## Answer Generation Guidelines:

1. **Each answer must follow a structured Markdown format** and contain:
   - **Introduction**: A brief explanation of the procedure.
   - **Prerequisites**: Any required tools, software, or dependencies.
   - **Procedure**: A step-by-step guide including any necessary images, tables, or notes.
   
2. **Ensure logical sequencing**:
   - Describe the **first step** clearly, referencing key details from the input.
   - Explain subsequent steps with necessary details, integrating relevant tables or images.
   - Maintain a smooth, sequential flow in the procedural steps.

3. **If there are tables, images, or screenshots relevant to a step, include them in the description**.

## Output Format:

- The response **must be a valid JSON array**.
- Each **Question-Answer pair must be enclosed in `{}`**.
- The JSON array must be enclosed in square brackets `[]`.
- The **Answer field must follow Markdown formatting**.
- **Avoid returning a tuple (`(...)`) or a single object (`{}`) instead of a JSON array (`[...]`).**

---

## **Expected Output Format (Example)**

```json
[
  {
    "Question": "What is the purpose of the Global Delivery Standard Operating Procedure?",
    "Answer": "# Purpose of the GD SOP Template\n\n## Introduction\n\nThe Global Delivery Standard Operating Procedure (GD SOP) outlines the key steps and guidelines for managing...\n\n## Prerequisites\n\n- Access to GD SOP documentation\n- Required user permissions\n\n## Procedure\n\n### Step 1: Access the SOP Document\n\n1. Open the GD SOP system.\n2. Navigate to the 'Standard Procedures' section.\n\n### Step 2: Review Compliance Guidelines\n\n1. Ensure all steps align with the compliance checklist.\n\n![SOP Compliance Guidelines](images/sop_guidelines.png) *Figure 1: Compliance checklist reference.*"
  },
  {
    "Question": "How do you perform DMH Data Maintenance for the AAS25 procedure?",
    "Answer": "# DMH Data Maintenance for AAS25 Procedure\n\n## Introduction\n\nDMH Data Maintenance ensures data consistency for AAS25 processing...\n\n## Prerequisites\n\n- Access to DMH System\n- Required input files\n\n## Procedure\n\n### Step 1: Prepare Reconciliation File\n\n1. Copy the NRM SIT RTS recon file from the previous month.\n2. Rename and save it in the working folder for the current month.\n3. Update the date to reflect the required report period.\n\n### Step 2: Update Formulas\n\n1. Ensure formulas in Column E are correct.\n\n![NRM SIT RTS summary](images/nrm_sit_rts_summary.png) *Figure 1: NRM SIT RTS summary overview.*\n\n### Step 3: Reconcile NRM Data\n\n1. Open the NRM file.\n2. Filter by 'Transaction Type' in Column V and select 'Total'.\n3. Subtotal columns A1-AZ.\n4. Input totals into the reconciliation template in Column D according to the column indicator.\n\n### Step 4: Final Review\n\n1. Verify data consistency.\n2. Submit the finalized reconciliation report."
  }
]










# Objective:

You are tasked to generate procedural question-answer pairs. Generate up to (num_questions) distinct procedural question-answer pairs based on the Input.

# Steps to read the Input extracts:

1. The input is an extraction of PDF contents in a structured markdown format.
2. Every detail present in the input is very important. Read through the input carefully to generate logical procedural question-answer pairs.
3. Read the descriptions of tables, images, and screenshots to generate logical procedural answers.

## Understanding Index Points:

1. The input consists of structured text, images, icons, screenshots, and tables.
2. Inline descriptions for images, screenshots, icons, and tables are also included.

**Input:**
{paragraph}

## Step-by-Step Answer Generation:

1. Generate question-answer pairs covering all steps in the procedure.
2. If tables, images, screenshots, or icons are relevant to the procedural steps, include their descriptions within the steps.
3. Clearly describe each step, referencing sections, tables, and key details.
4. Integrate relevant notes, calculations, or field names to provide context in procedural answers.
5. Ensure a logical step-by-step progression.

# Output Format:

- The response **must be a valid JSON array**.
- Each **Question-Answer pair must be enclosed in curly brackets `{}`**.
- The JSON array must be enclosed in square brackets `[]`.
- Each **Answer must follow Markdown formatting**.

**Expected JSON Output Format:**

```json
[
  {
    "Question": "What is the purpose of the Global Delivery Standard Operating Procedure?",
    "Answer": "# Purpose of the GD SOP Template\n\n## Introduction\n\nThe Global Delivery Standard Operating Procedure (GD SOP) outlines the key steps and guidelines..."
  },
  {
    "Question": "What are the steps involved in DMH Data Maintenance for the AAS25 procedure?",
    "Answer": "# DMH Data Maintenance for AAS25 Procedure\n\n## Introduction\n\nThis procedure ensures accurate data handling for AAS25...\n\n## Prerequisites\n\n- Access to DMH system\n- Required input files...\n\n## Procedure\n\n### Step 1: Prepare Reconciliation File\n\n1. Copy the NRM SIT RTS recon file from the previous month.\n2. Rename and save it in the applicable folder for the current month.\n\n### Step 2: Update Formulas\n\n1. Ensure formulas in Column E are accurate.\n\n![NRM SIT RTS summary](images/nrm_sit_rts_summary.png) *Figure 1: NRM SIT RTS summary overview.*\n\n### Step 3: Reconcile NRM Data\n\n1. Open the NRM file.\n2. Filter by 'Transaction Type' in Column V and select 'Total'.\n3. Subtotal columns A1-AZ.\n4. Input totals into the reconciliation template in Column D...\n\n### Step 4: Final Review\n\n1. Verify data consistency.\n2. Submit the finalized reconciliation report."
  }
]








# Objective:

You are tasked to generate procedural question-answer pairs. Generate up to (num_questions) distinct procedural question-answer pairs based on the Input.

# Steps to read the Input extracts:

1. The input is an extraction of PDF contents in a structured markdown format.
2. Every detail present in the input is very important. Read through the input carefully to generate logical procedural question-answer pairs.
3. Read the descriptions of tables, images, and screenshots to generate logical procedural answers.

## Understanding Index Points:

1. The input consists of structured text, images, icons, screenshots, and tables.
2. Inline descriptions for images, screenshots, icons, and tables are also included.

**Input:**
{paragraph}

## Step-by-Step Answer Generation:

1. Generate question-answer pairs covering all steps in the procedure.
2. If tables, images, screenshots, or icons are relevant to the procedural steps, include their descriptions within the steps.
3. Clearly describe each step, referencing sections, tables, and key details.
4. Integrate relevant notes, calculations, or field names to provide context in procedural answers.
5. Ensure a logical step-by-step progression.

# Output Format:

- The response **must be a valid JSON array**.
- Each question-answer pair must follow this format:

[
  {
    "Question Type": "<Type of Question>",
    "Question": "<Generated Question>",
    "Answer": "# <Title of the Procedure>\n\n## Introduction\n\n<Brief overview of the procedure>\n\n## Prerequisites\n\n<List of tools, software, or dependencies required>\n\n## Procedure\n\n### Step 1: <Step Title>\n\n1. <Detailed explanation of the step>\n2. <Additional details if needed>\n\n![Image Description](image1.png) *Figure 1: Short caption explaining the image.*\n\n### Step 2: <Step Title>\n\n1. <Detailed instructions for this step>\n2. If a command is required, format it as:\n\n```\nbash\ncommand-to-run\n```\n\n### Step 3: <Step Title>\n\n1. Continue describing each step logically.\n2. Include tables, images, screenshots, and relevant field names where applicable.\n\n### Step N: Finalization\n\n1. Wrap up the procedure ensuring all necessary validations are included.\n"
  }
]

- Ensure:
  - The **Answer** follows the required **procedural document format**.
  - The **Introduction, Prerequisites, and Procedure** are properly structured.
  - The **Procedure** includes step-by-step instructions, referencing relevant images, tables, and field names.
  - Formatting uses **Markdown style** for readability.
  - The response **must be a valid JSON array** without additional formatting errors.









# Objective:

You are tasked to generate procedural question-answer pairs. Generate up to (num_questions) distinct procedural question-answer pairs based on the Input.

# Steps to read the Input extracts:

1. The input is an extraction of PDF contents in a structured markdown format.
2. Every detail present in the input is very important. Read through the input carefully to generate logical procedural question-answer pairs.
3. Read the descriptions of tables, images, and screenshots to generate logical procedural answers.

## Understanding Index Points:

1. The input consists of structured text, images, icons, screenshots, and tables.
2. Inline descriptions for images, screenshots, icons, and tables are also included.

**Input:**
{paragraph}

## Step-by-Step Answer Generation:

1. Generate question-answer pairs covering all steps in the procedure.
2. If tables, images, screenshots, or icons are relevant to the procedural steps, include their descriptions within the steps.
3. Clearly describe each step, referencing sections, tables, and key details.
4. Integrate relevant notes, calculations, or field names to provide context in procedural answers.
5. Ensure a logical step-by-step progression.

# Output Format:

- The response **must be a valid JSON array**.
- Each question-answer pair must follow this format:

[
  {
    "Question Type": "<Type of Question>",
    "Question": "<Generated Question>",
    "Answer": {
      "Title": "<Title of the Procedure>",
      "Introduction": "<Brief overview of the procedure>",
      "Prerequisites": "<List of tools, software, or dependencies required>",
      "Procedure": [
        {
          "Step": 1,
          "Title": "<Step Title>",
          "Description": "<Detailed explanation of the step>",
          "Additional Details": "<Tables, images, or screenshots if applicable>"
        },
        {
          "Step": 2,
          "Title": "<Step Title>",
          "Description": "<Detailed explanation of the step>",
          "Additional Details": "<Tables, images, or screenshots if applicable>"
        }
      ]
    }
  }
]

- Ensure:
  - The **Answer** contains **Introduction, Prerequisites, and Procedure** in a structured format.
  - The **Procedure** follows a **step-by-step approach** with sequential numbering.
  - Relevant details such as **tables, images, screenshots, field names, and calculations** are included where applicable.
  - The response **must be a valid JSON array** without additional characters or formatting errors.








# Objective:

You are tasked to generate procedural question-answer pairs. Generate up to (num_questions) distinct procedural question-answer pairs based on the Input.

# Steps to read the Input extracts:

1. The input is an extraction of PDF contents in a structured markdown format.
2. Every detail present in the input is very important. Read through the input carefully to generate logical procedural question-answer pairs.
3. Read the descriptions of tables, images, and screenshots to generate logical procedural answers.

## Understanding Index Points:

1. The input consists of structured text, images, icons, screenshots, and tables.
2. Inline descriptions for images, screenshots, icons, and tables are also included.

**Input:**
{paragraph}

## Step-by-Step Answer Generation:

1. Generate question-answer pairs covering all steps in the procedure.
2. If tables, images, screenshots, or icons are relevant to the procedural steps, include their descriptions within the steps.
3. Clearly describe each step, referencing sections, tables, and key details.
4. Integrate relevant notes, calculations, or field names to provide context in procedural answers.
5. Ensure a logical step-by-step progression.

# Output Format:

- The response **must be a valid JSON array**.
- Each question-answer pair must follow this format:

[
  {
    "Question Type": "<Type of Question>",
    "Question": "<Generated Question>",
    "Answer": "<Generated Answer>"
  }
]

- Ensure:
  - Questions are logically structured.
  - Answers are well-explained and sequential.
  - No information from the input is ignored.
  - JSON formatting is strictly followed without additional characters or formatting errors.










PROCEDURAL_PROMPT = """
# Objective:
Generate up to {num_questions} distinct procedural question-answer pairs based on the Input.

# Instructions:

1. Read the Input carefully to generate **complete** procedural question-answer pairs.
2. **Do NOT stop in the middle of an answer**. Each answer must be a complete step-by-step process.
3. Ensure answers **always contain a full procedure, including an introduction, steps, and conclusions**.

## Input:

{paragraph}

## Output Format:
[
    {{
        "question_type": "Procedural",
        "question": "How to reconcile NRM, RTS, and SIT data in DMH?",
        "answer": "# Reconcile NRM, RTS, and SIT Data in DMH\\n\\n## Introduction\\nThis procedure explains how to reconcile NRM, RTS, and SIT data in DMH, ensuring data consistency and accuracy.\\n\\n## Prerequisites\\n- Access to DMH\\n- Previous month's NRM SIT RTS recon file\\n\\n## Procedure\\n### Step 1: Prepare Reconciliation File\\n1. Copy the NRM SIT RTS recon file of the previous month.\\n2. Rename it and save it in the applicable working folder of the current month.\\n3. Update the date to reflect the required report period.\\n\\n### Step 2: Update Formulas\\n1. Ensure the formulas in Column E are correct.\\n\\n![NRM SIT RTS summary July 19.xlsx](images/nrm_sit_rts_summary_july_19.png) *Figure 1: NRM SIT RTS summary for respective data.*\\n\\n### Step 3: Reconcile NRM Data\\n1. Open the NRM file.\\n2. Filter by column V Transaction Type and select Total."
    }}
]

**DO NOT STOP IN THE MIDDLE OF A SENTENCE.**
**Ensure all answers are fully generated before stopping.**
**Return only valid JSON.**
"""








PROCEDURAL_PROMPT = """
# Objective:
Generate up to {num_questions} distinct procedural question-answer pairs based on the Input.

# Steps to read the Input extracts:

1. Read the Input carefully to generate **logical and complete** procedural question-answer pairs.
2. **Ensure answers include full steps, tables, images, and references**.
3. **Each answer should be fully enclosed within JSON and not cut off**.

## Input:

{paragraph}

## Output Format:
[
    {{
        "question_type": "Procedural",
        "question": "How to reconcile NRM, RTS, and SIT data in DMH?",
        "answer": "# Reconcile NRM, RTS, and SIT Data in DMH\\n\\n## Introduction\\nThis procedure explains how to reconcile NRM, RTS, and SIT data in DMH, ensuring data consistency and accuracy.\\n\\n## Prerequisites\\n- Access to DMH\\n- Previous month's NRM SIT RTS recon file\\n\\n## Procedure\\n### Step 1: Prepare Reconciliation File\\n1. Copy the NRM SIT RTS recon file of the previous month.\\n2. Rename it and save it in the applicable working folder of the current month.\\n3. Update the date to reflect the required report period.\\n\\n### Step 2: Update Formulas\\n1. Ensure the formulas in Column E are correct.\\n\\n![NRM SIT RTS summary July 19.xlsx](images/nrm_sit_rts_summary_july_19.png) *Figure 1: NRM SIT RTS summary for respective data.*\\n\\n### Step 3: Reconcile NRM Data\\n1. Open the NRM file.\\n2. Filter by column V Transaction Type and select Total."
    }}
]

**Ensure the response contains complete answers.**
**Do not return text outside the JSON format.**
**Ensure all JSON objects are fully enclosed.**
"""







PROCEDURAL_PROMPT = """
# Objective:
Generate up to {num_questions} distinct procedural question-answer pairs based on the Input.

# Steps to read the Input extracts:

1. Read the Input carefully to generate **logical and complete** procedural question-answer pairs.
2. **Ensure answers include full steps, tables, images, and references**.
3. **Each answer should be fully enclosed within JSON and not cut off**.

## Input:

{paragraph}

## Output Format:
[
    {{
        "question_type": "Procedural",
        "question": "How to reconcile NRM, RTS, and SIT data in DMH?",
        "answer": "# Reconcile NRM, RTS, and SIT Data in DMH\\n\\n## Introduction\\nThis procedure explains how to reconcile NRM, RTS, and SIT data in DMH, ensuring data consistency and accuracy.\\n\\n## Prerequisites\\n- Access to DMH\\n- Previous month's NRM SIT RTS recon file\\n\\n## Procedure\\n### Step 1: Prepare Reconciliation File\\n1. Copy the NRM SIT RTS recon file of the previous month.\\n2. Rename it and save it in the applicable working folder of the current month.\\n3. Update the date to reflect the required report period.\\n\\n### Step 2: Update Formulas\\n1. Ensure the formulas in Column E are correct.\\n\\n![NRM SIT RTS summary July 19.xlsx](images/nrm_sit_rts_summary_july_19.png) *Figure 1: NRM SIT RTS summary for respective data.*\\n\\n### Step 3: Reconcile NRM Data\\n1. Open the NRM file.\\n2. Filter by column V Transaction Type and select Total."
    }}
]

**Ensure the response contains complete answers.**
**Do not return text outside the JSON format.**
**Ensure all JSON objects are fully enclosed.**
"""







PROCEDURAL_PROMPT = """
# Objective:
Generate {num_questions} distinct procedural question-answer pairs from the given Input.

## Input:

{paragraph}

## Instructions:

- Each question must have a **complete** procedural answer.
- The answer should **fully describe** the steps in a structured markdown format.

### **Output Format**
Return a **list of dictionaries** like this:
[
    {{
        "question_type": "Procedural",
        "question": "How to reconcile NRM, RTS, and SIT Data in DMH?",
        "answer": "# Reconcile NRM, RTS, and SIT Data in DMH\n\n## Introduction\nThis procedure explains how to reconcile NRM, RTS, and SIT data in DMH, ensuring data consistency and accuracy.\n\n## Prerequisites\n\n- Access to DMH\n- Previous month's NRM SIT RTS recon file\n\n## Procedure\n\n### Step 1: Prepare Reconciliation File\n1. Copy the NRM SIT RTS recon file of the previous month.\n2. Rename it and save it in the applicable working folder of the current month.\n3. Update the date to reflect the required report period.\n\n### Step 2: Update Formulas\n1. Ensure the formulas in Column E are correct.\n\n![NRM SIT RTS summary July 19.xlsx](images/nrm_sit_rts_summary_july_19.png) *Figure 1: NRM SIT RTS summary for respective data.*\n\n### Step 3: Reconcile NRM Data\n1. Open the NRM file.\n2. Filter by column V Transaction Type and select Total."
    }}
]

**Ensure:**
- **DO NOT return explanations or text outside JSON.**
- **All answers must be in structured markdown format.**
- **Include proper headings, lists, images, and tables where applicable.**
"""










def generate_questions_in_chunks(paragraph, num_questions, prompt_template, chunk_size=10):
    """Generate questions in smaller chunks to prevent truncation issues."""
    all_questions = []

    for i in range(0, num_questions, chunk_size):
        try:
            chunk_questions = generate_questions(
                paragraph=paragraph,
                num_questions=min(chunk_size, num_questions - i),
                prompt_template=prompt_template
            )
            all_questions.extend(chunk_questions)

        except Exception as e:
            print(f"❌ Error generating chunk for questions: {e}")
            continue  # Continue processing even if one chunk fails

    return all_questions

import json
import re

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start != -1 and json_end != -1:
            json_text = response[json_start:json_end + 1]

            # ✅ Fix unterminated strings and extra commas
            json_text = re.sub(r',\s*}', '}', json_text)  # Remove trailing commas before }
            json_text = re.sub(r',\s*\]', ']', json_text)  # Remove trailing commas before ]

            return json.loads(json_text)

        else:
            print("⚠ Warning: No valid JSON detected in response.")
            return []

    except json.JSONDecodeError as e:
        print(f"❌ Error: Invalid JSON detected. Reason: {e}")
        return []



def generate_questions(paragraph, num_questions, prompt_template):
    """Generate questions and answers while ensuring proper JSON formatting."""
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )

    formatted_prompt = prompt.format(paragraph=paragraph, num_questions=num_questions)

    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )

        print(f"\n📝 Raw LLM Response:\n{response}")  # ✅ Log response before JSON parsing

        questions = extract_valid_json(response)

        if not questions:
            print(f"⚠ Warning: No questions generated. Response was:\n{response}")

        return questions
    except Exception as e:
        print(f"❌ Error generating questions: {e}")
        return []

def remove_duplicates(questions):
    """Removes duplicate questions while retaining question type."""
    if not questions or not isinstance(questions, list):
        print("⚠ Warning: No valid questions provided for deduplication.")
        return []

    seen = set()
    unique_questions = []
    question_type_counts = {}

    for qa in questions:
        if not isinstance(qa, dict):  # ✅ Ensure each item is a dictionary
            continue

        question_text = qa.get("Question", "").strip()
        answer_text = qa.get("Answer", "").strip()
        question_type = qa.get("Question Type", "Unknown").strip()

        if not question_text:  # ✅ Skip empty questions
            continue

        if question_text not in seen:
            unique_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text
            })
            seen.add(question_text)

            # ✅ Count questions per type
            question_type_counts[question_type] = question_type_counts.get(question_type, 0) + 1

    # ✅ Print summary
    print("\n📊 **Final Question Counts After Removing Duplicates:**")
    for q_type, count in question_type_counts.items():
        print(f"{q_type}: {count}")

    return unique_questions

import pandas as pd

def save_questions_to_excel(questions, output_file):
    """Save questions and answers to an Excel file with proper formatting."""
    if not questions:
        print("⚠ No questions to save!")
        return

    # ✅ Ensure each item in 'questions' is a dictionary
    if isinstance(questions, list) and all(isinstance(q, dict) for q in questions):
        df = pd.DataFrame(questions)  # ✅ Convert list of dicts to DataFrame properly
    else:
        print("❌ Error: Expected a list of dictionaries but got something else.")
        return

    # ✅ Ensure DataFrame has the correct columns before saving
    required_columns = ["Question Type", "Question", "Answer"]
    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        print(f"⚠ Warning: Missing columns in DataFrame: {missing_columns}")
        return

    # ✅ Save using 'openpyxl' to prevent formatting issues
    df.to_excel(output_file, index=False, engine="openpyxl")
    print(f"✅ Questions successfully saved to {output_file}.")

def process_pdf_for_multiple_question_types(pdf_path, question_type_counts):
    """Extract text from PDF and generate different question types with custom counts."""
    
    extracted_text = extract_text_from_pdf(pdf_path)
    
    if not extracted_text:
        print(f"No text extracted from {pdf_path}!")
        return []

    question_modules = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-Based": REASONING_PROMPT
    }

    all_questions = []

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(
                generate_questions_in_chunks, extracted_text, num_questions, question_modules[q_type]
            ): q_type
            for q_type, num_questions in question_type_counts.items()
        }

        for future in futures:
            try:
                question_type = futures[future]
                questions = future.result()

                if questions:
                    for qa in questions:
                        all_questions.append({
                            "Question Type": question_type,
                            "Question": qa.get("question", "N/A"),
                            "Answer": qa.get("answer", "N/A")
                        })
                else:
                    print(f"No questions generated for {question_type}")

            except Exception as e:
                print(f"Error processing {futures[future]}: {e}")

    return remove_duplicates(all_questions)


def process_pdfs_in_directory(directory_path, question_type_counts, output_folder="output"):
    """Processes all PDF files in a given directory."""
    
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for filename in os.listdir(directory_path):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(directory_path, filename)

            try:
                print(f"Processing: {pdf_path}")

                questions = process_pdf_for_multiple_question_types(pdf_path, question_type_counts)

                output_file = os.path.join(output_folder, filename.replace(".pdf", "_questions.xlsx"))
                save_questions_to_excel(questions, output_file)

            except Exception as e:
                print(f"Error processing {pdf_path}: {e}")

if __name__ == "__main__":
    pdf_directory = "pdf_files"
    question_type_counts = {"Factual": 50, "Procedural": 20, "Conceptual": 15}

    process_pdfs_in_directory(pdf_directory, question_type_counts)








PROCEDURAL_PROMPT

# Objective:
You are an AI assistant that **extracts procedural question-answer pairs** from a structured markdown input.

## **Input Format**
- **Input:** A structured **markdown extraction** from a PDF.
- **Content:** The input includes **text, images, icons, screenshots, tables, and inline descriptions**.
- **Number of Questions:** You must generate exactly `{num_questions}` distinct procedural **question-answer pairs**.

---

## **Steps to Process the Input:**
1. Read the **structured markdown content** carefully.
2. Extract **key procedural details** to generate **logical procedural question-answer pairs**.
3. If **tables, images, screenshots, or icons** are present, **include them inside the procedural markdown answer**.
4. The **number of questions must match** `{num_questions}`.
5. **Ensure all JSON brackets `{}` and `[]` are properly closed**—do not generate invalid JSON.

---

# **Output Format**
The response **MUST** be a **valid JSON array**, following this structure:

```json
[
    {
        "question_type": "Procedure",
        "question": "What is the procedure for [specific process]?",
        "answer": "# Title of the Procedure\n\n## Introduction\n\nA brief overview of the procedure, explaining what it is and why it's needed.\n\n## Prerequisites\n\n- List of tools, software, or dependencies required before starting.\n\n## Procedure\n\n### Step 1: [Title of Step]\n\n1. Description of the step explaining what needs to be done.\n2. Additional details if needed.\n\n![Image Description](image1.png)\n\n*Figure 1: Short caption explaining the image*\n\n### Step 2: [Title of Step]\n\n1. Detailed instructions for this step.\n2. If a command is required, format it as:\n\n```bash\ncommand-to-run\n```"
    },
    {
        "question_type": "Procedure",
        "question": "What are the steps involved in [another process]?",
        "answer": "# Another Procedure\n\n## Introduction\n\nOverview of the new procedure...\n\n## Prerequisites\n\n- Required tools and dependencies.\n\n## Procedure\n\n### Step 1: [Step Name]\n\n1. Explanation of the step.\n2. Additional details.\n\n### Step 2: [Step Name]\n\n1. Further instructions.\n\n```bash\nexample-command\n```"
    }
]







## **Output Format**
The response **must** be a **valid JSON array** with the following structure:

```json
[
    {
        "question_type": "Procedure",
        "question": "What is the procedure for [specific process]?",
        "answer": "# Title of the Procedure\n\n## Introduction\n\nA brief overview of the procedure, explaining what it is and why it's needed.\n\n## Prerequisites\n\n- List of tools, software, or dependencies required before starting.\n\n## Procedure\n\n### Step 1: [Title of Step]\n\n1. Description of the step explaining what needs to be done.\n2. Additional details if needed.\n\n![Image Description](image1.png)\n\n*Figure 1: Short caption explaining the image*\n\n### Step 2: [Title of Step]\n\n1. Detailed instructions for this step.\n2. If a command is required, format it as:\n\n```bash\ncommand-to-run\n```"
    },
    {
        "question_type": "Procedure",
        "question": "What are the steps involved in [another process]?",
        "answer": "# Another Procedure\n\n## Introduction\n\nOverview of the new procedure...\n\n## Prerequisites\n\n- Required tools and dependencies.\n\n## Procedure\n\n### Step 1: [Step Name]\n\n1. Explanation of the step.\n2. Additional details.\n\n### Step 2: [Step Name]\n\n1. Further instructions.\n\n```bash\nexample-command\n```"
    }
]








import pandas as pd

def save_questions_to_excel(questions, output_file):
    """Save questions and answers to an Excel file with proper formatting."""
    if not questions:
        print("⚠ No questions to save!")
        return

    # ✅ Ensure each item in 'questions' is a dictionary
    if isinstance(questions, list) and all(isinstance(q, dict) for q in questions):
        df = pd.DataFrame(questions)  # ✅ Convert list of dicts to DataFrame properly
    else:
        print("❌ Error: Expected a list of dictionaries but got something else.")
        return

    # ✅ Ensure DataFrame has the correct columns before saving
    required_columns = ["Question Type", "Question", "Answer"]
    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        print(f"⚠ Warning: Missing columns in DataFrame: {missing_columns}")
        return

    # ✅ Save using 'openpyxl' to prevent formatting issues
    df.to_excel(output_file, index=False, engine="openpyxl")
    print(f"✅ Questions successfully saved to {output_file}.")

def remove_duplicates(questions):
    """Removes duplicate questions while retaining question type."""
    if not questions or not isinstance(questions, list):
        print("⚠ Warning: No valid questions provided for deduplication.")
        return []

    seen = set()
    unique_questions = []
    question_type_counts = {}

    for qa in questions:
        if not isinstance(qa, dict):  # ✅ Ensure each item is a dictionary
            continue

        question_text = qa.get("Question", "").strip()
        answer_text = qa.get("Answer", "").strip()
        question_type = qa.get("Question Type", "Unknown").strip()

        if not question_text:  # ✅ Skip empty questions
            continue

        if question_text not in seen:
            unique_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text
            })
            seen.add(question_text)

            # ✅ Count questions per type
            question_type_counts[question_type] = question_type_counts.get(question_type, 0) + 1

    # ✅ Print summary
    print("\n📊 **Final Question Counts After Removing Duplicates:**")
    for q_type, count in question_type_counts.items():
        print(f"{q_type}: {count}")

    return unique_questions  # ✅ Always return a structured list









.import pandas as pd

def save_questions_to_excel(questions, output_file):
    """Save questions and answers to an Excel file with proper column formatting."""
    if questions:
        # ✅ Ensure correct structure before converting to DataFrame
        cleaned_data = []
        for qa in questions:
            cleaned_data.append({
                "Question Type": qa.get("Question Type", "Unknown"),  # ✅ Ensure key exists
                "Question": qa.get("Question", "N/A"),
                "Answer": qa.get("Answer", "N/A")
            })

        # ✅ Convert to DataFrame and ensure column order
        df = pd.DataFrame(cleaned_data, columns=["Question Type", "Question", "Answer"])

        # ✅ Save using 'openpyxl' to prevent Excel formatting issues
        df.to_excel(output_file, index=False, engine="openpyxl")

        print(f"✅ Questions successfully saved to {output_file}.")
    else:
        print("⚠ No questions to save!")









def save_questions_to_excel(questions, output_file):
    """Save generated questions and answers to an Excel file with proper column formatting."""
    if questions:
        try:
            # ✅ Convert list of dictionaries into a properly formatted DataFrame
            df = pd.DataFrame(questions)

            # ✅ Ensure correct column order
            df = df[['Question Type', 'Question', 'Answer']]

            # ✅ Save the DataFrame to an Excel file with proper formatting
            df.to_excel(output_file, index=False, sheet_name="Questions")

            print(f"✅ Questions and answers saved to {output_file}")

        except Exception as e:
            print(f"❌ Error saving questions to Excel: {e}")
    else:
        print("⚠ No questions to save!")









import json

def extract_valid_json(response):
    """Extract and fix malformed JSON from an LLM response."""
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')

        if json_start != -1 and json_end != -1:
            json_text = response[json_start:json_end + 1]

            # ✅ Fix unclosed JSON by ensuring all brackets are balanced
            if json_text.count('{') > json_text.count('}'):
                json_text += '}' * (json_text.count('{') - json_text.count('}'))

            if json_text.count('[') > json_text.count(']'):
                json_text += ']' * (json_text.count('[') - json_text.count(']'))

            return json.loads(json_text)  # ✅ Parse JSON safely

        else:
            print("⚠ Warning: No valid JSON detected in response.")
            return []

    except json.JSONDecodeError as e:
        print(f"❌ Error: Invalid JSON detected. Reason: {e}")

        # ✅ Attempt a partial recovery: Extract JSON-like parts
        try:
            fixed_json = json.loads(json_text + ']}')  # Try closing brackets
            print("✅ Fixed JSON automatically.")
            return fixed_json
        except:
            return []






def generate_questions(paragraph, num_questions, prompt_template):
    """Generate questions and answers while ensuring proper JSON formatting."""
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )

    formatted_prompt = prompt.format(paragraph=paragraph, num_questions=num_questions)

    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )

        print(f"\n📝 Raw LLM Response:\n{response}")  # ✅ Log response before JSON parsing

        questions = extract_valid_json(response)

        if not questions:
            print(f"⚠ Warning: No questions generated. Response was:\n{response}")

        return questions
    except Exception as e:
        print(f"❌ Error generating questions: {e}")
        return []






def generate_questions(paragraph, num_questions, prompt_template):
    """Generate questions and answers."""
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )

    formatted_prompt = prompt.format(paragraph=paragraph, num_questions=num_questions)

    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        questions = extract_valid_json(response)

        if not questions:
            print(f"⚠ Warning: No questions generated. Response was:\n{response}")

        return questions
    except Exception as e:
        print(f"❌ Error generating questions: {e}")
        return []







def generate_questions(paragraph, question_type, num_questions, prompt_template):
    """Generate questions and answers for a specific type."""
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )

    formatted_prompt = prompt.format(paragraph=paragraph, num_questions=num_questions)

    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        questions = extract_valid_json(response)

        if not questions:
            print(f"⚠ Warning: No questions generated for {question_type}. Response was:\n{response}")

        return questions
    except Exception as e:
        print(f"❌ Error generating {question_type} questions: {e}")
        return []





def extract_valid_json(response):
    """Extract valid JSON from a response string, handling errors gracefully."""
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            json_text = response[json_start:json_end + 1]
            return json.loads(json_text)  # ✅ Properly parse JSON
        else:
            print("⚠ Warning: No valid JSON detected in response.")
            return []
    except json.JSONDecodeError as e:
        print(f"❌ Error: Invalid JSON detected. Reason: {e}")
        return []









def remove_duplicates(questions):
    """Removes duplicate questions while retaining question type and counting occurrences."""
    if not questions:
        return []

    seen = set()
    unique_questions = []
    question_type_counts = {}

    for qa in questions:
        # Ensure all required keys exist
        question_text = qa.get("Question", "").strip()
        answer_text = qa.get("Answer", "").strip()
        question_type = qa.get("Question Type", "Unknown").strip()

        # Skip completely empty questions
        if not question_text:
            continue

        if question_text not in seen:
            unique_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text
            })
            seen.add(question_text)

            # Count questions per type
            question_type_counts[question_type] = question_type_counts.get(question_type, 0) + 1

    # Print the final counts cleanly
    print("\n📊 **Final Question Counts After Removing Duplicates:**")
    for q_type, count in question_type_counts.items():
        print(f"{q_type}: {count}")

    return unique_questions, question_type_counts  # ✅ Return counts properly












def remove_duplicates(questions):
    """Removes duplicate questions while retaining question type."""
    if not questions:
        return []

    seen = set()
    unique_questions = []

    for qa in questions:
        # Ensure all required keys exist
        question_text = qa.get("Question", "").strip()
        answer_text = qa.get("Answer", "").strip()
        question_type = qa.get("Question Type", "Unknown").strip()

        # Skip completely empty questions
        if not question_text:
            continue

        if question_text not in seen:
            unique_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text
            })
            seen.add(question_text)

    print(f"Removed duplicates. {len(unique_questions)} unique questions remain.")
    return unique_questions










def remove_duplicates(questions):
    """Removes duplicate questions from the list while handling missing keys."""
    if not questions:
        return []

    seen = set()
    unique_questions = []

    for qa in questions:
        # Ensure keys exist, if missing, use default values
        question_text = qa.get("Question", "").strip()
        answer_text = qa.get("Answer", "").strip()

        # Skip completely empty questions
        if not question_text:
            continue

        if question_text not in seen:
            unique_questions.append({"Question": question_text, "Answer": answer_text})
            seen.add(question_text)

    print(f"Removed duplicates. {len(unique_questions)} unique questions remain.")
    return unique_questions







import os
import json
import fitz  # PyMuPDF for extracting text from PDFs
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize Azure OpenAI LLM Model
llm = AzureChatOpenAI(
    temperature=0,
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
)

# 🟢 1️⃣ Define Prompts for Each Question Type
FACTUAL_PROMPT = "Generate {num_questions} factual questions with answers from this text: {paragraph}"
PROCEDURAL_PROMPT = "Generate {num_questions} procedural questions with answers from this text: {paragraph}"
CONCEPTUAL_PROMPT = "Generate {num_questions} conceptual questions with answers from this text: {paragraph}"
INFERENTIAL_PROMPT = "Generate {num_questions} inferential questions with answers from this text: {paragraph}"
REASONING_PROMPT = "Generate {num_questions} reasoning-based questions with answers from this text: {paragraph}"

# 🟢 2️⃣ Extract Text from PDF
def extract_text_from_pdf(pdf_path):
    """Extract text from a given PDF file."""
    try:
        with fitz.open(pdf_path) as doc:
            text = "\n".join([page.get_text("text") for page in doc])
        return text.strip()
    except Exception as e:
        print(f"Error extracting text from {pdf_path}: {e}")
        return ""

# 🟢 3️⃣ Generate Questions & Answers for a Specific Type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    """Generate questions and answers for a specific type."""
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )

    formatted_prompt = prompt.format(paragraph=paragraph, num_questions=num_questions)

    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# 🟢 4️⃣ Extract Valid JSON Response
def extract_valid_json(response):
    """Extract valid JSON from a response string."""
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# 🟢 5️⃣ Remove Duplicate Questions
def remove_duplicates(questions):
    """Removes duplicate questions from the list."""
    if not questions:
        return []

    seen = set()
    unique_questions = []

    for qa in questions:
        question_text = qa.get("Question", "").strip()

        if question_text not in seen:
            unique_questions.append(qa)
            seen.add(question_text)

    print(f"Removed duplicates. {len(unique_questions)} unique questions remain.")
    return unique_questions

# 🟢 6️⃣ Define Functions for Each Question Type
def generate_factual_questions(paragraph, num_questions):
    return generate_questions(paragraph, "Factual", num_questions, FACTUAL_PROMPT)

def generate_procedural_questions(paragraph, num_questions):
    return generate_questions(paragraph, "Procedural", num_questions, PROCEDURAL_PROMPT)

def generate_conceptual_questions(paragraph, num_questions):
    return generate_questions(paragraph, "Conceptual", num_questions, CONCEPTUAL_PROMPT)

def generate_inferential_questions(paragraph, num_questions):
    return generate_questions(paragraph, "Inferential", num_questions, INFERENTIAL_PROMPT)

def generate_reasoning_questions(paragraph, num_questions):
    return generate_questions(paragraph, "Reasoning-Based", num_questions, REASONING_PROMPT)

# 🟢 7️⃣ Run Multiple Question Types with Custom Counts
def process_pdf_for_multiple_question_types(pdf_path, question_type_counts):
    """Extract text from PDF and generate different question types with custom counts."""
    
    extracted_text = extract_text_from_pdf(pdf_path)
    
    if not extracted_text:
        print(f"No text extracted from {pdf_path}!")
        return []

    # Dictionary of available question modules
    question_modules = {
        "Factual": generate_factual_questions,
        "Procedural": generate_procedural_questions,
        "Conceptual": generate_conceptual_questions,
        "Inferential": generate_inferential_questions,
        "Reasoning-Based": generate_reasoning_questions
    }

    # Validate selected types
    selected_types = [q for q in question_type_counts.keys() if q in question_modules]
    
    if not selected_types:
        print("No valid question types selected!")
        return []

    # Store generated questions
    all_questions = []

    # Run selected question types in parallel
    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(question_modules[q_type], extracted_text, num_questions): q_type
            for q_type, num_questions in question_type_counts.items()
        }

        for future in futures:
            try:
                question_type = futures[future]
                questions = future.result()

                if questions:
                    for qa in questions:
                        all_questions.append({
                            "Question Type": question_type,
                            "Question": qa.get("question", "N/A"),
                            "Answer": qa.get("answer", "N/A")
                        })
                else:
                    print(f"No questions generated for {question_type}")

            except Exception as e:
                print(f"Error processing {futures[future]}: {e}")

    # Remove duplicate questions before returning
    return remove_duplicates(all_questions)

# 🟢 8️⃣ Save Questions & Answers to Excel
def save_questions_to_excel(questions, output_file):
    """Save generated questions and answers to an Excel file."""
    if questions:
        df = pd.DataFrame(questions)
        df.to_excel(output_file, index=False)
        print(f"Questions saved to {output_file}")
    else:
        print("No questions to save!")

# 🟢 9️⃣ Process All PDFs in a Directory
def process_pdfs_in_directory(directory_path, question_type_counts, output_folder="output"):
    """Processes all PDF files in a given directory."""
    
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for filename in os.listdir(directory_path):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(directory_path, filename)

            try:
                print(f"Processing: {pdf_path}")

                # Extract text from PDF and generate questions
                questions = process_pdf_for_multiple_question_types(pdf_path, question_type_counts)

                # Save results to Excel
                output_file = os.path.join(output_folder, filename.replace(".pdf", "_questions.xlsx"))
                save_questions_to_excel(questions, output_file)

            except Exception as e:
                print(f"Error processing {pdf_path}: {e}")

# 🟢 🔟 Run with Custom Question Counts for Each Type
if __name__ == "__main__":
    # Example Directory Path
    pdf_directory = "pdf_files"

    # Define different number of questions for each selected type
    question_type_counts = {
        "Factual": 50,  # Generate 50 factual questions
        "Procedural": 20,  # Generate 20 procedural questions
        "Conceptual": 15  # Generate 15 conceptual questions
    }

    # Process all PDFs in the directory
    process_pdfs_in_directory(pdf_directory, question_type_counts)









import fitz  # PyMuPDF
import os
from PIL import Image
import base64
from langchain.schema import HumanMessage
from langchain.chat_models import AzureChatOpenAI

def extract_text_from_pdf(pdf_path, output_folder="pdf_images"):
    os.makedirs(output_folder, exist_ok=True)
    doc = fitz.open(pdf_path)

    extract_list = ""
    
    for i, page in enumerate(doc):
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        image_path = os.path.join(output_folder, f"page_{i+1}.png")
        img.save(image_path, "PNG")
        
        print(f"\n--- Processing Page {i+1} ---\n")

        # Send image to GPT-40
        with open(image_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode("utf-8")

        message = HumanMessage(
            content=[
                {
                    "type": "text",
                    "objective": "You are an AI assistant that extracts data from documents and returns them in structured markdown format...",
                },
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},
                }
            ]
        )

        # Call GPT-40 using Azure OpenAI
        llm = AzureChatOpenAI(
            temperature=0,
            api_version="your_api_version",
            azure_endpoint="your_azure_endpoint",
            deployment_name="ssgpt-40",
            seed=3
        )

        response = llm.invoke([message])
        print("Response:", response)

        extract_list += response.content

    return extract_list








Vivek,

I ran the same code today on yesterday's file, but I am getting a different response. How can we resolve this issue where the output changes with each run? Additionally, the results are not as good as yesterday's, even though the previous results were good.







import json
import re

def ensure_valid_json(response):
    """
    Ensures the response is valid JSON.
    - If it's a JSON string, convert it to a Python object.
    - If it's already a dictionary or list, return as is.
    - If malformed, attempt to extract valid JSON from it.
    """
    if isinstance(response, (dict, list)):
        return response  # Already valid JSON

    elif isinstance(response, str):
        try:
            return json.loads(response)  # Directly parse JSON string
        except json.JSONDecodeError:
            print("⚠ Warning: Malformed JSON detected, attempting cleanup.")
            extracted_json = extract_json_from_text(response)
            if extracted_json:
                return extracted_json

            print("❌ Failed to recover JSON, returning raw response.")
            return {"error": "Invalid JSON", "raw_response": response}

    else:
        print("❌ Unsupported response type, returning raw response.")
        return {"error": "Unsupported response type", "raw_response": response}

def extract_json_from_text(response_text):
    """
    Extracts valid JSON from text if the response contains extra content.
    Handles cases where JSON is wrapped in Markdown or has extra text.
    """
    try:
        # Look for JSON inside ```json\n ... \n``` blocks
        json_match = re.search(r'```json\n([\s\S]+?)\n```', response_text)
        if json_match:
            json_str = json_match.group(1).strip()
        else:
            json_str = response_text.strip()

        # Attempt to parse extracted JSON
        return json.loads(json_str)
    
    except json.JSONDecodeError:
        print("⚠ Warning: Failed to extract valid JSON.")
        return None

def post_process_questions(response):
    """
    Processes and cleans up the JSON response from `generate_questions()`
    - Ensures valid JSON
    - Extracts structured questions
    - Handles malformed JSON, extra text, and nested structures
    """
    processed_response = ensure_valid_json(response)

    if isinstance(processed_response, str):
        print(f"⚠ Warning: Received a raw string response: {processed_response}")
        return []

    questions_list = []

    # If response is a list of Q&A pairs
    if isinstance(processed_response, list):
        for qa in processed_response:
            question_text = qa.get("question", "N/A").strip()
            answer_text = qa.get("answer", "N/A")

            if isinstance(answer_text, dict):  # Handle structured answers
                formatted_answer = format_complex_answer(answer_text)
            else:
                formatted_answer = str(answer_text).strip()

            questions_list.append({
                "Question": question_text,
                "Answer": formatted_answer,
                "Question Type": qa.get("question_type", "Unknown")
            })

    elif isinstance(processed_response, dict):
        # If response is a single dictionary instead of a list
        question_text = processed_response.get("question", "N/A").strip()
        answer_text = processed_response.get("answer", "N/A")

        formatted_answer = format_complex_answer(answer_text) if isinstance(answer_text, dict) else str(answer_text).strip()

        questions_list.append({
            "Question": question_text,
            "Answer": formatted_answer,
            "Question Type": processed_response.get("question_type", "Unknown")
        })

    return questions_list

def format_complex_answer(answer_dict):
    """
    Converts complex structured answers into a readable format.
    Handles nested steps, images, and descriptions.
    """
    formatted_answer = ""

    for key, value in answer_dict.items():
        if isinstance(value, str):
            formatted_answer += f"**{key}:** {value}\n"
        elif isinstance(value, list):
            formatted_answer += f"**{key}:**\n"
            for item in value:
                if isinstance(item, str):
                    formatted_answer += f"- {item}\n"
                elif isinstance(item, dict):  # Handle nested dictionaries
                    for sub_key, sub_value in item.items():
                        formatted_answer += f"  - **{sub_key}:** {sub_value}\n"
        elif isinstance(value, dict):
            formatted_answer += f"**{key}:**\n"
            for sub_key, sub_value in value.items():
                formatted_answer += f"  - **{sub_key}:** {sub_value}\n"

    return formatted_answer.strip()








import json
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

def ensure_dict(response):
    """
    Ensures the response is a dictionary or list.
    If it's a JSON string, convert it.
    If it's already a dictionary or list, return it as is.
    Otherwise, return as a raw string.
    """
    if isinstance(response, (dict, list)):  # If response is already valid
        return response  
    elif isinstance(response, str):  
        try:
            return json.loads(response)  # Convert JSON string to Python object
        except json.JSONDecodeError:
            print("❌ Cleanup failed, returning as raw string.")
            return response  # Return as a raw string if JSON parsing fails
    else:
        print("❌ Unsupported response type, returning as is.")
        return response  # Return as is if it's neither string nor dict

def extract_questions(response):
    """
    Extracts questions from a structured LLM response.
    Supports different formats, including:
    - Simple Q&A format
    - Nested answers (procedures, steps, images)
    - Invalid JSON handling
    """
    processed_response = ensure_dict(response)

    if isinstance(processed_response, str):
        print(f"⚠ Warning: Received a raw string response instead of structured data: {processed_response}")
        return []

    questions_list = []

    # If the response is a list of dictionaries (standard case)
    if isinstance(processed_response, list):
        for qa in processed_response:
            question_text = qa.get("question", "N/A").strip()
            answer_text = qa.get("answer", "N/A")

            # Handle complex answer structures (e.g., Procedures, Steps, Images)
            if isinstance(answer_text, dict):
                formatted_answer = format_complex_answer(answer_text)
            else:
                formatted_answer = str(answer_text).strip()

            questions_list.append({
                "Question": question_text,
                "Answer": formatted_answer,
                "Question Type": qa.get("question_type", "Unknown")
            })

    elif isinstance(processed_response, dict):  
        # If response is a single dictionary instead of a list
        question_text = processed_response.get("question", "N/A").strip()
        answer_text = processed_response.get("answer", "N/A")

        formatted_answer = format_complex_answer(answer_text) if isinstance(answer_text, dict) else str(answer_text).strip()

        questions_list.append({
            "Question": question_text,
            "Answer": formatted_answer,
            "Question Type": processed_response.get("question_type", "Unknown")
        })

    return questions_list

def format_complex_answer(answer_dict):
    """
    Converts complex structured answers into readable format.
    Handles nested steps, images, and descriptions.
    """
    formatted_answer = ""

    for key, value in answer_dict.items():
        if isinstance(value, str):
            formatted_answer += f"**{key}:** {value}\n"
        elif isinstance(value, list):
            formatted_answer += f"**{key}:**\n"
            for item in value:
                if isinstance(item, str):
                    formatted_answer += f"- {item}\n"
                elif isinstance(item, dict):  # Handle images, descriptions, or other metadata
                    for sub_key, sub_value in item.items():
                        formatted_answer += f"  - **{sub_key}:** {sub_value}\n"
        elif isinstance(value, dict):
            formatted_answer += f"**{key}:**\n"
            for sub_key, sub_value in value.items():
                formatted_answer += f"  - **{sub_key}:** {sub_value}\n"

    return formatted_answer.strip()

def generate_questions(paragraph, question_type, num_questions, prompt_template, llm):
    """
    Generates questions using an LLM based on the given prompt template.
    Handles complex and nested JSON responses.
    """

    # Create the prompt template
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_template
    )

    # Fill the prompt using format
    fill_prompt = prompt.format(
        paragraph=paragraph, 
        num_questions=num_questions, 
        question_type=question_type
    )

    print(f"✅ Generated Prompt: \n{fill_prompt}")

    # Create the LLM chain
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        # Call LLM to generate questions
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions),
            question_type=str(question_type)
        )

        print(f"✅ Raw LLM Response: {response}")

        # Process and extract valid questions
        questions = extract_questions(response)

        print(f"✅ Extracted Questions: {questions}")

        return questions

    except Exception as e:
        print(f"❌ Error generating questions: {e}")
        return []








import json
import re

def clean_json_response(response):
    """
    Cleans an LLM response that contains improperly formatted JSON.
    Extracts and returns a properly formatted dictionary or list.
    """

    if isinstance(response, dict):
        return response  # Already a dictionary, return as is

    if not isinstance(response, str):
        print("❌ Unsupported data type, returning as is.")
        return response  # Return as is if it's neither string nor dict

    try:
        # Try parsing directly
        return json.loads(response)
    except json.JSONDecodeError:
        print("❌ Invalid JSON format, attempting cleanup...")

        # 1️⃣ Remove any unwanted characters (like extra backticks or non-JSON content)
        response = response.strip("```json").strip("```").strip()

        # 2️⃣ Extract JSON objects manually using regex
        json_match = re.search(r"(\{.*\}|\[.*\])", response, re.DOTALL)

        if json_match:
            json_data = json_match.group(1)
            try:
                return json.loads(json_data)  # Try parsing extracted JSON
            except json.JSONDecodeError:
                print("❌ Cleanup failed, returning as raw string.")
                return response  # Return raw response if parsing fails

    return response  # If everything fails, return original response


def generate_questions(paragraph, question_type, num_questions, prompt_template, llm):
    """
    Generates questions using an LLM based on the given prompt template.
    Ensures JSON response is properly parsed.
    """

    # Create the prompt template
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_template
    )

    # Fill the prompt using format
    fill_prompt = prompt.format(
        paragraph=paragraph, 
        num_questions=num_questions, 
        question_type=question_type
    )

    print(f"✅ Generated Prompt: \n{fill_prompt}")

    # Create the LLM chain
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        # Call LLM to generate questions
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions),
            question_type=str(question_type)
        )

        # Ensure response is valid JSON
        processed_response = clean_json_response(response)

        print(f"✅ LLM Processed Response: {processed_response}")

        return processed_response

    except Exception as e:
        print(f"❌ Error generating questions: {e}")
        return []









import json
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

def ensure_dict(response):
    """
    Ensures the response is a dictionary.
    If it's a JSON string, convert it.
    If it's already a dictionary, return it as is.
    """
    if isinstance(response, dict):
        return response  # Already a dictionary, return as is
    elif isinstance(response, str):  
        try:
            return json.loads(response)  # Convert JSON string to dictionary
        except json.JSONDecodeError:
            print("❌ Invalid JSON format, returning as string.")
            return response  # Keep it as a string if conversion fails
    else:
        print("❌ Unsupported data type, returning as is.")
        return response  # Return as is if it's neither string nor dict

def extract_valid_json(response):
    """Extract valid JSON from a response string."""
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("❌ Invalid JSON detected.")
    return []

def generate_questions(paragraph, question_type, num_questions, prompt_template, llm):
    """
    Generates questions using an LLM based on the given prompt template.
    Ensures JSON response is properly parsed.
    """

    # Create the prompt template
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_template
    )

    # Fill the prompt using format
    fill_prompt = prompt.format(
        paragraph=paragraph, 
        num_questions=num_questions, 
        question_type=question_type
    )

    print(f"✅ Generated Prompt: \n{fill_prompt}")

    # Create the LLM chain
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        # Call LLM to generate questions
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions),
            question_type=str(question_type)
        )

        # Ensure response is a valid dictionary
        processed_response = ensure_dict(response)

        print(f"✅ LLM Response: {processed_response}")

        return processed_response

    except Exception as e:
        print(f"❌ Error generating questions: {e}")
        return []






def generate_questions_by_type(paragraph, question_type, total_questions, chunk_size=10):
    question_functions = {
        "Factual": generate_factual_questions,
        "Procedural": generate_procedural_questions,
        "Conceptual": generate_conceptual_questions,
        "Inferential": generate_inferential_questions,
        "Reasoning-based": generate_reasoning_questions,
    }

    question_type = question_type.strip().capitalize()

    if question_type not in question_functions:
        print(f"❌ Error: {question_type} is not a valid question type.")
        return []

    print(f"✅ Generating {total_questions} {question_type} questions...")
    questions = question_functions[question_type](paragraph, total_questions, chunk_size)

    if not questions:
        print(f"❌ No questions generated for {question_type}.")
        return []

    formatted_questions = []
    
    # 🔹 Process and format each question-answer pair
    for qa in questions:
        question_text = qa.get("question", "N/A")
        answer_text = qa.get("answer", "N/A")

        formatted_questions.append({
            "Question Type": question_type,
            "Question": question_text,
            "Answer": answer_text,
            # "Question Length (Chars)": len(question_text),
            # "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else 0,
            # "Question Tokens": calculate_tokens(question_text),
            # "Answer Tokens": calculate_tokens(answer_text) if isinstance(answer_text, str) else 0,
        })

    print(f"\n🔍 Generated Questions:\n", formatted_questions)  # Print formatted questions

    return formatted_questions









import os
import json
import base64
import pandas as pd
import dotenv
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from pdf2image import convert_from_path
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_core.messages import HumanMessage
from langchain_openai import AzureChatOpenAI

# Load environment variables
dotenv.load_dotenv()

# Define Azure OpenAI configuration (Update these with your actual values)
azure_openai_api_version = os.getenv("AZURE_OPENAI_API_VERSION")
azure_openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")

# Define prompts for each question type
FACTUAL_PROMPT = "Generate factual questions based on the given paragraph."
PROCEDURAL_PROMPT = "Generate procedural questions based on the given paragraph."
CONCEPTUAL_PROMPT = "Generate conceptual questions based on the given paragraph."
INFERENTIAL_PROMPT = "Generate inferential questions based on the given paragraph."
REASONING_PROMPT = "Generate reasoning-based questions based on the given paragraph."

# Utility Functions
def extract_valid_json(response):
    """Extract valid JSON from a response string."""
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

def save_to_excel(questions_and_answers, output_file):
    """Save questions and answers to an Excel file."""
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

def remove_duplicates(questions):
    """Remove duplicate questions."""
    if not questions:
        return []
    seen = set()
    unique_questions = []
    for question in questions:
        if question["question"] not in seen:
            unique_questions.append(question)
            seen.add(question["question"])
    return unique_questions

# PDF Processing
def extract_text_from_pdf(pdf_path, output_folder="pdf_images"):
    """Extract text from a PDF by converting it to images and using GPT to process images."""
    os.makedirs(output_folder, exist_ok=True)
    
    # Convert PDF to images
    images = convert_from_path(pdf_path, 500)
    extract_list = ""

    for i, img in enumerate(images):
        image_path = os.path.join(output_folder, f"page_{i+1}.png")
        img.save(image_path, "PNG")
        print(f"\n--- Processing Page {i+1} ---\n")

        # Read and encode the image
        with open(image_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode("utf-8")

        # Process image with GPT
        llm = AzureChatOpenAI(temperature=0, api_version=azure_openai_api_version, azure_endpoint=azure_openai_endpoint)
        message = HumanMessage(content=[
            {"type": "text", "text": "Extract structured data from this image."},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
        ])

        response = llm.invoke([message])
        extract_list += response.content

    return extract_list

# Question Generation Functions
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    """Generate questions using LLM based on a specific type."""
    prompt = PromptTemplate(input_variables=["paragraph", "num_questions", "question_type"], template=prompt_template)
    formatted_prompt = prompt.format(paragraph=paragraph, num_questions=num_questions, question_type=question_type)

    llm = AzureChatOpenAI(temperature=0, api_version=azure_openai_api_version, azure_endpoint=azure_openai_endpoint)
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(paragraph=paragraph, num_questions=str(num_questions), question_type=str(question_type))
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating questions: {e}")
        return []

def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    """Generate questions in chunks to handle large inputs."""
    all_questions = []
    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(
                paragraph, question_type, min(chunk_size, total_questions - i), prompt_template
            )
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
    return all_questions

# Individual Modules for Each Question Type
def generate_factual_questions(paragraph, total_questions, chunk_size=10):
    return generate_questions_in_chunks(paragraph, "Factual", total_questions, FACTUAL_PROMPT, chunk_size)

def generate_procedural_questions(paragraph, total_questions, chunk_size=10):
    return generate_questions_in_chunks(paragraph, "Procedural", total_questions, PROCEDURAL_PROMPT, chunk_size)

def generate_conceptual_questions(paragraph, total_questions, chunk_size=10):
    return generate_questions_in_chunks(paragraph, "Conceptual", total_questions, CONCEPTUAL_PROMPT, chunk_size)

def generate_inferential_questions(paragraph, total_questions, chunk_size=10):
    return generate_questions_in_chunks(paragraph, "Inferential", total_questions, INFERENTIAL_PROMPT, chunk_size)

def generate_reasoning_questions(paragraph, total_questions, chunk_size=10):
    return generate_questions_in_chunks(paragraph, "Reasoning-based", total_questions, REASONING_PROMPT, chunk_size)

# Master Function to Call Specific Modules
def generate_questions_by_type(paragraph, question_type, total_questions, chunk_size=10):
    question_functions = {
        "Factual": generate_factual_questions,
        "Procedural": generate_procedural_questions,
        "Conceptual": generate_conceptual_questions,
        "Inferential": generate_inferential_questions,
        "Reasoning-based": generate_reasoning_questions,
    }

    if question_type not in question_functions:
        print(f"Error: {question_type} is not a valid question type.")
        return []

    print(f"Generating {total_questions} {question_type} questions...")
    return question_functions[question_type](paragraph, total_questions, chunk_size)

# Main Processing Function
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output", question_type=None):
    """Processes all PDFs and generates specific or all question types."""
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)

                try:
                    print(f"Processing: {pdf_path}")
                    pdf_text = extract_text_from_pdf(pdf_path)

                    if question_type:
                        questions_and_answers = generate_questions_by_type(pdf_text, question_type, total_questions, chunk_size)
                    else:
                        questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")








def generate_factual_questions(paragraph, total_questions, chunk_size=10):
    return generate_questions_in_chunks(paragraph, "Factual", total_questions, FACTUAL_PROMPT, chunk_size)

def generate_procedural_questions(paragraph, total_questions, chunk_size=10):
    return generate_questions_in_chunks(paragraph, "Procedural", total_questions, PROCEDURAL_PROMPT, chunk_size)

def generate_conceptual_questions(paragraph, total_questions, chunk_size=10):
    return generate_questions_in_chunks(paragraph, "Conceptual", total_questions, CONCEPTUAL_PROMPT, chunk_size)

def generate_inferential_questions(paragraph, total_questions, chunk_size=10):
    return generate_questions_in_chunks(paragraph, "Inferential", total_questions, INFERENTIAL_PROMPT, chunk_size)

def generate_reasoning_questions(paragraph, total_questions, chunk_size=10):
    return generate_questions_in_chunks(paragraph, "Reasoning-based", total_questions, REASONING_PROMPT, chunk_size)







{
  "question_type": "Procedural",
  "question": "How do you reset a user's password in the system?",
  "answer": {
    "introduction": "Resetting a user's password is required when they forget it or for security reasons.",
    "prerequisites": [
      "Administrator access to the system.",
      "User's registered email or username."
    ],
    "procedure": {
      "step_1": {
        "title": "Navigate to Password Reset",
        "steps": [
          "Log into the admin panel.",
          "Go to 'User Management' and select 'Reset Password'."
        ]
      },
      "step_2": {
        "title": "Reset and Notify User",
        "steps": [
          "Enter the user's email or username.",
          "Generate a new password or send a reset link.",
          "Notify the user via email."
        ],
        "example_email_format": {
          "subject": "Password Reset Request",
          "body": "Dear User,\n\nYour password has been reset successfully. Use the link below to set a new password:\n[Reset Link]\n\nBest Regards,\nSupport Team"
        }
      }
    },
    "example_output_format": {
      "user_id": "12345",
      "status": "Password reset successfully",
      "reset_link": "https://example.com/reset-password"
    }
  }
}











import os
import json
import base64
import pandas as pd
from pdf2image import convert_from_path
from langchain_core.messages import HumanMessage
from langchain_openai import AzureChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Set up Azure OpenAI connection
azure_openai_api_version = "your_api_version"
azure_openai_endpoint = "your_endpoint"
llm = AzureChatOpenAI(
    temperature=0,
    top_p=0,
    api_version=azure_openai_api_version,
    azure_endpoint=azure_openai_endpoint,
    deployment_name="ssgpt-40",
    seed=1
)

# Define procedural prompt template
PROCEDURAL_PROMPT = """ 
#Objective:
You are tasked to generate a procedural question and answer pairs. Generate up to {num_questions} distinct procedural question-answer pairs based on the Input.
...
"""

# Function to extract valid JSON from response
def extract_valid_json(response):
    try:
        json_start = response.find("[")
        json_end = response.rfind("]")
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to extract question-answer pairs
def extract_questions_answers(raw_text):
    pattern = re.compile(r"(Question Type: .+?) (?=(Question Type: |$))", re.DOTALL)
    matches = pattern.findall(raw_text)
    
    questions_answers = []
    for match in matches:
        question_block = match[0].strip()
        question_type = re.search(r"Question Type: (.+)", question_block)
        question = re.search(r"Question: (.+)", question_block)
        answer = re.search(r"Answer:\s*(.+)", question_block, re.DOTALL)
        
        questions_answers.append({
            "Question Type": question_type.group(1).strip() if question_type else "Unknown",
            "Question": question.group(1).strip() if question else "No Question",
            "Answer": answer.group(1).strip() if answer else "No Answer"
        })
    return questions_answers

# Function to save JSON data to Excel
def save_json_to_excel(json_data, file_name):
    if isinstance(json_data, list) and all(isinstance(i, dict) for i in json_data):
        df = pd.DataFrame(json_data)
        df.to_excel(file_name, index=False)
        print(f"JSON saved to {file_name} successfully.")
    else:
        print("Error: Provided data is not in the expected format.")

# Function to process PDFs from a directory
def process_pdfs(root_dir, output_dir, num_questions):
    os.makedirs(output_dir, exist_ok=True)
    
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith(".pdf"):
                pdf_path = os.path.join(root, file)
                output_folder = os.path.join(output_dir, os.path.splitext(file)[0])
                os.makedirs(output_folder, exist_ok=True)
                
                print(f"Processing: {pdf_path}")
                
                # Convert PDF to images
                images = convert_from_path(pdf_path, 500, poppler_path=r'C:\Users\P872643\Downloads\poppler-0.68.0\bin')
                extract_list = []
                
                for i, img in enumerate(images):
                    image_path = os.path.join(output_folder, f"page_{i+1}.png")
                    img.save(image_path, "PNG")
                    
                    with open(image_path, "rb") as image_file:
                        image_data = base64.b64encode(image_file.read()).decode("utf-8")
                    
                    message = HumanMessage(
                        content=[
                            {"type": "text", "text": "Extract structured data from this document in markdown format."},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                        ]
                    )
                    
                    response = llm.invoke([message])
                    extract_list.append(response.content)

                # Generate question-answer pairs
                formatted_prompt = PROCEDURAL_PROMPT.format(paragraph=extract_list, num_questions=num_questions)
                llm_chain = LLMChain(llm=llm, prompt=PromptTemplate(input_variables=["paragraph", "num_questions"], template=PROCEDURAL_PROMPT))
                qa_response = llm_chain.predict(paragraph=extract_list, num_questions=str(num_questions))

                # Extract valid JSON
                result = extract_valid_json(qa_response)
                
                # Save JSON and Excel output
                json_output_path = os.path.join(output_folder, f"{os.path.splitext(file)[0]}.json")
                excel_output_path = os.path.join(output_folder, f"{os.path.splitext(file)[0]}.xlsx")
                
                with open(json_output_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, indent=4, ensure_ascii=False)
                
                save_json_to_excel(result, excel_output_path)

# Define input directory, output directory, and number of questions
root_directory = "h:/your_root_directory"
output_directory = "h:/your_output_directory"
total_questions = 40  # Set the desired number of questions

# Run the process
process_pdfs(root_directory, output_directory, total_questions)


PROCEDURAL PROMPT="

# Objective:

You are tasked to generate procedural question-and-answer pairs. Generate up to (num_questions) distinct procedural question-answer pairs based on the Input.

# Steps to Read the Input Extracts:

1. The input is the extraction of PDF contents in a structured markdown format.  
2. Every detail in the input is crucial. Read through it carefully to generate **logical procedural question-and-answer pairs**.  
3. If the input contains tables, images, screenshots, or icons, read their inline descriptions to generate precise procedural answers.

## Understanding Index Points:

1. The input consists of text, images, icons, screenshots, tables, etc.  
2. The input also contains inline descriptions for images, screenshots, icons, and tables.  

## Step-by-Step Answer:

1. **Generate question-answer pairs** that comprehensively cover all steps of the procedure.  
2. **Include relevant descriptions** of tables, images, screenshots, or icons in the procedural steps where applicable.  
3. Clearly describe **each step**, integrating referenced sections and key details from the text and descriptions.  
4. If a **command, calculation, or action** is required, format it appropriately.  
5. Continue steps **sequentially**, ensuring logical progression.  
6. Extract key details from the text and images, ensuring no important information is left out.

# Output Instructions:

1. Ensure the response is a **valid JSON array**.  

2. The JSON response should follow this format:

```json
[
  {{
    "Question Type": "Procedural",
    "Question": "What are the steps to complete [Procedure Name]?",
    "Answer": {{
      "Title": "[Procedure Name]",
      "Introduction": "Brief overview of what this procedure achieves.",
      "Prerequisites": [
        "Requirement 1",
        "Requirement 2"
      ],
      "Procedure": [
        {{
          "Step": 1,
          "Title": "[Step 1 Title]",
          "Description": "Detailed explanation of step 1.",
          "Additional Details": "Any extra context needed.",
          "Image": "image1.png",
          "Image Caption": "Description of the image."
        }},
        {{
          "Step": 2,
          "Title": "[Step 2 Title]",
          "Description": "Detailed explanation of step 2.",
          "Table": {{
            "Header": ["Column 1", "Column 2"],
            "Rows": [
              ["Data 1", "Data 2"],
              ["Data 3", "Data 4"]
            ]
          }}
        }}
      ]
    }}
  }}
]









PROCEDURAL PROMPT="

# Objective:

You are tasked to generate procedural question-and-answer pairs. Generate up to (num_questions) distinct procedural question-answer pairs based on the Input.

# Steps to Read the Input Extracts:

1. The input is the extraction of PDF contents in a structured markdown format.  

2. Every detail in the input is crucial. Read through it carefully to generate **logical procedural question-and-answer pairs**.  

3. If the input contains tables, images, screenshots, or icons, read their inline descriptions to generate precise procedural answers.

## Understanding Index Points:

1. The input consists of text, images, icons, screenshots, tables, etc.  

2. The input also contains inline descriptions for images, screenshots, icons, and tables.  

## Step-by-Step Answer:

1. **Generate question-answer pairs** that comprehensively cover all steps of the procedure.  

2. **Include relevant descriptions** of tables, images, screenshots, or icons in the procedural steps where applicable.  

3. Clearly describe **each step**, integrating referenced sections and key details from the text and descriptions.  

4. If a **command, calculation, or action** is required, format it appropriately.  

5. Continue steps **sequentially**, ensuring logical progression.  

6. Extract key details from the text and images, ensuring no important information is left out.

# Output Instructions:

1. Ensure the response is a **valid JSON array**.  

2. The JSON response should follow this format:

```json
[
  {
    "Question Type": "Procedural",
    "Question": "What are the steps to complete [Procedure Name]?",
    "Answer": {
      "Title": "[Procedure Name]",
      "Introduction": "Brief overview of what this procedure achieves.",
      "Prerequisites": [
        "Requirement 1",
        "Requirement 2"
      ],
      "Procedure": [
        {
          "Step": 1,
          "Title": "[Step 1 Title]",
          "Description": "Detailed explanation of step 1.",
          "Additional Details": "Any extra context needed.",
          "Image": "image1.png",
          "Image Caption": "Description of the image."
        },
        {
          "Step": 2,
          "Title": "[Step 2 Title]",
          "Description": "Detailed explanation of step 2.",
          "Table": {
            "Header": ["Column 1", "Column 2"],
            "Rows": [
              ["Data 1", "Data 2"],
              ["Data 3", "Data 4"]
            ]
          }
        }
      ]
    }
  }
]









import pandas as pd
import json

def save_json_to_excel(json_data, file_name="procedural_data.xlsx"):
    # Ensure the data is a valid list of dictionaries
    if isinstance(json_data, list) and all(isinstance(i, dict) for i in json_data):
        # Convert to DataFrame
        df = pd.DataFrame(json_data)
        # Save to Excel
        df.to_excel(file_name, index=False)
        print(f"JSON saved to {file_name} successfully.")
    else:
        print("Error: Provided data is not in the expected format (list of dictionaries).")

# Correct JSON data as a list of dictionaries
json_data = [
    {
        "Question Type": "Procedural",
        "Question": "How do you ensure the monthly book in MCH has been released?",
        "Answer": "# Ensure Monthly Book in MCH has been Released\n## Introduction\nThis procedure ensures that the monthly book in MCH has been released correctly."
    },
    {
        "Question Type": "Procedural",
        "Question": "How do you ensure a new fund has been added into Fund Master?",
        "Answer": "# Ensure New Fund has been Added into Fund Master\n## Introduction\nThis procedure ensures that a new fund is added into the Fund Master correctly."
    },
    {
        "Question Type": "Procedural",
        "Question": "How do you run the job \"AAS_MC_UPDATE_CLIENT_VFMC\" in DMH?",
        "Answer": "# Run Job \"AAS_MC_UPDATE_CLIENT_VFMC\" in DMH\n## Introduction\nThis procedure outlines the steps to run the job AAS_MC_UPDATE_CLIENT_VFMC in DMH."
    },
    {
        "Question Type": "Procedural",
        "Question": "How do you handle job failures in DMH?",
        "Answer": "# Handle Job Failures in DMH\n## Introduction\nThis procedure outlines the steps to handle job failures in DMH."
    }
]

# Call the function to save the data to Excel
save_json_to_excel(json_data)










import pandas as pd
import json

def save_json_to_excel(json_data, file_name="procedural_data.xlsx"):
    # Check if the data is a valid list of dictionaries
    if isinstance(json_data, list) and all(isinstance(i, dict) for i in json_data):
        # Convert the list of dictionaries to a DataFrame
        df = pd.DataFrame(json_data)
        # Save to Excel
        df.to_excel(file_name, index=False)
        print(f"JSON saved to {file_name} successfully.")
    else:
        print("Error: Provided data is not in the expected format (list of dictionaries).")

# Example of how to use the function:
json_output = [
    {
        "Question Type": "Procedural",
        "Question": "How do you reconcile NRM, RTS, and SIT data in DMH?",
        "Answer": "Reconcile NRM, RTS, and SIT data in DMH steps..."
    },
    {
        "Question Type": "Procedural",
        "Question": "How do you download Working Trial Balance Report and Account Position Appraisal Report in CMOD?",
        "Answer": "Steps to download reports from CMOD..."
    }
]

# Call the function to save the data to Excel
save_json_to_excel(json_output)












import json
import re

# Sample raw text input
raw_text = """
Question Type: Procedural

Question: How do you reconcile NRM, RTS, and SIT data in DMH?

Answer:

#Reconcile NRM, RTS, and SIT Data in DMH

## Introduction

This procedure outlines the steps to reconcile NRM, RTS, and SIT data in DMH, ensuring data consistency and accuracy.

** Prerequisites

Access to DMH.

Previous month's NRM SIT RTS recon file.

## Procedure

### Step 1: Prepare Reconciliation File

1. Copy the NRM SIT RTS recon file of the previous month.

2. Rename it and save it in the applicable working folder for the current month.

3. Update the date to reflect the required report period.

### Step 2: Update Formulas

1. Ensure the formulas in Column E are correct.

2. Verify the data for accuracy and consistency.

*** Step 3: Perform Reconciliation

1. Reconcile the NRM, RTS, and SIT data by comparing the relevant columns and values.

2. Ensure there are no discrepancies between the data sets.
-------

Question Type: Procedural

Question: How do you download Working Trial Balance Report and Account Position Appraisal Report in CMOD?

Answer:

Download Working Trial Balance Report and Account Position Appraisal Report in CMOD

Introduction

Es procedure outlines the steps to download the Working Trial Balance Report and Account Position Appraisal Report in CMOD.

Prerequisites

Access to CMOD.

Procedure

Step 1: Log in to CMOD

Log in to CMOD.

Select 'Sydney 00 ALL REPORTS and click 'Open'.

Step 2: Search for Reports

Input the fund code, report code (working trial-VM649; APA-VM910), and click search. ! [Screenshot of the software interface showing search criteria and document list.](images/screenshot.png)

*Figure 1: Software interface showing search criteria and document list.*

Step 3: Download Reports

Download the Working Trial Balance Report and Account Position Appraisal Report.

Ensure the reports are for the correct period and have the status "APPROVED".
"""

# Function to extract questions and answers from raw text and convert them into JSON
def extract_questions_answers(raw_text):
    # Regex pattern to capture Question Type, Question, and Answer
    pattern = re.compile(r"(Question Type: .+?)(?=(Question Type:|$))", re.DOTALL)

    # Find all matches
    matches = pattern.findall(raw_text)

    # Prepare the final JSON structure
    questions_answers = []
    for match in matches:
        # Extract question type, question, and answer
        question_block = match[0].strip()

        # Extract Question Type
        question_type_match = re.search(r"Question Type: (.+)", question_block)
        question_type = question_type_match.group(1).strip() if question_type_match else "Unknown"

        # Extract Question
        question_match = re.search(r"Question: (.+)", question_block)
        question = question_match.group(1).strip() if question_match else "No Question"

        # Extract Answer
        answer_match = re.search(r"Answer:\s*(.+)", question_block, re.DOTALL)
        answer = answer_match.group(1).strip() if answer_match else "No Answer"

        # Append to the final result
        questions_answers.append({
            "Question Type": question_type,
            "Question": question,
            "Answer": answer
        })

    return questions_answers

# Extract data and convert to JSON
questions_answers = extract_questions_answers(raw_text)
json_output = json.dumps(questions_answers, indent=4)

# Print the JSON output
print(json_output)











import json
import re

# Sample raw text input
raw_text = """
Question Type: Procedural

Question: How do you reconcile NRM, RTS, and SIT data in DMH?

Answer:

#Reconcile NRM, RTS, and SIT Data in DMH

## Introduction

This procedure outlines the steps to reconcile NRM, RTS, and SIT data in DMH, ensuring data consistency and accuracy.

** Prerequisites

Access to DMH.

Previous month's NRM SIT RTS recon file.

## Procedure

### Step 1: Prepare Reconciliation File

1. Copy the NRM SIT RTS recon file of the previous month.

2. Rename it and save it in the applicable working folder for the current month.

3. Update the date to reflect the required report period.

### Step 2: Update Formulas

1. Ensure the formulas in Column E are correct.

2. Verify the data for accuracy and consistency.

*** Step 3: Perform Reconciliation

1. Reconcile the NRM, RTS, and SIT data by comparing the relevant columns and values.

2. Ensure there are no discrepancies between the data sets.
-------

Question Type: Procedural

Question: How do you download Working Trial Balance Report and Account Position Appraisal Report in CMOD?

Answer:

Download Working Trial Balance Report and Account Position Appraisal Report in CMOD

Introduction

Es procedure outlines the steps to download the Working Trial Balance Report and Account Position Appraisal Report in CMOD.

Prerequisites

Access to CMOD.

Procedure

Step 1: Log in to CMOD

Log in to CMOD.

Select 'Sydney 00 ALL REPORTS and click 'Open'.

Step 2: Search for Reports

Input the fund code, report code (working trial-VM649; APA-VM910), and click search. ! [Screenshot of the software interface showing search criteria and document list.](images/screenshot.png)

*Figure 1: Software interface showing search criteria and document list.*

Step 3: Download Reports

Download the Working Trial Balance Report and Account Position Appraisal Report.

Ensure the reports are for the correct period and have the status "APPROVED".
"""

# Function to extract questions and answers from raw text and convert them into JSON
def extract_questions_answers(raw_text):
    # Regex pattern to capture Question Type, Question, and Answer
    pattern = re.compile(r"Question Type: (.+?)\nQuestion: (.+?)\nAnswer:\s*(.*?)\n(?=(Question Type:|$))", re.DOTALL)

    # Find all matches
    matches = pattern.findall(raw_text)

    # Prepare the final JSON structure
    questions_answers = []
    for match in matches:
        question_type, question, answer = match
        questions_answers.append({
            "Question Type": question_type.strip(),
            "Question": question.strip(),
            "Answer": answer.strip()
        })

    return questions_answers

# Extract data and convert to JSON
questions_answers = extract_questions_answers(raw_text)
json_output = json.dumps(questions_answers, indent=4)

# Print the JSON output
print(json_output)







import json
import re

# Sample text containing multiple questions and answers
raw_text = """
Question Type: Procedural
Question: How do you perform RTS reconciliation in DMH?
Answer:
# Perform RTS Reconciliation in DMH

## Introduction

This procedure outlines the steps to perform RTS reconciliation in DMH, ensuring data accuracy and consistency.

**Prerequisites**

- Access to DMH.
- RTS report.

## Procedure

### Step 1: Prepare RTS Reconciliation File

1. Delete two subtotals at the bottom of the report.
2. Subtotal column J-0.

### Step 2: Perform Reasonability Check

1. Validate cusips with AAS25 adjustment but nil MCH G/L.
2. Ensure the data is accurate and consistent.

### Step 3: Reconcile Data

1. Perform the same steps as in NRM reconciliation.
2. Ensure there are no discrepancies between the data sets.

Question Type: Procedural
Question: How do you perform XYZ reconciliation?
Answer:
# Perform XYZ Reconciliation

## Introduction

XYZ reconciliation ensures data integrity across systems.

**Prerequisites**

- Access to XYZ system.
- XYZ report.

## Procedure

### Step 1: Extract Data

1. Download the latest XYZ report.
2. Ensure all required columns are included.

### Step 2: Compare Entries

1. Validate transactions against the master dataset.
2. Identify and document discrepancies.

### Step 3: Finalize Reconciliation

1. Adjust incorrect entries.
2. Submit the final reconciliation report.
"""

# Function to extract questions and answers from raw text and convert into JSON
def extract_questions_answers(raw_text):
    # Regular expression to match question-answer pairs
    pattern = re.compile(r"Question Type: (.+?)\nQuestion: (.+?)\nAnswer:\s+(.+?)(?=(Question Type:|$))", re.DOTALL)
    
    # Find all matches
    matches = pattern.findall(raw_text)

    # Prepare the final JSON structure
    questions_answers = []
    for match in matches:
        question_type, question, answer = match
        questions_answers.append({
            "Question Type": question_type.strip(),
            "Question": question.strip(),
            "Answer": answer.strip()
        })

    return questions_answers

# Extract data and convert to JSON
questions_answers = extract_questions_answers(raw_text)
json_output = json.dumps(questions_answers, indent=4)

# Print the JSON output
print(json_output)









import json
import pandas as pd

def process_response(response_text):
    """
    Convert a procedural text response into a structured JSON format.
    """
    lines = response_text.strip().split("\n")
    
    # Extract question
    question = lines[0].strip()

    # Extract title and introduction
    title = lines[1].strip()
    introduction = lines[2].strip() if len(lines) > 2 else ""

    prerequisites = []
    procedure_steps = []
    
    section = None
    for line in lines[3:]:  # Start processing after intro
        line = line.strip()
        if line.startswith("Prerequisites:"):
            section = "prerequisites"
            continue
        elif line.startswith("Procedure:"):
            section = "procedure"
            continue
        
        if section == "prerequisites" and line:
            prerequisites.append(line.lstrip("- "))
        elif section == "procedure" and line:
            step_number = len(procedure_steps) + 1
            procedure_steps.append(f"Step {step_number}: {line}")

    # Ensure the answer contains all required fields
    structured_data = {
        "question_type": "Procedural",
        "question": question,
        "answer": {
            "title": title,
            "introduction": introduction,
            "prerequisites": prerequisites,
            "procedure": procedure_steps
        }
    }
    
    return structured_data

def save_json_to_excel(json_list, file_name="procedural_data.xlsx"):
    """
    Save multiple JSON responses into an Excel file.
    """
    df = pd.DataFrame([
        {
            "Question Type": data["question_type"],
            "Question": data["question"],
            "Answer": json.dumps(data["answer"], indent=4)  # Store answer as a JSON string
        }
        for data in json_list
    ])
    
    df.to_excel(file_name, index=False)
    print(f"All JSON responses saved to {file_name} successfully.")

# Example: Multiple procedural responses
responses = [
    """How do you ensure the monthly book in MCH has been released?
Ensuring Monthly Book Release in MCH
This procedure ensures that the monthly book in MCH has been released, which is crucial for accurate accounting data flow.

Prerequisites:
- Access to MCH BLRL screen
- Knowledge of fund SDVK (composite are SFZC/DBZC/DRWC)

Procedure:
1. Go to the MCH BLRL screen.
2. Verify that the S1 status shows 'released'.
3. Ensure that DATE, TIME, FUND, and other parameters are correct.
4. Confirm that the monthly book will be released on SBD.""",

    """How do you ensure a new fund has been added into the Fund Master?
Ensuring New Fund Addition into Fund Master
This procedure ensures that a new fund is properly added to the Fund Master for accurate record-keeping.

Prerequisites:
- Access to Fund Master System
- Permissions to add new funds

Procedure:
1. Log into the Fund Master System.
2. Navigate to the 'Add Fund' section.
3. Enter the fund details and ensure correctness.
4. Save the changes and verify the fund appears in the list."""
]

# Process multiple responses
json_output_list = [process_response(response) for response in responses]

# Save all responses to Excel
save_json_to_excel(json_output_list)








import json
import pandas as pd

def process_response(response_text):
    """
    Convert a procedural text response into a structured JSON format.
    """
    lines = response_text.strip().split("\n")
    question = lines[0].strip()
    title = lines[1].strip()
    introduction = lines[3].strip()
    
    prerequisites = []
    procedure_steps = []
    
    section = None
    for line in lines[5:]:  # Skip first few lines (question, title, intro)
        line = line.strip()
        if line.startswith("Prerequisites:"):
            section = "prerequisites"
            continue
        elif line.startswith("Procedure:"):
            section = "procedure"
            continue
        
        if section == "prerequisites":
            prerequisites.append(line.lstrip("- "))
        elif section == "procedure":
            step_number = len(procedure_steps) + 1
            procedure_steps.append(f"Step {step_number}: {line}")
    
    # Store everything under the "answer" key
    structured_data = {
        "question_type": "Procedural",
        "question": question,
        "answer": {
            "title": title,
            "introduction": introduction,
            "prerequisites": prerequisites,
            "procedure": procedure_steps
        }
    }
    
    return structured_data

def save_json_to_excel(json_data, file_name="procedural_data.xlsx"):
    """
    Save JSON data into an Excel file.
    """
    # Convert nested JSON into a structured DataFrame
    df = pd.DataFrame([{
        "Question Type": json_data["question_type"],
        "Question": json_data["question"],
        "Answer": json.dumps(json_data["answer"], indent=4)  # Store answer as a formatted JSON string
    }])
    
    # Save DataFrame to Excel
    df.to_excel(file_name, index=False)
    print(f"JSON saved to {file_name} successfully.")

# Example function response
response_text = """How do you ensure the monthly book in MCH has been released?
Ensuring Monthly Book Release in MCH
Introduction:
This procedure ensures that the monthly book in MCH has been released, which is crucial for accurate accounting data flow.

Prerequisites:
- Access to MCH BLRL screen
- Knowledge of fund SDVK (composite are SFZC/DBZC/DRWC)

Procedure:
1. Go to the MCH BLRL screen.
2. Verify that the S1 status shows 'released'.
3. Ensure that DATE, TIME, FUND, and other parameters are correct.
4. Confirm that the monthly book will be released on SBD.
"""

# Process response
json_output = process_response(response_text)

# Save JSON data to Excel
save_json_to_excel(json_output)










import json
import pandas as pd

def process_response(response_text):
    """
    Convert a procedural text response into a structured JSON format.
    """
    lines = response_text.strip().split("\n")
    question = lines[0].strip()
    title = lines[1].strip()
    introduction = lines[3].strip()
    
    prerequisites = []
    procedure_steps = []
    
    # Parse prerequisites and procedure steps
    section = None
    for line in lines[5:]:  # Skip first few lines (question, title, intro)
        line = line.strip()
        if line.startswith("Prerequisites:"):
            section = "prerequisites"
            continue
        elif line.startswith("Procedure:"):
            section = "procedure"
            continue
        
        if section == "prerequisites":
            prerequisites.append(line.lstrip("- "))
        elif section == "procedure":
            step_number = len(procedure_steps) + 1
            procedure_steps.append({"step": step_number, "description": line})
    
    # Convert response into structured JSON format
    structured_data = {
        "question_type": "Procedural",
        "question": question,
        "answer": {
            "title": title,
            "introduction": introduction,
            "prerequisites": prerequisites,
            "procedure": procedure_steps
        }
    }
    
    return structured_data

def save_json_to_excel(json_data, file_name="procedural_data.xlsx"):
    """
    Save JSON data into an Excel file.
    """
    df = pd.json_normalize(json_data)
    df.to_excel(file_name, index=False)
    print(f"JSON saved to {file_name} successfully.")

# Example function response (This will be replaced with your actual function response)
response_text = """How do you ensure the monthly book in MCH has been released?
Ensuring Monthly Book Release in MCH
Introduction:
This procedure ensures that the monthly book in MCH has been released, which is crucial for accurate accounting data flow.

Prerequisites:
- Access to MCH BLRL screen
- Knowledge of fund SDVK (composite are SFZC/DBZC/DRWC)

Procedure:
1. Go to the MCH BLRL screen.
2. Verify that the S1 status shows 'released'.
3. Ensure that DATE, TIME, FUND, and other parameters are correct.
4. Confirm that the monthly book will be released on SBD.
"""

# Process response
json_output = process_response(response_text)

# Save JSON data to Excel
save_json_to_excel(json_output)












# FINAL GENERALIZED PROMPT FOR LARGE-SCALE QUESTION GENERATION

"""
You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.

TASK: Generate a variety of **generalized financial questions and answers** based on the provided input content (financial data, reports, or PDFs). The questions should focus on extracting meaningful details such as financial terms, key metrics, relationships, and entities.

REQUIREMENTS:
1. **Scalable and Generalized:** The questions should be flexible and adaptable to any financial document, ensuring general relevance without relying on static or specific values.
2. **Diverse Question Types:** Include questions about financial details, key terms, metrics, or any information extracted from the input.
3. **No Negative or Unrelated Questions:** Only focus on document-related factual questions.
4. **Answer Clarity:** Provide concise and precise answers based on the input content.
5. **No Mention of Specific Inputs:** The questions and answers should be general and not reference any particular document by name.

EXAMPLES OF QUESTION TYPES:
1. "What is the financial period covered in the report?"
2. "Which key financial metric indicates profitability?"
3. "Who is responsible for preparing the financial report?"
4. "What are the major assets listed?"
5. "Which liabilities have been highlighted in the report?"

OUTPUT FORMAT (JSON ARRAY EXAMPLE):
[
  {
    "question": "What is the financial period covered in the report?",
    "answer": "January 1, 2024, to December 31, 2024",
    "context": "Extracted from financial data"
  },
  {
    "question": "What key metric indicates the company’s net profit?",
    "answer": "Net income after tax",
    "context": "General financial data"
  },
  {
    "question": "Which assets are included under current assets?",
    "answer": "Cash, accounts receivable, and inventory",
    "context": "General financial data"
  }
]
"""








# GENERALIZED FACTUAL PROMPT FOR DIVERSE QUESTION GENERATION

"""
You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.

TASK: Generate a variety of questions based on:
1. **Input-specific content:** Generate questions based on financial details, relationships, or entities present in the input document or PDF.
2. **Generalized financial queries:** Include commonly asked questions related to accounting, investments, taxation, budgeting, risk management, and financial planning.
3. **Miscellaneous/general-purpose questions:** Include everyday chatbot queries such as “What is today’s date?”, “What is the capital of France?”, or other general knowledge questions.

REQUIREMENTS:
1. **Scalability:** Ensure questions are general and adaptable across various datasets or PDF inputs, without being dependent on specific static details from one document.
2. **Diverse Question Types:** Generate:
    - Document-related questions (e.g., about titles, key concepts, entities).
    - General financial advice questions.
    - Unrelated/general knowledge or chatbox-style questions.
3. **Negative and Exploratory Questions:** Include questions about missing, excluded, or unexplored topics.
4. **Answer Clarity:** Provide concise and precise answers for each question.

EXAMPLES OF QUESTION CATEGORIES:
1. **Document-Related Questions (Generalized):**
   - "What type of financial statement is presented in the document?"
   - "Which financial metric indicates company performance?"

2. **General Financial Questions:**  
   - "How can I reduce my taxable income?"
   - "What are the benefits of investing in mutual funds?"

3. **Miscellaneous/General Questions:**  
   - "What is the current year?"
   - "Who is the current President of the United States?"

OUTPUT FORMAT (JSON ARRAY EXAMPLE):
[
  {
    "question": "What type of financial statement is described in the document?",
    "answer": "Balance sheet, income statement, or similar financial statement based on the input.",
    "context": "Generalized document-related question"
  },
  {
    "question": "What key risks are mentioned in the document?",
    "answer": "The risks described in the financial document or context.",
    "context": "Generalized document-related question"
  },
  {
    "question": "What are some common ways to minimize investment risk?",
    "answer": "Diversification, regular portfolio reviews, and investing for the long term.",
    "context": "General finance query"
  },
  {
    "question": "How can businesses manage cash flow efficiently?",
    "answer": "By creating cash flow forecasts, managing receivables, and optimizing expenses.",
    "context": "General finance query"
  },
  {
    "question": "What is today’s date?",
    "answer": "The date of query execution.",
    "context": "General chatbot-style question"
  },
  {
    "question": "What is the capital of Germany?",
    "answer": "Berlin",
    "context": "General knowledge question"
  }
]
"""














# FINAL PROMPT FOR GENERATING FINANCIAL, GENERAL, AND MISCELLANEOUS QUESTIONS

"""
You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.

TASK: Generate a combination of:
1. **Document-related financial questions:** These should focus on the key aspects of the financial document, including names, relationships, exclusions, or key metrics mentioned.
2. **General finance-related customer queries:** Include general questions typically asked in financial consultations or chatbots (e.g., investment advice, tax tips, budgeting).
3. **Unrelated or miscellaneous questions:** Generate general-purpose questions commonly asked in chatboxes, unrelated to finance or the document (e.g., “What is today’s weather?”, “What time is it?”, or “Who won the last FIFA World Cup?”).

REQUIREMENTS:
1. **Diversity:** Ensure the questions cover document-related topics, financial queries, and general miscellaneous topics.
2. **Negative Questions:** Include at least a few questions that focus on exclusions or missing details (e.g., "What was not covered in the financial report?").
3. **Answer Clarity:** Each question should have a clear, concise answer.
4. **Context Specification:** Indicate whether the question relates to document content, financial advice, or general knowledge.

OUTPUT FORMAT (JSON ARRAY EXAMPLE):
[
  {
    "question": "What is the title of the financial document mentioned in the input text?",
    "answer": "Working Trial Balance Report",
    "context": "Text content from the financial document"
  },
  {
    "question": "Which key liabilities were excluded from the financial statement?",
    "answer": "Details about excluded liabilities as mentioned in the input.",
    "context": "Financial document"
  },
  {
    "question": "How can I reduce capital gains tax legally?",
    "answer": "Strategies include tax-loss harvesting, long-term investments, and taking advantage of retirement accounts.",
    "context": "General finance-related query"
  },
  {
    "question": "What are the benefits of portfolio diversification?",
    "answer": "Diversification helps minimize risk by spreading investments across different asset classes.",
    "context": "General finance-related query"
  },
  {
    "question": "What is today’s weather?",
    "answer": "The weather depends on your location. Please provide your city.",
    "context": "General knowledge or chatbot-style query"
  },
  {
    "question": "What time is it now?",
    "answer": "Current time depends on your time zone.",
    "context": "General knowledge or chatbot-style query"
  }
]
"""










import pytesseract
import cv2
from pdf2image import convert_from_path
from PyPDF2 import PdfMerger
from PIL import Image
import os
import re
import warnings

warnings.filterwarnings("ignore")

# Set the TESSDATA_PREFIX environment variable
os.environ["TESSDATA_PREFIX"] = r"C:\Users\P872643\AppData\Local\Programs\Tesseract-OCR\tessdata"

# Cleaning directories
def clean_dir():
    dirs = ["image_path", "pdfs_out_path"]
    for d in dirs:
        if os.path.exists(d):
            files_to_remove = [os.path.join(d, f) for f in os.listdir(d)]
            for f in files_to_remove:
                os.remove(f)

# Deskew image to correct alignment issues
def deskew_image(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 50, 150, apertureSize=3)
    lines = cv2.HoughLines(edges, 1, cv2.HOUGH_GRADIENT, 150)
    
    if lines is not None:
        for rho, theta in lines[0]:
            angle = (theta * 180 / 3.14159265359) - 90
            h, w = image.shape[:2]
            center = (w // 2, h // 2)
            M = cv2.getRotationMatrix2D(center, angle, 1.0)
            return cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
    return image

# Preprocess image to enhance OCR accuracy
def preprocess_image(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    return thresh

# Correct common OCR errors
def post_process_text(text):
    corrections = {
        r"\b1\b": "I",    # Correct "1" to "I"
        r"\b0\b": "O"     # Correct "0" to "O"
    }
    for pattern, replacement in corrections.items():
        text = re.sub(pattern, replacement, text)
    return text

# Convert scanned PDF to a searchable PDF
def create_searchable_pdf(pdf_file):
    ext = os.path.splitext(pdf_file)
    filename = os.path.basename(ext[0])
    total_images = 0

    # Configure Tesseract
    pytesseract.pytesseract.tesseract_cmd = r'C:\Users\P872643\AppData\Local\Programs\Tesseract-OCR\tesseract.exe'
    tessdata_dir_config = '--tessdata-dir "C:/Users/P872643/AppData/Local/Programs/Tesseract-OCR/tessdata"'

    # Convert PDF pages to images (higher DPI for better quality)
    pages = convert_from_path(pdf_file, 600, poppler_path=r'C:\Users\P872643\Downloads\poppler-0.68.0\bin')
    if not os.path.exists("image_path/"):
        os.mkdir("image_path/")

    # Save and process each page
    for page_num, img_blob in enumerate(pages):
        image_path = f"image_path/{page_num}_{filename}_image_converted.jpg"
        img_blob.save(image_path)
        total_images += 1

    print(f'PDF file converted to {total_images} images successfully.')

    # Perform OCR on each image and save as individual PDFs
    if not os.path.exists("pdfs_out_path/"):
        os.mkdir("pdfs_out_path/")
    
    pdfs = []
    for page_num in range(total_images):
        image_path = f"image_path/{page_num}_{filename}_image_converted.jpg"
        img = cv2.imread(image_path, 1)
        
        if img is None:
            raise FileNotFoundError(f"Image not found or cannot read: {image_path}")

        # Deskew and preprocess the image
        img = deskew_image(img)
        img = preprocess_image(img)

        # Convert BGR to RGB for pytesseract
        pil_img = Image.fromarray(img)

        # Perform OCR and generate a searchable PDF
        result = pytesseract.image_to_pdf_or_hocr(pil_img, lang="eng", config='--oem 1 --psm 6')
        pdf_path = f"pdfs_out_path/{page_num}_{filename}_searchable.pdf"
        
        with open(pdf_path, "w+b") as f:
            f.write(bytearray(result))
        
        pdfs.append(pdf_path)

    # Merge individual searchable PDFs into one
    merger = PdfMerger()
    for pdf in pdfs:
        merger.append(pdf)
    
    if not os.path.exists("result/"):
        os.mkdir("result/")
    output_file = f"result/{filename}_searchable_pdf_from_pdf.pdf"
    merger.write(output_file)
    print(f"Searchable PDF created successfully at {output_file}")
    merger.close()
    clean_dir()

# Convert a scanned image to a searchable PDF
def img_to_searchable_pdf(file):
    ext = os.path.splitext(file)
    filename = os.path.basename(ext[0])

    # Configure Tesseract
    pytesseract.pytesseract.tesseract_cmd = r'C:\Users\P872643\AppData\Local\Programs\Tesseract-OCR\tesseract.exe'
    tessdata_dir_config = '--tessdata-dir "C:/Users/P872643/AppData/Local/Programs/Tesseract-OCR/tessdata"'

    # Read and validate image
    img = cv2.imread(file, 1)
    if img is None:
        raise FileNotFoundError(f"Image not found or cannot read: {file}")

    # Deskew and preprocess the image
    img = deskew_image(img)
    img = preprocess_image(img)

    # Convert BGR to RGB for pytesseract
    pil_img = Image.fromarray(img)

    # Perform OCR and create a searchable PDF
    result = pytesseract.image_to_pdf_or_hocr(pil_img, lang="eng", config='--oem 1 --psm 6')
    
    if not os.path.exists("result/"):
        os.mkdir("result/")
    output_file = f"result/{filename}_searchable_pdf_from_image.pdf"
    
    with open(output_file, "w+b") as f:
        f.write(bytearray(result))
    print(f"Searchable PDF created successfully at {output_file}")

# Decide whether to process a PDF or an image
def searchable_pdf_process(file):
    ext = os.path.splitext(file)
    if ext[-1].lower() == '.pdf':
        create_searchable_pdf(file)
    else:
        img_to_searchable_pdf(file)

# Process scanned PDF or image
path = r"h:\Question and answer Fund accounting (Resubmit_1025)36. VFMC AAS25 SOP 5.0 1.pdf"
searchable_pdf_process(path)

-->

351

raise DeprecationError(msg)

DeprecationError: PdfFileMerger is deprecated and was removed in PyPDF2 3.0.0. Use PdfMerger instead.







# REVISED FACTUAL PROMPT FOR JUPYTER NOTEBOOK

"""
You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.

TASK: Generate up to {num_questions} distinct, factual question-answer pairs based on the input, which contains both text and image content combined.

REQUIREMENTS:

1. Question Diversity: Ensure the questions are varied and meaningful, avoiding repetitive or overly specific data-point queries such as "What is the balance of X?" or "What is the change in Y?"
2. Focus: Questions should explore details involving concepts, relationships, comparisons, analysis, and entities mentioned in the input.
3. Context: If details come from an image or text description, explicitly state that context in the output.
4. Precision: Answers should be concise and directly tied to the provided input.

EXAMPLES OF ACCEPTABLE QUESTIONS:
- "What financial metric shows the company’s profitability trend?"
- "Which entity was responsible for managing investment decisions as per the input?"
- "What accounting method is mentioned in the report for tracking asset depreciation?"
  
EXAMPLES TO AVOID:
- Repetitive, static numerical questions like:
  - "What is the starting balance of X?"
  - "What is the closing balance of Y?"
  - "What is the change between Z and W?"

OUTPUT FORMAT:
Return a valid JSON array of questions and answers.

[
  {
    "question": "What investment strategy is highlighted for maximizing portfolio returns?",
    "answer": "Strategy mentioned in the provided content.",
    "context": "Text content from the report"
  },
  {
    "question": "What financial institution was responsible for auditing the company’s accounts?",
    "answer": "Name of the institution.",
    "context": "Text data provided"
  }
]
"""











# Importing Libraries
# !pip install pytesseract
# !pip install opencv-python
# !pip install pdf2image
# !pip install PyPDF2

import pytesseract
import cv2
from pdf2image import convert_from_path
from PyPDF2 import PdfFileMerger
from PIL import Image
import os
import glob
import warnings

warnings.filterwarnings("ignore")

# Cleaning directories
def clean_dir():
    dirs = ["image_path", "pdfs_out_path"]
    for d in dirs:
        if os.path.exists(d):
            files_to_remove = [os.path.join(d, f) for f in os.listdir(d)]
            for f in files_to_remove:
                os.remove(f)

# Convert scanned PDF to a searchable PDF
def create_searchable_pdf(pdf_file):
    ext = os.path.splitext(pdf_file)
    filename = os.path.basename(ext[0])
    total_images = 0

    # Configure Tesseract
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
    tessdata_dir_config = '--tessdata-dir "C://Program Files//Tesseract-OCR//tessdata"'

    # Convert PDF pages to images
    pages = convert_from_path(pdf_file, 500, poppler_path=r'C:\Program Files\poppler-0.68.0\bin')
    if not os.path.exists("image_path/"):
        os.mkdir("image_path/")

    # Save and process each page
    for page_num, img_blob in enumerate(pages):
        image_path = f"image_path/{page_num}_{filename}_image_converted.jpg"
        img_blob.save(image_path)
        total_images += 1

    print(f'PDF file converted to {total_images} images successfully.')

    # Perform OCR on each image and save as individual PDFs
    if not os.path.exists("pdfs_out_path/"):
        os.mkdir("pdfs_out_path/")
    
    pdfs = []
    for page_num in range(total_images):
        image_path = f"image_path/{page_num}_{filename}_image_converted.jpg"
        img = cv2.imread(image_path, 1)
        
        if img is None:
            raise FileNotFoundError(f"Image not found or cannot read: {image_path}")

        # Convert BGR to RGB for pytesseract
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(img)

        # Perform OCR and generate a searchable PDF
        result = pytesseract.image_to_pdf_or_hocr(pil_img, lang="eng", config=tessdata_dir_config)
        pdf_path = f"pdfs_out_path/{page_num}_{filename}_searchable.pdf"
        
        with open(pdf_path, "w+b") as f:
            f.write(bytearray(result))
        
        pdfs.append(pdf_path)

    # Merge individual searchable PDFs into one
    merger = PdfFileMerger()
    for pdf in pdfs:
        merger.append(pdf)
    
    if not os.path.exists("result/"):
        os.mkdir("result/")
    output_file = f"result/{filename}_searchable_pdf_from_pdf.pdf"
    merger.write(output_file)
    print(f"Searchable PDF created successfully at {output_file}")
    merger.close()
    clean_dir()

# Convert a scanned image to a searchable PDF
def img_to_searchable_pdf(file):
    ext = os.path.splitext(file)
    filename = os.path.basename(ext[0])

    # Configure Tesseract
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
    tessdata_dir_config = '--tessdata-dir "C://Program Files//Tesseract-OCR//tessdata"'

    # Read and validate image
    img = cv2.imread(file, 1)
    if img is None:
        raise FileNotFoundError(f"Image not found or cannot read: {file}")

    # Convert BGR to RGB for pytesseract
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pil_img = Image.fromarray(img)

    # Perform OCR and create a searchable PDF
    result = pytesseract.image_to_pdf_or_hocr(pil_img, lang="eng", config=tessdata_dir_config)
    
    if not os.path.exists("result/"):
        os.mkdir("result/")
    output_file = f"result/{filename}_searchable_pdf_from_image.pdf"
    
    with open(output_file, "w+b") as f:
        f.write(bytearray(result))
    print(f"Searchable PDF created successfully at {output_file}")

# Decide whether to process a PDF or an image
def searchable_pdf_process(file):
    ext = os.path.splitext(file)
    if ext[-1].lower() == '.pdf':
        create_searchable_pdf(file)
    else:
        img_to_searchable_pdf(file)

# Process scanned PDF or image
path = r"c:\Users\subba\OneDrive\Desktop\Certificates_Software\KLU\December 2017.pdf"
searchable_pdf_process(path)











# Importing Libraries
# !pip install pytesseract
# !pip install opencv-python

import pytesseract 
import cv2
# !pip install pdf2image
from pdf2image import convert_from_path
import glob
from PyPDF2 import PdfFileMerger
import os
import warnings
warnings.filterwarnings("ignore")

# Cleaning directories
def clean_dir():
    d = "image_path"
    filesToRemove = [os.path.join(d, f) for f in os.listdir(d)]
    for f in filesToRemove:
        os.remove(f)

    d = "pdfs_out_path"
    filesToRemove = [os.path.join(d, f) for f in os.listdir(d)]
    for f in filesToRemove:
        os.remove(f)

# Converting Scanned PDF to searchable PDF
def create_searchable_pdf(pdf_file):
    ext = os.path.splitext(pdf_file)
    filename = ext[0].split("//")[-1]
    total_images = 0

    # Setting Tesseract path
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

    pdfs = glob.glob(pdf_file)
    for pdf_path in pdfs:
        # Converting PDF to images using Poppler
        pages = convert_from_path(pdf_path, 500, poppler_path=r'C:\Program Files\poppler-0.68.0\bin')
        for pageNum, imgBlob in enumerate(pages):
            if not os.path.isdir("image_path/"):
                os.mkdir("image_path/", mode=0o777)
            imgBlob.save(f'image_path/{pageNum}_{filename}_image_converted.jpg')
            total_images += 1

    print(f'PDF file converted to {total_images} images successfully.')

    pdfs = []
    tessdata_dir_config = '--tessdata-dir "C://Program Files//Tesseract-OCR//tessdata"'
    for root, dirs, files in os.walk("image_path/"):
        for file in files:
            fname = os.path.splitext(file)[0].split("//")[-1]
            img = cv2.imread(f"image_path/{file}", 1)
            result = pytesseract.image_to_pdf_or_hocr(img, lang="eng", config=tessdata_dir_config)
            if not os.path.isdir("pdfs_out_path/"):
                os.mkdir("pdfs_out_path/", mode=0o777)
            with open(f"pdfs_out_path/{fname}_searchable_pdf.pdf", "w+b") as f:
                f.write(bytearray(result))
            pdfs.append(f"{fname}_searchable_pdf.pdf")

    # Merging individual PDFs
    merger = PdfFileMerger()
    for pdf in pdfs:
        merger.append(f"pdfs_out_path/{pdf}")

    if not os.path.isdir("result/"):
        os.mkdir("result/", mode=0o777)
    merger.write(f"result/{filename}_searchable_pdf_from_pdf.pdf")
    print("Searchable PDF Created Successfully")
    merger.close()
    clean_dir()

# Converting scanned image to searchable PDF
def img_to_searchable_pdf(file):
    ext = os.path.splitext(file)
    filename = ext[0].split("//")[-1]

    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
    tessdata_dir_config = '--tessdata-dir "C://Program Files//Tesseract-OCR//tessdata"'

    img = cv2.imread(file, 1)
    result = pytesseract.image_to_pdf_or_hocr(img, lang="eng", config=tessdata_dir_config)
    if not os.path.isdir("result/"):
        os.mkdir("result/", mode=0o777)
    with open(f"result/{filename}_searchable_pdf_from_image.pdf", "w+b") as f:
        f.write(bytearray(result))
    print("Searchable PDF Created Successfully")

# Process for PDF or Image files
def searchable_pdf_process(file):
    ext = os.path.splitext(file)
    if ext[-1].lower() == '.pdf':
        create_searchable_pdf(file)
    else:
        img_to_searchable_pdf(file)

# For scanned PDF
path = r"c:\Users\subba\OneDrive\Desktop\Certificates_Software\KLU\December 2017.pdf"
searchable_pdf_process(path)












from azure.ai.openai import OpenAIClient
from azure.core.credentials import AzureKeyCredential
import fitz  # PyMuPDF
import io
import os
from PIL import Image
import base64
import json

# Azure OpenAI Configuration
azure_openai_endpoint = "https://YOUR_AZURE_ENDPOINT.openai.azure.com/"
azure_openai_key = "YOUR_AZURE_OPENAI_KEY"
deployment_name = "YOUR_DEPLOYMENT_NAME"  # Your model deployment in Azure
client = OpenAIClient(azure_openai_endpoint, AzureKeyCredential(azure_openai_key))

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

def pdf_to_base64_images(pdf_path):
    pdf_document = fitz.open(pdf_path)
    base64_images = []
    temp_image_paths = []

    total_pages = len(pdf_document)
    for page_num in range(total_pages):
        page = pdf_document.load_page(page_num)
        pix = page.get_pixmap()
        img = Image.open(io.BytesIO(pix.tobytes()))
        temp_image_path = f"temp_page_{page_num}.png"
        img.save(temp_image_path, format="PNG")
        temp_image_paths.append(temp_image_path)
        base64_image = encode_image(temp_image_path)
        base64_images.append(base64_image)

    for temp_image_path in temp_image_paths:
        os.remove(temp_image_path)

    return base64_images  # Return list of base64 images for all pages

def extract_invoice_data(base64_image):
    system_prompt = """
    You are an OCR-like data extraction tool that extracts hotel invoice data from PDFs.
    1. Please extract the data in this hotel invoice, grouping data according to theme/sub groups, and output it into JSON.
    2. Please keep the keys and values of the JSON in the original language.
    3. The type of data you might encounter includes hotel info, guest info, room charges, taxes, and total charges, etc.
    4. If there is no charge data, output an empty JSON object.
    5. For blank fields, include them as "null" in the JSON object.
    6. Maintain table structure for charges, capturing all rows/columns, even if blank.
    """

    # Azure OpenAI API call
    response = client.chat_completions.create(
        deployment_id=deployment_name,
        messages=[
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": f"Extract the data in this hotel invoice and output into JSON:\nImage data: data:image/png;base64,{base64_image}"
            }
        ],
        temperature=0.0
    )

    # Parse the response
    return json.loads(response.choices[0].message["content"])

# Example usage
pdf_path = "path/to/your/hotel_invoice.pdf"
base64_images = pdf_to_base64_images(pdf_path)

# Process each image (page)
for base64_image in base64_images:
    invoice_data = extract_invoice_data(base64_image)
    print(json.dumps(invoice_data, indent=2))  # Pretty print JSON output








https://medium.com/@odhitom09/using-gpt-4o-instead-of-ocr-for-data-extraction-and-processing-in-elt-workflows-a6558f769c12







# Objective:

Your task is to generate **procedural question-and-answer pairs** by analyzing the provided input, which includes indexed **text content** and **image descriptions**. The answers must be **comprehensive and logically structured**, combining information from both **text** and **image descriptions**, including details from consecutive indexes if required.

---

## Steps to read the input:

1. **Read all provided indexed content**, including both **text and image descriptions**.
2. For each index, identify the instructions, tables, or fields in the **text content**. **If related information is in the image description, integrate it into the relevant procedural steps.**
3. Consider the possibility that **the next or subsequent indexes may contain additional details** for the current step, especially if dependencies are implied (e.g., formulas, tables, references, or specific instructions).
4. **Include any additional procedural details found in images** (e.g., file locations, column descriptions, formats) where relevant.
5. Maintain the **logical sequence of steps** across indexes while ensuring that no detail is skipped or left incomplete.

---

## Key Guidelines for Generating Procedural Steps:

1. **Combine both text and image descriptions:** Integrate procedural steps wherever applicable from both the textual content and image description for the corresponding index or subsequent indexes.
2. **Dependencies across pages:** If a step in an image description requires data or instructions from the next or another index, **track and include them within the same procedural answer.**
3. Ensure that all steps are **clearly defined and sequential**, and reference any visual context (e.g., tables, diagrams, or images) where required.
4. Output the responses in the following **JSON format only**.

---

## JSON format:
```json
[
  {
    "question": "What are the steps to complete this process?",
    "answer": [
      "Step 1: [Extract and include necessary details from the indexed text or image description].",
      "Step 2: [Continue logical steps based on text or visual details].",
      "Step n: [If applicable, include any relevant follow-up information from subsequent indexes or visual elements]."
    ]
  }
]











# Objective:

Your task is to analyze the provided input containing indexed text and image descriptions to generate procedural question-and-answer pairs, ensuring no important details from the text or image descriptions are omitted. The procedural responses should include **step-by-step instructions** while considering the **logical flow** and **contextual relevance** of each index.

# Steps for reading the input:

1. Read the input carefully, including **both indexed text and image descriptions**.
2. For each index, consider all text provided as well as any image descriptions associated with it.
3. **If the content of the image description adds procedural details, integrate those details into the appropriate step within the answer.**
4. Ensure that the steps are **logically ordered** and maintain a **cohesive flow** from start to end.

# Guidelines for combining text and image descriptions:
- If an image description provides additional details (e.g., tables, diagrams, or data representations), **include them where they enhance or explain specific steps**.
- Avoid duplicating content but ensure that **relevant visual information** is properly referenced in procedural steps where applicable.

# Key procedural instructions:
1. Generate a **JSON array** containing procedural question-answer pairs.
2. Ensure that **step numbers are sequential** and cover all necessary details, including those from both text and image descriptions.
3. Maintain accuracy, logical progression, and clarity for each step.
4. The output must **ONLY** contain a JSON array response, and each entry must follow this format:

# JSON format:

```json
[
  {
    "question": "What are the steps to complete this process?",
    "answer": [
      "Step 1: [Include necessary details from indexed text or image description].",
      "Step 2: [Continue logical steps].",
      "Step n: [Incorporate visual context if relevant, referencing the image description]."
    ]
  }
]












import re
import json
import pandas as pd
from langchain.schema import HumanMessage

def clean_response_text(response_text):
    """
    Cleans the LLM response by removing unwanted metadata and fixing JSON formatting issues.
    """
    # Step 1: Remove unwanted metadata like 'content_filter_result'
    cleaned_text = re.sub(r"'?content_filter_result.*", "", response_text, flags=re.DOTALL)

    # Step 2: Fix any common JSON formatting issues
    cleaned_text = cleaned_text.strip().rstrip(",")  # Remove trailing commas
    
    # Ensure it’s a valid JSON array
    if not cleaned_text.startswith("["):
        cleaned_text = "[" + cleaned_text
    if not cleaned_text.endswith("]"):
        cleaned_text += "]"

    return cleaned_text

def main(pdf_path, num_questions):
    # Assuming text and images extraction are done before this
    # Prepare and send the message to LLM
    # Replace 'llm.invoke' and 'message' with your existing API call and input
    response = llm.invoke([message])

    # Step 3: Extract and clean response
    response_text = response[0].content  # Directly access the content attribute
    cleaned_text = clean_response_text(response_text)  # Clean unwanted content

    # Step 4: Parse the cleaned response into JSON
    try:
        parsed_response = json.loads(cleaned_text)
    except json.JSONDecodeError as e:
        raise ValueError(f"Failed to parse cleaned JSON response: {e}")

    # Step 5: Extract questions and answers
    qa_pairs = [(item["question"], item.get("answer", "No answer provided")) for item in parsed_response if "question" in item]

    # Save to Excel
    save_to_excel(qa_pairs, "extracted_questions_answers.xlsx")

def save_to_excel(data, output_file):
    # Save questions and answers to an Excel sheet
    df = pd.DataFrame(data, columns=["Question", "Answer"])
    df.to_excel(output_file, index=False)
    print(f"Saved to {output_file}")

if __name__ == "__main__":
    pdf_path = r"path/to/your/pdf.pdf"
    num_questions = 10  # Pass as needed
    main(pdf_path, num_questions)







# Assuming this is the part where you process the response
response = llm.invoke([message])

# Extracting response content
if isinstance(response[0], dict) and "content" in response[0]:
    response_text = response[0]["content"]
elif hasattr(response[0], 'content'):
    response_text = response[0].content
else:
    raise ValueError("Unable to extract content from the response.")

# Validate and parse the JSON content
try:
    parsed_response = json.loads(response_text)
except json.JSONDecodeError as e:
    raise ValueError(f"Invalid JSON response: {e}")

# Process the parsed response as needed
qa_pairs = [(item["question"], item.get("answer", "No answer provided")) for item in parsed_response]
save_to_excel(qa_pairs, "extracted_questions_answers.xlsx")










import fitz  # PyMuPDF
import base64
import json
import pandas as pd
from langchain.schema import HumanMessage
from azure.ai.openai import AzureOpenAI  # Assuming this part is already configured

def extract_text_and_images(pdf_path, num_pages):
    pdf_document = fitz.open(pdf_path)
    encoded_images = []
    combined_text = []

    for page_number in range(min(num_pages, pdf_document.page_count)):
        page = pdf_document[page_number]
        
        # Extract text
        page_text = page.get_text("text").strip()
        combined_text.append(page_text)

        # Extract image and encode as base64
        pix = page.get_pixmap(dpi=200)
        image_bytes = pix.tobytes("png")
        encoded_image = base64.b64encode(image_bytes).decode("utf-8")
        encoded_images.append(encoded_image)

    text = "\n\n".join(combined_text)
    return text, encoded_images

def detect_and_correct_json(response_text):
    try:
        # Try parsing the response as JSON
        parsed_response = json.loads(response_text)
        return parsed_response
    except json.JSONDecodeError:
        print("Invalid JSON detected in the response!")
        return None

def save_to_excel(data, output_file):
    # Save questions and answers to an Excel file
    df = pd.DataFrame(data, columns=["Question", "Answer"])
    df.to_excel(output_file, index=False)
    print(f"Saved to {output_file}")

def main(pdf_path, num_questions):
    # Extract text and images
    text, encoded_images = extract_text_and_images(pdf_path, num_pages=40)

    # Define prompt and instructions
    prompt_template = "Please extract {num_questions} meaningful questions and their corresponding answers from the given content."
    instructions = "Provide the output in JSON format."
    
    # Create the message for the model
    message = HumanMessage(
        content=[
            {
                "type": "text",
                "text": prompt_template.format(num_questions=num_questions),
            }
        ] + [
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{img_data}"}
            }
            for img_data in encoded_images
        ]
    )

    # Invoke the model
    azure_openai = AzureOpenAI(api_version="2023-03-15-preview", deployment_name="ssgpt-46")  # Replace as per configuration
    response = azure_openai.invoke([message])

    # Process and validate the response
    parsed_response = detect_and_correct_json(response)

    if parsed_response:
        # Assuming JSON response contains a list of {"question": ..., "answer": ...}
        qa_pairs = [(qa["question"], qa["answer"]) for qa in parsed_response]
        save_to_excel(qa_pairs, "extracted_questions_answers.xlsx")
    else:
        print("Failed to save due to invalid JSON.")

if __name__ == "__main__":
    # Run the function with the desired PDF path and number of questions
    pdf_path = r"h:\Question and answer Fund accounting (Resubmit 1025)36. VFMC AA525 SOP 5.8 1.pdf"
    num_questions = 10  # Specify the number of questions
    main(pdf_path, num_questions)











import fitz  # PyMuPDF
import base64
from langchain.schema import HumanMessage
import openai
import json
import pandas as pd

# Azure OpenAI Configuration (Example using invoke)
openai.api_type = "azure"
openai.api_version = "azure_openai_api_version"
openai.api_base = "https://azure_openai_endpoint"
openai.api_key = "your_azure_openai_api_key"
deployment_name = "ssgpt-40"

# Function to extract text and images from the PDF
def extract_pdf_content(pdf_path, max_pages=40):
    pdf_document = fitz.open(pdf_path)

    encoded_images = []
    combined_text = []

    for page_number in range(min(max_pages, pdf_document.page_count)):
        page = pdf_document[page_number]

        # Extract text from the page
        page_text = page.get_text("text").strip()
        combined_text.append(page_text)

        # Extract image and encode as base64
        pix = page.get_pixmap(dpi=200)
        image_bytes = pix.tobytes("png")
        encoded_image = base64.b64encode(image_bytes).decode("utf-8")
        encoded_images.append(encoded_image)

    all_text = "\n\n".join(combined_text)
    return all_text, encoded_images

# Function to generate questions and answers using OpenAI
def generate_questions_and_answers(text_content, images, num_questions):
    # Define the prompt template
    prompt_template = f"""
    Based on the following text and image content, generate {num_questions} meaningful questions and answers in JSON format:
    [
        {{"question": "?", "answer": "?"}}
    ]

    Text Content:
    {text_content}
    """

    # Create the HumanMessage as per the given structure
    message = HumanMessage(
        content=[
            {
                "type": "text",
                "text": prompt_template
            }
        ] + [
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{img_data}"}
            }
            for img_data in images
        ]
    )

    # Azure OpenAI API call
    response = openai.ChatCompletion.create(
        engine=deployment_name,
        messages=[{"role": "system", "content": "You are an assistant that processes both text and image data."}, message],
        temperature=0
    )

    # Parse the response
    response_content = response['choices'][0]['message']['content']
    return json.loads(response_content)

# Store results in an Excel file
def store_results_to_excel(results, output_file="questions_and_answers.xlsx"):
    data = []

    for item in results:
        data.append({
            "Question Type": "Generated",
            "Question": item["question"],
            "Answer": item["answer"]
        })

    # Create a DataFrame and save to Excel
    df = pd.DataFrame(data)
    df.to_excel(output_file, index=False)

# Main function to extract content and generate questions
def main(pdf_path, num_questions=5, output_file="questions_and_answers.xlsx"):
    # Extract text and images
    text_content, images = extract_pdf_content(pdf_path)

    # Generate questions and answers
    questions_and_answers = generate_questions_and_answers(text_content, images, num_questions)

    # Store results to Excel
    store_results_to_excel(questions_and_answers, output_file)

    print(f"Questions and answers have been stored in {output_file}")

# Run the main function
pdf_path = "your_pdf_file.pdf"
main(pdf_path, num_questions=10)












import pandas as pd
import os

def save_to_excel(questions_and_answers, output_file):
    # Check if there is data to save
    if questions_and_answers:
        # Create a DataFrame
        df = pd.DataFrame(questions_and_answers)
        
        # Save to Excel
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Define the output path with the correct file extension
output_file = r"H:\Question and answer\Fund accounting.xlsx"

# Ensure the subdirectory structure in the output folder
os.makedirs(os.path.dirname(output_file), exist_ok=True)

# Example response to test the function (replace this with your actual data)
response = [
    {"question": "What is AI?", "answer": "AI stands for Artificial Intelligence."},
    {"question": "What is Machine Learning?", "answer": "A subset of AI that focuses on learning from data."},
]

# Save the responses to the Excel file
save_to_excel(response, output_file)






import fitz  # PyMuPDF
import base64
from langchain.schema import HumanMessage

# Azure OpenAI Configuration (Example using invoke)
# Adjust this if you need based on the specific library being used
import openai

openai.api_type = "azure"
openai.api_version = "azure_openai_api_version"
openai.api_base = "https://azure_openai_endpoint"
openai.api_key = "your_azure_openai_api_key"
deployment_name = "ssgpt-40"

# Extract text and images from PDF
pdf_path = "your_pdf_file.pdf"
pdf_document = fitz.open(pdf_path)

encoded_images = []
combined_text = []

for page_number in range(min(40, pdf_document.page_count)):
    page = pdf_document[page_number]
    
    # Extract text from the page
    page_text = page.get_text("text").strip()
    combined_text.append(page_text)
    
    # Extract image and encode as base64
    pix = page.get_pixmap(dpi=200)
    image_bytes = pix.tobytes("png")
    encoded_image = base64.b64encode(image_bytes).decode("utf-8")
    encoded_images.append(encoded_image)

# Define the prompt template
prompt_template = """
Based on the following text and image content, generate meaningful questions and answers in JSON format:
[
    {{"question": "?", "answer": "?"}}
]

Text Content:
{text}

"""

# Placeholder instructions and parser (if applicable)
instructions = "Extract meaningful questions and answers from the given content."
parser = {
    "get_format_instructions": lambda: "Please provide the output in JSON format."
}

# Combine all text
all_text = "\n\n".join(combined_text)

# Create the HumanMessage as per the given structure
message = HumanMessage(
    content=[
        {
            "type": "text",
            "text": prompt_template.format(
                text=all_text,
                priority_instructions=instructions,
                format_instructions={"format_instructions": parser["get_format_instructions"]()},
            )
        }
    ] + [
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/png;base64,{img_data}"}
        }
        for img_data in encoded_images
    ]
)

# Azure OpenAI API call using invoke
response = openai.ChatCompletion.create(
    engine=deployment_name,
    messages=[{"role": "system", "content": "You are an assistant that processes both text and image data."}, message],
    temperature=0
)

# Print the response
print(response['choices'][0]['message']['content'])













import fitz  # PyMuPDF
import base64
from langchain.schema import HumanMessage
from azure.ai.openai import AzureOpenAI

# Azure OpenAI Configuration
azure_openai = AzureOpenAI(
    api_version="azure_openai_api_version",
    endpoint="azure_openai_endpoint",
    api_key="your_azure_openai_api_key"
)

# Extract text and images from PDF
pdf_path = "your_pdf_file.pdf"
pdf_document = fitz.open(pdf_path)
encoded_images = []
combined_pages = []

for page_number in range(min(40, pdf_document.page_count)):
    page = pdf_document[page_number]
    
    # Extract text from the page
    page_text = page.get_text("text").strip()
    
    # Extract and encode image as base64
    pix = page.get_pixmap(dpi=200)
    image_bytes = pix.tobytes("png")
    encoded_image = base64.b64encode(image_bytes).decode("utf-8")
    
    # Store combined text and image
    combined_pages.append({
        "text": page_text,
        "encoded_image": f"data:image/png;base64,{encoded_image}"
    })

# Prompt template to combine text and image
prompt_template = """
Based on the provided text and image content, generate meaningful questions and their corresponding answers. 
Format the output as follows:
[
    {{"question": "?", "answer": "?"}}
]

Text Content:
{text}

Image Content: [Attached as a base64-encoded image]
"""

results = []

# Call the Azure LLM using HumanMessage
for page_info in combined_pages:
    prompt = prompt_template.format(text=page_info["text"])
    
    # Human message combining text and base64 image
    message = HumanMessage(
        content=prompt + f"\nImage Content: {page_info['encoded_image']}"
    )
    
    # Azure LLM response
    response = azure_openai.invoke(messages=[message])
    
    # Collect the output
    result = response["choices"][0]["message"]["content"]
    results.append(result)

# Display results for each page
for index, output in enumerate(results):
    print(f"\nPage {index + 1} Output:\n{output}")









def generate_markdown_from_pages(data):
    markdown_output = ""
    
    for idx, page in enumerate(data["pages"]):
        # Add text section
        markdown_output += f"### {idx} text:\n```\n{page['text']}\n```\n"
        
        # Collect and add image descriptions dynamically
        for key, value in page.items():
            if key.startswith("image_description"):
                markdown_output += f"### {idx} {key.replace('_', ' ')}:\n```\n{value}\n```\n"
    
    return markdown_output

# Example usage
input_data = {
    "pages": [
        {
            "page_number": 1,
            "text": "Information Classification: Limited Access \n \nPage 1 \nInformation Classificat",
            "image_description_1": "The document in question appears to be a professional financial st"
        },
        {
            "page_number": 2,
            "text": "Hub Procedure Oversight Owner \nNames(s): Not Applicable Oversight Owner Email:",
            "image_description_1": "No image descriptions available"
        },
        {
            "page_number": 3,
            "text": "1. DMH Data Maintenance \n1.1 Ensure monthly book in MCH have been released \n\u",
            "image_description_1": "The image described appears to be a detailed financial document, li",
            "image_description_2": "The image depicts a detailed financial document from State Street C"
        }
    ]
}

# Generate markdown output
markdown_result = generate_markdown_from_pages(input_data)
print(markdown_result)











Key Points to Understand the JSON Structure:
Page Number:

Represents the page number of the document, allowing you to track the sequence of content.
This helps in navigation or retrieval of specific pages.
Text:

Refers to the textual content of the page (paragraphs, headings, or other important content).
Each page may have one or more lines of text, which could include titles, instructions, or other relevant details.
Image Descriptions:

Describes the images present on each page of the document.
The description provides context for the image, explaining what the image represents.
Each image has a corresponding description and can be referenced in the following ways:
Image Description 1: Refers to the first image.
Image Description 2: Refers to the second image.
Image Description 3: Refers to the third image.
If there are no images on a page, this field is an empty array.
Multiple Image Descriptions:

A page can contain multiple images, and each image is described separately in the image_description array.
If a page has multiple images, they are listed sequentially as "Image Description 1", "Image Description 2", etc.
JSON Format:

The structure of the JSON input ensures data organization with clear references for page numbers, text, and images.
Pages are represented as objects within an array, making it easy to represent documents with multiple pages.
Example JSON Format Recap:
json
Copy code
[
  {
    "page_number": 1,
    "text": "Information Classification: Limited Access\nGlobal Delivery Standard Operating Procedure (SOP) Template",
    "image_description": [
      "STATE STREET Logo",
      "Graph showing global delivery flow",
      "Schematic diagram of SOP process"
    ]
  },
  {
    "page_number": 2,
    "text": "Hub Procedure Oversight Owner",
    "image_description": [
      "Flowchart of oversight roles"
    ]
  },
  {
    "page_number": 3,
    "text": "1. DMH Data Maintenance\nEnsure monthly book in MCH have been released\nIn may take omin s for MCH accounting data flow to FOR platform and DMH will ge",
    "image_description": []
  }
]
Summary of Key Points:
Page Number: A reference to each page in the document.
Text: Contains textual content (e.g., paragraphs, headings).
Image Descriptions: Describes images found on the page (could be one or more).
Each image is listed sequentially (Image Description 1, Image Description 2, etc.).
If no images are present, the array remains empty.
Structured Format: Easy to parse and understand, especially for documents with multiple pages.
This breakdown covers all the essential points for understanding how to interpret and structure the JSON input effectively. Let me know if you'd like any additional information!










Search










output_file_path = 'output.md'

# Open file in UTF-8 mode to support special characters
with open(output_file_path, "w", encoding="utf-8") as file:
    file.write(markdown_content)

print(f"Markdown content saved to {output_file_path}")










import json

# Load the JSON content (you can replace this with file reading if necessary)
json_data = {
    "pages": [
        {
            "page_number": 1,
            "text": "Information Classification: Limited Access \nPage 1",
            "image_description_1": "STATE STREET"
        },
        {
            "page_number": 2,
            "text": "Hub Procedure Oversight Owner \nNames(s): Not Applicable",
            "image_description_1": "No image descriptions available"
        },
        {
            "page_number": 3,
            "text": "1. DMH Data Maintenance \n1.1 Ensure monthly book in MCH",
            "image_description_1": "It looks like you've uploaded an icon of an Excel file",
            "image_description_2": "DATE: 190822\nTIME: 05:54\nSTATE STREET CORP"
        }
    ]
}

# Function to convert JSON to Markdown
def json_to_markdown(data):
    markdown_text = ""
    for page in data.get("pages", []):
        page_number = page.get("page_number", "N/A")
        text = page.get("text", "").strip()
        
        # Add page header
        markdown_text += f"# Page {page_number}\n\n"
        
        # Add text section
        if text:
            markdown_text += f"**Text:**\n{text}\n\n"
        
        # Add image descriptions
        for key, value in page.items():
            if key.startswith("image_description"):
                markdown_text += f"**{key.replace('_', ' ').capitalize()}:**\n{value}\n\n"

    return markdown_text

# Convert JSON to Markdown
markdown_content = json_to_markdown(json_data)

# Save to markdown file
output_file_path = '/mnt/data/output.md'
with open(output_file_path, "w") as file:
    file.write(markdown_content)

output_file_path








import json

# Sample JSON Data (Replace this with actual JSON file reading if needed)
json_data = [
    {
        "page_number": 9,
        "text": "2.5 Download and save NRM as CSV format.\n\n2.6 Report Dashboard \na) =",
        "image_description_1": "Here is the extracted text from the image:\n\nNavigation\nAp",
        "image_description_2": "Sure, here is the extracted text from the image:\n\n**Naviga"
    },
    {
        "page_number": 10,
        "text": "• In the second tile ‘Job History of AAS_OUB_ESP_VFMC’, sele",
        "image_description_1": "**Navigation**\nApplications: AAS25 Report\nClient: VFMC\n\n-",
        "image_description_2": "Sure, here is the extracted text from the image:\n\n\nNorr"
    }
]

# Function to convert JSON to Markdown
def json_to_markdown(data):
    markdown_text = ""
    for item in data:
        page_number = item.get("page_number", "N/A")
        text = item.get("text", "").strip()
        
        # Add header for each page
        markdown_text += f"# Page {page_number}\n\n"
        
        # Add text section
        if text:
            markdown_text += f"**Text:**\n{text}\n\n"
        
        # Add image descriptions
        image_descriptions = [key for key in item.keys() if key.startswith("image_description")]
        for desc_key in image_descriptions:
            markdown_text += f"**{desc_key.replace('_', ' ').capitalize()}:**\n{item[desc_key]}\n\n"
    
    return markdown_text

# Convert JSON to Markdown
markdown_content = json_to_markdown(json_data)

# Save to markdown file
output_file_path = "output.md"
with open(output_file_path, "w") as file:
    file.write(markdown_content)

print(f"Markdown content saved to {output_file_path}")









import os
import base64
import PyPDF2
import fitz  # PyMuPDF
import math
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema.messages import HumanMessage


def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text


def extract_images_from_pdf(pdf_path: str) -> List[str]:
    doc = fitz.open(pdf_path)
    encoded_images = []

    for page_number in range(len(doc)):
        images = doc[page_number].get_images(full=True)
        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    
    return encoded_images


def prepare_human_message(pdf_text: str, image_data: List[str], prompt_template: str) -> HumanMessage:
    base_prompt = (
        f"{prompt_template}\n\n"
        f"List to Ignore: ['Line number', 'MSH Item', 'SKU number']\n"
        f"Priority Instructions: Include image-related content if available.\n"
        f"Format Instructions: Extract in JSON format with sections and subsections.\n\n"
    )
    
    content = f"{base_prompt}Extracted Text:\n{pdf_text}\n\n"
    for img_data in image_data:
        content += f"Image (base64): data:image/jpeg;base64,{img_data}\n\n"
    
    return HumanMessage(content=content)


def generate_questions_from_message(message_content: HumanMessage, question_type: str, num_questions: int, prompt_template: str) -> List[Dict]:
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_template
    )
    
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    try:
        # Correcting the method call to use predict()
        response = llm_chain.predict(content=message_content.content)
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating questions: {e}")
        return []


def generate_questions_in_chunks(message_content: HumanMessage, question_type: str, total_questions: int, prompt_template: str, chunk_size: int = 10) -> List[Dict]:
    all_questions = []

    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions_from_message(
                message_content=message_content,
                question_type=question_type,
                num_questions=min(chunk_size, total_questions - i),
                prompt_template=prompt_template
            )
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue

    return all_questions


def generate_questions_by_all_types(message_content: HumanMessage, total_questions: int, chunk_size: int = 10) -> List[Dict]:
    question_types_and_prompts = {
        "Conceptual": "CONCEPTUAL PROMPT",
        "Procedural": "PROCEDURAL PROMPT",
        "Inferential": "INFERENTIAL PROMPT",
        "Reasoning-based": "REASONING PROMPT"
    }

    question_counts = {q_type: 0 for q_type in question_types_and_prompts}
    all_questions = []

    def generate_for_type(question_type, prompt, questions_to_generate):
        questions = generate_questions_in_chunks(message_content, question_type, questions_to_generate, prompt, chunk_size)
        if questions:
            questions = remove_duplicates(questions)
            question_counts[question_type] += len(questions)
            return question_type, questions
        return question_type, []

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_for_type, question_type, prompt, max(1, math.ceil(total_questions / len(question_types_and_prompts)))):
            question_type for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            try:
                question_type, questions = future.result()
                if questions:
                    for qa in questions:
                        all_questions.append({
                            "Question Type": question_type,
                            "Question": qa["question"],
                            "Answer": qa["answer"],
                            "Extracted": qa["extracted"]
                        })
            except Exception as e:
                print(f"Error processing questions for {futures[future]}: {e}")

    return all_questions[:total_questions]


def process_pdfs_in_subdirectories(root_directory: str, total_questions: int = 10, chunk_size: int = 10, output_folder: str = "output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    # Step 1: Extract text and metadata
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Step 2: Extract image descriptions
                    image_data = extract_images_from_pdf(pdf_path)

                    # Step 3: Prepare HumanMessage with properly structured text and image content
                    message_content = prepare_human_message(pdf_text, image_data, "Your defined prompt here.")

                    # Step 4: Generate questions using the structured message content
                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    # Step 5: Save to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)

                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")













import os
import base64
import PyPDF2
import fitz  # PyMuPDF
import math
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema.messages import HumanMessage


# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text


# Function to extract images and encode them in base64
def extract_images_from_pdf(pdf_path: str) -> List[str]:
    doc = fitz.open(pdf_path)
    encoded_images = []

    for page_number in range(len(doc)):
        images = doc[page_number].get_images(full=True)
        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    
    return encoded_images


# Prepare HumanMessage with proper content formatting
def prepare_human_message(pdf_text: str, image_data: List[str], prompt_template: str) -> HumanMessage:
    base_prompt = prompt_template.format(
        listIgnore=["Line number", "MSH Item", "SKU number"],
        priority_instructions="Include image-related content if available.",
        format_instructions="Extract in JSON format with sections and subsections."
    )
    
    content = f"{base_prompt}\n\nExtracted Text:\n{pdf_text}\n\n"
    for img_data in image_data:
        content += f"Image (base64): data:image/jpeg;base64,{img_data}\n\n"
    
    return HumanMessage(content=content)


# LLM question generation function using structured message content
def generate_questions_from_message(message_content: HumanMessage, question_type: str, num_questions: int, prompt_template: str) -> List[Dict]:
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_template
    )
    
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    try:
        response = llm_chain.predict_messages([message_content])
        return extract_valid_json(response.content)
    except Exception as e:
        print(f"Error generating questions: {e}")
        return []


def generate_questions_in_chunks(message_content: HumanMessage, question_type: str, total_questions: int, prompt_template: str, chunk_size: int = 10) -> List[Dict]:
    all_questions = []

    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions_from_message(
                message_content=message_content,
                question_type=question_type,
                num_questions=min(chunk_size, total_questions - i),
                prompt_template=prompt_template
            )
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue

    return all_questions


# Main function to generate questions for all types
def generate_questions_by_all_types(message_content: HumanMessage, total_questions: int, chunk_size: int = 10) -> List[Dict]:
    question_types_and_prompts = {
        "Conceptual": "CONCEPTUAL PROMPT",
        "Procedural": "PROCEDURAL PROMPT",
        "Inferential": "INFERENTIAL PROMPT",
        "Reasoning-based": "REASONING PROMPT"
    }

    question_counts = {q_type: 0 for q_type in question_types_and_prompts}
    all_questions = []

    def generate_for_type(question_type, prompt, questions_to_generate):
        questions = generate_questions_in_chunks(message_content, question_type, questions_to_generate, prompt, chunk_size)
        if questions:
            questions = remove_duplicates(questions)
            question_counts[question_type] += len(questions)
            return question_type, questions
        return question_type, []

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_for_type, question_type, prompt, max(1, math.ceil(total_questions / len(question_types_and_prompts)))):
            question_type for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            try:
                question_type, questions = future.result()
                if questions:
                    for qa in questions:
                        all_questions.append({
                            "Question Type": question_type,
                            "Question": qa["question"],
                            "Answer": qa["answer"],
                            "Extracted": qa["extracted"]
                        })
            except Exception as e:
                print(f"Error processing questions for {futures[future]}: {e}")

    return all_questions[:total_questions]


def process_pdfs_in_subdirectories(root_directory: str, total_questions: int = 10, chunk_size: int = 10, output_folder: str = "output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    # Step 1: Extract text and metadata
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Step 2: Extract image descriptions
                    image_data = extract_images_from_pdf(pdf_path)

                    # Step 3: Prepare HumanMessage with properly structured text and image content
                    message_content = prepare_human_message(pdf_text, image_data, "Your defined prompt here.")

                    # Step 4: Generate questions using the structured message content
                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    # Step 5: Save to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)

                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")









import os
import base64
import PyPDF2
import fitz  # PyMuPDF
import math
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema.messages import HumanMessage


# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text


# Function to extract images and encode them in base64
def extract_images_from_pdf(pdf_path: str) -> List[str]:
    doc = fitz.open(pdf_path)
    encoded_images = []

    for page_number in range(len(doc)):
        images = doc[page_number].get_images(full=True)
        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    
    return encoded_images


# LLM question generation function using structured message content
def generate_questions_from_message(message_content: HumanMessage, question_type: str, num_questions: int, prompt_template: str) -> List[Dict]:
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_template
    )
    
    # Creating the LLM chain and generating questions
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    try:
        response = llm_chain.predict_messages([message_content])
        # Process response to extract valid JSON
        return extract_valid_json(response.content)
    except Exception as e:
        print(f"Error generating questions: {e}")
        return []


def generate_questions_in_chunks(message_content: HumanMessage, question_type: str, total_questions: int, prompt_template: str, chunk_size: int = 10) -> List[Dict]:
    all_questions = []

    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions_from_message(
                message_content=message_content,
                question_type=question_type,
                num_questions=min(chunk_size, total_questions - i),
                prompt_template=prompt_template
            )
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue

    return all_questions


# Main function to generate questions for all types
def generate_questions_by_all_types(message_content: HumanMessage, total_questions: int, chunk_size: int = 10) -> List[Dict]:
    question_types_and_prompts = {
        "Conceptual": "CONCEPTUAL PROMPT",
        "Procedural": "PROCEDURAL PROMPT",
        "Inferential": "INFERENTIAL PROMPT",
        "Reasoning-based": "REASONING PROMPT"
    }

    question_counts = {q_type: 0 for q_type in question_types_and_prompts}
    all_questions = []

    def generate_for_type(question_type, prompt, questions_to_generate):
        questions = generate_questions_in_chunks(message_content, question_type, questions_to_generate, prompt, chunk_size)
        if questions:
            questions = remove_duplicates(questions)
            question_counts[question_type] += len(questions)
            return question_type, questions
        return question_type, []

    # Use ThreadPoolExecutor for concurrent generation of questions
    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_for_type, question_type, prompt, max(1, math.ceil(total_questions / len(question_types_and_prompts)))):
            question_type for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            try:
                question_type, questions = future.result()
                if questions:
                    for qa in questions:
                        all_questions.append({
                            "Question Type": question_type,
                            "Question": qa["question"],
                            "Answer": qa["answer"],
                            "Extracted": qa["extracted"]
                        })
            except Exception as e:
                print(f"Error processing questions for {futures[future]}: {e}")

    return all_questions[:total_questions]


# Process PDFs in subdirectories
def process_pdfs_in_subdirectories(root_directory: str, total_questions: int = 10, chunk_size: int = 10, output_folder: str = "output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    # Step 1: Extract text and metadata
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Step 2: Extract image descriptions
                    image_data = extract_images_from_pdf(pdf_path)

                    # Step 3: Prepare HumanMessage with properly structured text and image content
                    prompt_template = "Your defined prompt here."
                    message_content = HumanMessage(
                        content=[
                            {
                                "type": "text",
                                "text": prompt_template.format(
                                    listIgnore=["Line number", "MSH Item", "SKU number"],
                                    priority_instructions="Include image-related content if available.",
                                    format_instructions="Extract in JSON format with sections and subsections."
                                )
                            },
                            {
                                "type": "text",
                                "text": pdf_text  # Correctly passing the PDF text
                            }
                        ] + [
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/jpeg;base64,{img_data}"}
                            } for img_data in image_data
                        ]
                    )

                    # Step 4: Generate questions using the structured message content
                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    # Step 5: Save to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)

                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")








import os
import base64
import PyPDF2
import fitz  # PyMuPDF
import math
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema.messages import HumanMessage


# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text


# Function to extract images and encode them in base64
def extract_images_from_pdf(pdf_path: str) -> List[str]:
    doc = fitz.open(pdf_path)
    encoded_images = []

    for page_number in range(len(doc)):
        images = doc[page_number].get_images(full=True)
        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    
    return encoded_images


# LLM question generation function using prompts
def generate_questions(paragraph: str, question_type: str, num_questions: int, prompt_template: str) -> List[Dict]:
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_template
    )
    
    # Fill the prompt using provided values
    filled_prompt = prompt.format(
        paragraph=paragraph,
        num_questions=str(num_questions),
        question_type=question_type
    )
    print(f"Generated prompt:\n{filled_prompt}")

    # Creating the LLM chain and generating questions
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions),
            question_type=question_type
        )
        # Process response to extract valid JSON
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating questions: {e}")
        return []


def generate_questions_in_chunks(paragraph: str, question_type: str, total_questions: int, prompt_template: str, chunk_size: int = 10) -> List[Dict]:
    all_questions = []

    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(
                paragraph=paragraph,
                question_type=question_type,
                num_questions=min(chunk_size, total_questions - i),
                prompt_template=prompt_template
            )
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue

    return all_questions


# Main function to generate questions for all types
def generate_questions_by_all_types(paragraph: str, total_questions: int, chunk_size: int = 10) -> List[Dict]:
    question_types_and_prompts = {
        "Conceptual": "CONCEPTUAL PROMPT",
        "Procedural": "PROCEDURAL PROMPT",
        "Inferential": "INFERENTIAL PROMPT",
        "Reasoning-based": "REASONING PROMPT"
    }

    question_counts = {q_type: 0 for q_type in question_types_and_prompts}
    all_questions = []

    def generate_for_type(question_type, prompt, questions_to_generate):
        questions = generate_questions_in_chunks(paragraph, question_type, questions_to_generate, prompt, chunk_size)
        if questions:
            questions = remove_duplicates(questions)
            question_counts[question_type] += len(questions)
            return question_type, questions
        return question_type, []

    # Use ThreadPoolExecutor for concurrent generation of questions
    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_for_type, question_type, prompt, max(1, math.ceil(total_questions / len(question_types_and_prompts)))):
            question_type for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            try:
                question_type, questions = future.result()
                if questions:
                    for qa in questions:
                        all_questions.append({
                            "Question Type": question_type,
                            "Question": qa["question"],
                            "Answer": qa["answer"],
                            "Extracted": qa["extracted"]
                        })
            except Exception as e:
                print(f"Error processing questions for {futures[future]}: {e}")

    return all_questions[:total_questions]


# Process PDFs in subdirectories
def process_pdfs_in_subdirectories(root_directory: str, total_questions: int = 10, chunk_size: int = 10, output_folder: str = "output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    # Step 1: Extract text and metadata
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Step 2: Extract image descriptions
                    image_data = extract_images_from_pdf(pdf_path)

                    # Step 3: Prepare HumanMessage with properly structured text and image content
                    prompt_template = "Your defined prompt here."
                    message_content = HumanMessage(
                        content=[
                            {
                                "type": "text",
                                "text": prompt_template.format(
                                    listIgnore=["Line number", "MSH Item", "SKU number"],
                                    priority_instructions="Include image-related content if available.",
                                    format_instructions="Extract in JSON format with sections and subsections."
                                )
                            },
                            {
                                "type": "text",
                                "text": pdf_text  # Correctly passing the PDF text
                            }
                        ] + [
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/jpeg;base64,{img_data}"}
                            } for img_data in image_data
                        ]
                    )

                    # Step 4: Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Step 5: Save to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)

                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")











import os
import base64
import PyPDF2
import fitz  # PyMuPDF
import json
import math
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.schema.messages import HumanMessage

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# Function to extract images and encode them in base64
def extract_images_from_pdf(pdf_path: str) -> List[str]:
    doc = fitz.open(pdf_path)
    encoded_images = []

    for page_number in range(len(doc)):
        images = doc[page_number].get_images(full=True)
        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    
    return encoded_images

# LLM question generation function using prompts
def generate_questions(paragraph: str, question_type: str, num_questions: int, prompt_template: str) -> List[Dict]:
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_template
    )
    
    # Fill the prompt using provided values
    filled_prompt = prompt.format(
        paragraph=paragraph,
        num_questions=str(num_questions),
        question_type=question_type
    )
    print(f"Generated prompt:\n{filled_prompt}")

    # Creating the LLM chain and generating questions
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions),
            question_type=question_type
        )
        # Process response to extract valid JSON
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating questions: {e}")
        return []

def generate_questions_in_chunks(paragraph: str, question_type: str, total_questions: int, prompt_template: str, chunk_size: int = 10) -> List[Dict]:
    all_questions = []

    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(
                paragraph=paragraph,
                question_type=question_type,
                num_questions=min(chunk_size, total_questions - i),
                prompt_template=prompt_template
            )
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue

    return all_questions

# Main function to generate questions for all types
def generate_questions_by_all_types(paragraph: str, total_questions: int, chunk_size: int = 10) -> List[Dict]:
    question_types_and_prompts = {
        "Conceptual": "CONCEPTUAL PROMPT",
        "Procedural": "PROCEDURAL PROMPT",
        "Inferential": "INFERENTIAL PROMPT",
        "Reasoning-based": "REASONING PROMPT"
    }

    question_counts = {q_type: 0 for q_type in question_types_and_prompts}
    all_questions = []

    def generate_for_type(question_type, prompt, questions_to_generate):
        questions = generate_questions_in_chunks(paragraph, question_type, questions_to_generate, prompt, chunk_size)
        if questions:
            questions = remove_duplicates(questions)
            question_counts[question_type] += len(questions)
            return question_type, questions
        return question_type, []

    # Use ThreadPoolExecutor for concurrent generation of questions
    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_for_type, question_type, prompt, max(1, math.ceil(total_questions / len(question_types_and_prompts)))):
            question_type for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            try:
                question_type, questions = future.result()
                if questions:
                    for qa in questions:
                        all_questions.append({
                            "Question Type": question_type,
                            "Question": qa["question"],
                            "Answer": qa["answer"],
                            "Extracted": qa["extracted"]
                        })
            except Exception as e:
                print(f"Error processing questions for {futures[future]}: {e}")

    return all_questions[:total_questions]

# Process PDFs in subdirectories
def process_pdfs_in_subdirectories(root_directory: str, total_questions: int = 10, chunk_size: int = 10, output_folder: str = "output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    # Step 1: Extract text and metadata
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Step 2: Extract image descriptions
                    image_data = extract_images_from_pdf(pdf_path)

                    # Step 3: Prepare input for LLM
                    prompt_template = "Your defined prompt here."
                    message_content = [
                        {
                            "type": "text",
                            "text": prompt_template.format(
                                listIgnore=["Line number", "MSH Item", "SKU number"],
                                priority_instructions="Include image-related content if available.",
                                format_instructions="Extract in JSON format with sections and subsections."
                            ) + "\n" + pdf_text
                        }
                    ] + [
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_data}"}} for img_data in image_data
                    ]

                    # Step 4: Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Step 5: Save to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)

                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")







import PyPDF2
import fitz  # PyMuPDF
import base64
import os
import math
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# PDF Text Extraction Function
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text += page_text.strip() + "\n"
    return text.strip()

# PDF Image Extraction Function
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []
    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    return encoded_images

# Create a valid PromptTemplate object
def create_prompt_template(prompt_text):
    return PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_text
    )

# Convert list of messages into a single paragraph string
def extract_text_from_messages(messages):
    text_content = " ".join([msg['text'] for msg in messages if msg['type'] == 'text'])
    return text_content.strip()

# Function to safely parse the LLM response and handle empty/invalid cases
def extract_valid_json(response):
    if not response or response.strip() == "":
        print("LLM response is empty. Skipping JSON parsing.")
        return []
    
    try:
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"JSON parsing error: {e}. Response received: {response[:100]}...")
        return []

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_text):
    prompt_template = create_prompt_template(prompt_text)
    paragraph_text = extract_text_from_messages(paragraph)  # Convert messages to text

    filled_prompt = prompt_template.format(paragraph=paragraph_text, question_type=question_type, num_questions=num_questions)
    print(f"Generated prompt:\n{filled_prompt}")

    llm_chain = LLMChain(llm=llm, prompt=prompt_template)
    response = llm_chain.predict(paragraph=paragraph_text, num_questions=str(num_questions), question_type=question_type)

    # Handle and debug the response
    if response is None or response.strip() == "":
        print("No response received from LLM.")
        return []

    questions = extract_valid_json(response)

    if not questions:
        print("No valid questions generated.")
    return questions

# Generate questions in chunks
def generate_questions_by_all_types(paragraph, total_questions):
    question_types_and_prompts = {
        "Factual": "You are an expert in extracting factual details from documents. Generate {num_questions} factual question-answer pairs based on the text: {paragraph}",
        "Procedural": "You are an expert in identifying procedures and steps. Generate {num_questions} procedural question-answer pairs from the given text: {paragraph}",
        "Conceptual": "You are a financial expert with corporate and conceptual knowledge. Generate {num_questions} conceptual questions from the input: {paragraph}",
        "Inferential": "Generate {num_questions} inferential question-answer pairs based on this input text: {paragraph}",
        "Reasoning-based": "Generate {num_questions} reasoning-based questions from this text: {paragraph}"
    }

    all_questions = []
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_questions, paragraph, question_type, questions_per_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            question_type = futures[future]
            try:
                questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                for qa in questions:
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": qa["question"],
                        "Answer": qa["answer"],
                        "Extracted From": qa["extracted"]
                    })
            except Exception as e:
                print(f"Error processing {question_type}: {e}")

    return all_questions[:total_questions]

# Process PDFs in directories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, output_folder="output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    pdf_text = extract_text_from_pdf(pdf_path)
                    image_data = extract_images_from_pdf(pdf_path)

                    message_content = [{"type": "text", "text": pdf_text}] + [
                        {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_data}"}
                        for img_data in image_data
                    ]

                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions)

                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")








import PyPDF2
import fitz  # PyMuPDF
import base64
import os
import math
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# PDF Text Extraction Function
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text += page_text.strip() + "\n"
    return text.strip()

# PDF Image Extraction Function
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []
    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    return encoded_images

# Create a valid PromptTemplate object
def create_prompt_template(prompt_text):
    return PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_text
    )

# Convert list of messages into a single paragraph string
def extract_text_from_messages(messages):
    text_content = " ".join([msg['text'] for msg in messages if msg['type'] == 'text'])
    return text_content.strip()

# Function to chunk large paragraphs if they exceed token limits
def chunk_text(paragraph, max_tokens=3000):
    words = paragraph.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) > max_tokens:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

# Function to safely parse the LLM response and handle empty/invalid cases
def extract_valid_json(response):
    if not response or response.strip() == "":
        print("LLM response is empty. Skipping JSON parsing.")
        return []
    
    try:
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"JSON parsing error: {e}. Response received: {response[:100]}...")
        return []

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_text):
    prompt_template = create_prompt_template(prompt_text)
    paragraph_text = extract_text_from_messages(paragraph)  # Convert messages to text

    chunks = chunk_text(paragraph_text)  # Split large inputs into smaller chunks
    all_questions = []

    for chunk in chunks:
        filled_prompt = prompt_template.format(paragraph=chunk, question_type=question_type, num_questions=num_questions)
        print(f"Generated prompt:\n{filled_prompt}")

        llm_chain = LLMChain(llm=llm, prompt=prompt_template)
        response = llm_chain.predict(paragraph=chunk, num_questions=str(num_questions), question_type=question_type)

        # Handle and debug the response
        if response is None or response.strip() == "":
            print(f"No response received for the chunk: {chunk[:100]}...")
            continue
        
        questions = extract_valid_json(response)

        if questions:
            all_questions.extend(questions)
        else:
            print(f"No valid questions generated for the chunk: {chunk[:100]}...")

    return all_questions

# Generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    all_questions = []
    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, min(chunk_size, total_questions - i), prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Main question generation logic
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": "You are an expert in extracting factual details from documents. Generate {num_questions} factual question-answer pairs based on the text: {paragraph}",
        "Procedural": "You are an expert in identifying procedures and steps. Generate {num_questions} procedural question-answer pairs from the given text: {paragraph}",
        "Conceptual": "You are a financial expert with corporate and conceptual knowledge. Generate {num_questions} conceptual questions from the input: {paragraph}",
        "Inferential": "Generate {num_questions} inferential question-answer pairs based on this input text: {paragraph}",
        "Reasoning-based": "Generate {num_questions} reasoning-based questions from this text: {paragraph}"
    }

    all_questions = []
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_questions, paragraph, question_type, questions_per_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            question_type = futures[future]
            try:
                questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                for qa in questions:
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": qa["question"],
                        "Answer": qa["answer"],
                        "Extracted From": qa["extracted"]
                    })
            except Exception as e:
                print(f"Error processing {question_type}: {e}")

    return all_questions[:total_questions]

# Process PDFs in directories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    pdf_text = extract_text_from_pdf(pdf_path)
                    image_data = extract_images_from_pdf(pdf_path)

                    message_content = [{"type": "text", "text": pdf_text}] + [
                        {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_data}"}
                        for img_data in image_data
                    ]

                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")








import PyPDF2
import fitz  # PyMuPDF
import base64
import os
import math
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# PDF Text Extraction Function
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text += page_text.strip() + "\n"
    return text.strip()

# PDF Image Extraction Function
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []
    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    return encoded_images

# Create a valid PromptTemplate object
def create_prompt_template(prompt_text):
    return PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_text
    )

# Convert list of messages into a single paragraph string
def extract_text_from_messages(messages):
    text_content = " ".join([msg['text'] for msg in messages if msg['type'] == 'text'])
    return text_content.strip()

# Function to chunk large paragraphs if they exceed token limits
def chunk_text(paragraph, max_tokens=3000):
    words = paragraph.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) > max_tokens:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

# Function to safely parse the LLM response and extract JSON
def extract_valid_json(response):
    try:
        # Ensure the response contains valid JSON
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"JSON parsing error: {e}")
        return []

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_text):
    prompt_template = create_prompt_template(prompt_text)
    paragraph_text = extract_text_from_messages(paragraph)  # Convert messages to text

    chunks = chunk_text(paragraph_text)  # Split large inputs into smaller chunks
    all_questions = []

    for chunk in chunks:
        filled_prompt = prompt_template.format(paragraph=chunk, question_type=question_type, num_questions=num_questions)
        print(f"Generated prompt:\n{filled_prompt}")

        llm_chain = LLMChain(llm=llm, prompt=prompt_template)
        response = llm_chain.predict(paragraph=chunk, num_questions=str(num_questions), question_type=question_type)
        questions = extract_valid_json(response)

        if questions:
            all_questions.extend(questions)
        else:
            print(f"No questions generated for chunk: {chunk[:100]}...")

    return all_questions

# Generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    all_questions = []
    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, min(chunk_size, total_questions - i), prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Main question generation logic
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": "You are an expert in extracting factual details from documents. Generate {num_questions} factual question-answer pairs based on the text: {paragraph}",
        "Procedural": "You are an expert in identifying procedures and steps. Generate {num_questions} procedural question-answer pairs from the given text: {paragraph}",
        "Conceptual": "You are a financial expert with corporate and conceptual knowledge. Generate {num_questions} conceptual questions from the input: {paragraph}",
        "Inferential": "Generate {num_questions} inferential question-answer pairs based on this input text: {paragraph}",
        "Reasoning-based": "Generate {num_questions} reasoning-based questions from this text: {paragraph}"
    }

    all_questions = []
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_questions, paragraph, question_type, questions_per_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            question_type = futures[future]
            try:
                questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                for qa in questions:
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": qa["question"],
                        "Answer": qa["answer"],
                        "Extracted From": qa["extracted"]
                    })
            except Exception as e:
                print(f"Error processing {question_type}: {e}")

    return all_questions[:total_questions]

# Process PDFs in directories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    pdf_text = extract_text_from_pdf(pdf_path)
                    image_data = extract_images_from_pdf(pdf_path)

                    message_content = [{"type": "text", "text": pdf_text}] + [
                        {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_data}"}
                        for img_data in image_data
                    ]

                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")





import PyPDF2
import fitz  # PyMuPDF
import base64
import os
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# PDF Text Extraction Function
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text += page_text.strip() + "\n"
    return text.strip()

# PDF Image Extraction Function
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []
    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    return encoded_images

# Create a valid PromptTemplate object
def create_prompt_template(prompt_text):
    return PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_text
    )

# Function to convert list of messages into a single string
def extract_text_from_messages(messages):
    text_content = " ".join([msg['text'] for msg in messages if msg['type'] == 'text'])
    return text_content.strip()

# Function to chunk text if it exceeds a token limit
def chunk_text(paragraph, max_tokens=3000):
    words = paragraph.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) > max_tokens:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_text):
    prompt_template = create_prompt_template(prompt_text)
    paragraph_text = extract_text_from_messages(paragraph)  # Convert messages to text

    chunks = chunk_text(paragraph_text)  # Split large inputs into smaller chunks
    all_questions = []

    for chunk in chunks:
        filled_prompt = prompt_template.format(paragraph=chunk, question_type=question_type, num_questions=num_questions)
        print(f"Generated prompt:\n{filled_prompt}")

        llm_chain = LLMChain(llm=llm, prompt=prompt_template)
        response = llm_chain.predict(paragraph=chunk, num_questions=str(num_questions), question_type=question_type)
        questions = extract_valid_json(response)
        all_questions.extend(questions)

    return all_questions

# Generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    all_questions = []
    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, min(chunk_size, total_questions - i), prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Main question generation logic
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": "You are an expert in extracting factual details from documents. Generate {num_questions} factual question-answer pairs based on the text: {paragraph}",
        "Procedural": "You are an expert in identifying procedures and steps. Generate {num_questions} procedural question-answer pairs from the given text: {paragraph}",
        "Conceptual": "You are a financial expert with corporate and conceptual knowledge. Generate {num_questions} conceptual questions from the input: {paragraph}",
        "Inferential": "Generate {num_questions} inferential question-answer pairs based on this input text: {paragraph}",
        "Reasoning-based": "Generate {num_questions} reasoning-based questions from this text: {paragraph}"
    }

    all_questions = []
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_questions, paragraph, question_type, questions_per_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            question_type = futures[future]
            try:
                questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                for qa in questions:
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": qa["question"],
                        "Answer": qa["answer"],
                        "Extracted From": qa["extracted"]
                    })
            except Exception as e:
                print(f"Error processing {question_type}: {e}")

    return all_questions[:total_questions]

# Process PDFs in directories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    pdf_text = extract_text_from_pdf(pdf_path)
                    image_data = extract_images_from_pdf(pdf_path)

                    message_content = [{"type": "text", "text": pdf_text}] + [
                        {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_data}"}
                        for img_data in image_data
                    ]

                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")











import PyPDF2
import fitz  # PyMuPDF
import base64
import os
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# PDF Text Extraction Function
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text += page_text.strip() + "\n"
    return text.strip()

# PDF Image Extraction Function
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []
    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    return encoded_images

# Create a valid PromptTemplate object
def create_prompt_template(prompt_text):
    return PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_text
    )

# Function to chunk text if it exceeds a token limit
def chunk_text(paragraph, max_tokens=3000):
    words = paragraph.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) > max_tokens:
            chunks.append(' '.join(current_chunk))
            current_chunk = []
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_text):
    prompt_template = create_prompt_template(prompt_text)
    chunks = chunk_text(paragraph)  # Split large inputs into smaller chunks

    all_questions = []

    for chunk in chunks:
        # Fill the prompt with the chunked content
        filled_prompt = prompt_template.format(paragraph=chunk, question_type=question_type, num_questions=num_questions)

        print(f"Generated prompt:\n{filled_prompt}")

        llm_chain = LLMChain(llm=llm, prompt=prompt_template)

        # Ensure structured input to the model
        response = llm_chain.predict(paragraph=chunk, num_questions=str(num_questions), question_type=question_type)
        questions = extract_valid_json(response)
        all_questions.extend(questions)

    return all_questions

# Generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    all_questions = []
    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, min(chunk_size, total_questions - i), prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Main question generation logic
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": "You are an expert in extracting factual details from documents. Generate {num_questions} factual question-answer pairs based on the text: {paragraph}",
        "Procedural": "You are an expert in identifying procedures and steps. Generate {num_questions} procedural question-answer pairs from the given text: {paragraph}",
        "Conceptual": "You are a financial expert with corporate and conceptual knowledge. Generate {num_questions} conceptual questions from the input: {paragraph}",
        "Inferential": "Generate {num_questions} inferential question-answer pairs based on this input text: {paragraph}",
        "Reasoning-based": "Generate {num_questions} reasoning-based questions from this text: {paragraph}"
    }

    all_questions = []
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_questions, paragraph, question_type, questions_per_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            question_type = futures[future]
            try:
                questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                for qa in questions:
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": qa["question"],
                        "Answer": qa["answer"],
                        "Extracted From": qa["extracted"]
                    })
            except Exception as e:
                print(f"Error processing {question_type}: {e}")

    return all_questions[:total_questions]

# Process PDFs in directories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    pdf_text = extract_text_from_pdf(pdf_path)
                    image_data = extract_images_from_pdf(pdf_path)

                    message_content = [{"type": "text", "text": pdf_text}] + [
                        {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_data}"}
                        for img_data in image_data
                    ]

                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")








import PyPDF2
import fitz  # PyMuPDF
import base64
import os
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# PDF Text Extraction Function
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text += page_text.strip() + "\n"
    return text.strip()

# PDF Image Extraction Function
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []
    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    return encoded_images

# Create a valid PromptTemplate object
def create_prompt_template(prompt_text):
    return PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_text
    )

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_text):
    prompt_template = create_prompt_template(prompt_text)
    # Fill the prompt with the required inputs
    filled_prompt = prompt_template.format(paragraph=paragraph, question_type=question_type, num_questions=num_questions)

    print(f"Generated prompt:\n{filled_prompt}")

    llm_chain = LLMChain(llm=llm, prompt=prompt_template)

    # Pass the structured input to the LLMChain
    response = llm_chain.predict(paragraph=paragraph, num_questions=str(num_questions), question_type=question_type)
    return extract_valid_json(response)

# Generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    all_questions = []
    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, min(chunk_size, total_questions - i), prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Main question generation logic
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": "You are an expert in extracting factual details from documents. Generate {num_questions} factual question-answer pairs based on the text: {paragraph}",
        "Procedural": "You are an expert in identifying procedures and steps. Generate {num_questions} procedural question-answer pairs from the given text: {paragraph}",
        "Conceptual": "You are a financial expert with corporate and conceptual knowledge. Generate {num_questions} conceptual questions from the input: {paragraph}",
        "Inferential": "Generate {num_questions} inferential question-answer pairs based on this input text: {paragraph}",
        "Reasoning-based": "Generate {num_questions} reasoning-based questions from this text: {paragraph}"
    }

    all_questions = []
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_questions, paragraph, question_type, questions_per_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            question_type = futures[future]
            try:
                questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                for qa in questions:
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": qa["question"],
                        "Answer": qa["answer"],
                        "Extracted From": qa["extracted"]
                    })
            except Exception as e:
                print(f"Error processing {question_type}: {e}")

    return all_questions[:total_questions]

# Process PDFs in directories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    pdf_text = extract_text_from_pdf(pdf_path)
                    image_data = extract_images_from_pdf(pdf_path)

                    message_content = [{"type": "text", "text": pdf_text}] + [
                        {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_data}"}
                        for img_data in image_data
                    ]

                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")











import PyPDF2
import fitz  # PyMuPDF
import base64
import os
import time
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# PDF Text Extraction Function
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text += page_text.strip() + "\n"
    return text.strip()

# PDF Image Extraction Function
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []
    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    return encoded_images

# Retry logic to handle rate limits
def retry_with_exponential_backoff(func, *args, max_retries=5):
    delay = 1
    for attempt in range(max_retries):
        try:
            return func(*args)
        except Exception as e:
            if "429" in str(e):
                print(f"Rate limit hit. Retrying after {delay} seconds...")
                time.sleep(delay)
                delay *= 2
            else:
                print(f"Error: {e}")
                break
    raise RuntimeError("Max retries exceeded.")

# Create a valid PromptTemplate object
def create_prompt_template(prompt_text):
    return PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_text
    )

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_text):
    prompt_template = create_prompt_template(prompt_text)
    fill_prompt = prompt_template.format(paragraph=paragraph, question_type=question_type, num_questions=num_questions)

    print(f"Generated prompt:\n{fill_prompt}")

    llm_chain = LLMChain(llm=llm, prompt=prompt_template)

    # Pass the message content correctly
    response = retry_with_exponential_backoff(
        llm_chain.predict,
        {"paragraph": paragraph, "num_questions": str(num_questions), "question_type": question_type}
    )
    return extract_valid_json(response)

# Generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    all_questions = []
    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, min(chunk_size, total_questions - i), prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Main question generation logic
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": "You are an expert in extracting factual details from documents. Generate {num_questions} factual question-answer pairs based on the text: {paragraph}",
        "Procedural": "You are an expert in identifying procedures and steps. Generate {num_questions} procedural question-answer pairs from the given text: {paragraph}",
        "Conceptual": "You are a financial expert with corporate and conceptual knowledge. Generate {num_questions} conceptual questions from the input: {paragraph}",
        "Inferential": "Generate {num_questions} inferential question-answer pairs based on this input text: {paragraph}",
        "Reasoning-based": "Generate {num_questions} reasoning-based questions from this text: {paragraph}"
    }

    all_questions = []
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_questions, paragraph, question_type, questions_per_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            question_type = futures[future]
            try:
                questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                for qa in questions:
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": qa["question"],
                        "Answer": qa["answer"],
                        "Extracted From": qa["extracted"]
                    })
            except Exception as e:
                print(f"Error processing {question_type}: {e}")

    return all_questions[:total_questions]

# Process PDFs in directories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    pdf_text = extract_text_from_pdf(pdf_path)
                    image_data = extract_images_from_pdf(pdf_path)

                    message_content = [{"type": "text", "text": pdf_text}] + [
                        {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_data}"}
                        for img_data in image_data
                    ]

                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")






import PyPDF2
import fitz  # PyMuPDF
import base64
import os
import time
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate  # Ensure this is installed and imported
from langchain.chains import LLMChain  # Import LLMChain from LangChain

# PDF Text Extraction Function
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text += page_text.strip() + "\n"
    return text.strip()

# PDF Image Extraction Function
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []
    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            # Convert image data to base64 and store it
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    return encoded_images

# Retry logic to handle rate limits
def retry_with_exponential_backoff(func, *args, max_retries=5):
    delay = 1
    for attempt in range(max_retries):
        try:
            return func(*args)
        except Exception as e:
            if "429" in str(e):  # Handle rate limit
                print(f"Rate limit hit. Retrying after {delay} seconds...")
                time.sleep(delay)
                delay *= 2  # Exponential backoff
            else:
                print(f"Error: {e}")
                break
    raise RuntimeError("Max retries exceeded.")

# Create a valid PromptTemplate object
def create_prompt_template(prompt_text):
    return PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_text
    )

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_text):
    # Create a valid PromptTemplate object
    prompt_template = create_prompt_template(prompt_text)
    
    # Fill the prompt using the template
    fill_prompt = prompt_template.format(paragraph=paragraph, question_type=question_type, num_questions=num_questions)
    print(f"Generated prompt:\n{fill_prompt}")

    # Assuming LLMChain or equivalent is configured correctly
    llm_chain = LLMChain(llm=llm, prompt=prompt_template)

    # Retry logic to handle rate-limiting and errors
    response = retry_with_exponential_backoff(llm_chain.predict, fill_prompt)  # Fix applied here
    return extract_valid_json(response)

# Generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    all_questions = []
    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, min(chunk_size, total_questions - i), prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Main question generation logic
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": "You are an expert in extracting factual details from documents. Generate {num_questions} factual question-answer pairs based on the text: {paragraph}",
        "Procedural": "You are an expert in identifying procedures and steps. Generate {num_questions} procedural question-answer pairs from the given text: {paragraph}",
        "Conceptual": "You are a financial expert with corporate and conceptual knowledge. Generate {num_questions} conceptual questions from the input: {paragraph}",
        "Inferential": "Generate {num_questions} inferential question-answer pairs based on this input text: {paragraph}",
        "Reasoning-based": "Generate {num_questions} reasoning-based questions from this text: {paragraph}"
    }

    all_questions = []
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_questions, paragraph, question_type, questions_per_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            question_type = futures[future]
            try:
                questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                for qa in questions:
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": qa["question"],
                        "Answer": qa["answer"],
                        "Extracted From": qa["extracted"]
                    })
            except Exception as e:
                print(f"Error processing {question_type}: {e}")

    return all_questions[:total_questions]

# Process PDFs in directories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    # Extract text and images
                    pdf_text = extract_text_from_pdf(pdf_path)
                    image_data = extract_images_from_pdf(pdf_path)

                    message_content = [{"type": "text", "text": pdf_text}] + [
                        {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_data}"}
                        for img_data in image_data
                    ]

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    # Save results to Excel (assuming save_to_excel function)
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")







import PyPDF2
import fitz  # PyMuPDF
import base64
import os
import time
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate  # Ensure this is installed and imported

# PDF Text Extraction Function
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text += page_text.strip() + "\n"
    return text.strip()

# PDF Image Extraction Function
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []
    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            # Convert image data to base64 and store it
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    return encoded_images

# Retry logic to handle rate limits
def retry_with_exponential_backoff(func, *args, max_retries=5):
    delay = 1
    for attempt in range(max_retries):
        try:
            return func(*args)
        except Exception as e:
            if "429" in str(e):  # Handle rate limit
                print(f"Rate limit hit. Retrying after {delay} seconds...")
                time.sleep(delay)
                delay *= 2  # Exponential backoff
            else:
                print(f"Error: {e}")
                break
    raise RuntimeError("Max retries exceeded.")

# Create a valid PromptTemplate object
def create_prompt_template(prompt_text):
    return PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_text
    )

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_text):
    # Create a valid PromptTemplate object
    prompt_template = create_prompt_template(prompt_text)
    
    # Fill the prompt using the template
    fill_prompt = prompt_template.format(paragraph=paragraph, question_type=question_type, num_questions=num_questions)
    print(f"Generated prompt:\n{fill_prompt}")

    # Assuming LLMChain or equivalent is configured correctly
    llm_chain = LLMChain(llm=llm, prompt=prompt_template)

    # Retry logic to handle rate-limiting and errors
    response = retry_with_exponential_backoff(llm_chain.predict, paragraph, str(num_questions), question_type)
    return extract_valid_json(response)

# Generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    all_questions = []
    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, min(chunk_size, total_questions - i), prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Main question generation logic
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": "You are an expert in extracting factual details from documents. Generate {num_questions} factual question-answer pairs based on the text: {paragraph}",
        "Procedural": "You are an expert in identifying procedures and steps. Generate {num_questions} procedural question-answer pairs from the given text: {paragraph}",
        "Conceptual": "You are a financial expert with corporate and conceptual knowledge. Generate {num_questions} conceptual questions from the input: {paragraph}",
        "Inferential": "Generate {num_questions} inferential question-answer pairs based on this input text: {paragraph}",
        "Reasoning-based": "Generate {num_questions} reasoning-based questions from this text: {paragraph}"
    }

    all_questions = []
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_questions, paragraph, question_type, questions_per_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            question_type = futures[future]
            try:
                questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                for qa in questions:
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": qa["question"],
                        "Answer": qa["answer"],
                        "Extracted From": qa["extracted"]
                    })
            except Exception as e:
                print(f"Error processing {question_type}: {e}")

    return all_questions[:total_questions]

# Process PDFs in directories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    # Extract text and images
                    pdf_text = extract_text_from_pdf(pdf_path)
                    image_data = extract_images_from_pdf(pdf_path)

                    message_content = [{"type": "text", "text": pdf_text}] + [
                        {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_data}"}
                        for img_data in image_data
                    ]

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    # Save results to Excel (assuming save_to_excel function)
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")







import PyPDF2
import fitz  # PyMuPDF
import base64
import os
import time
import math
from concurrent.futures import ThreadPoolExecutor, as_completed

# PDF Text Extraction Function
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text += page_text.strip() + "\n"
    return text.strip()

# PDF Image Extraction Function
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []
    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            # Convert image data to base64 and store it
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))
    return encoded_images

# Retry logic to handle rate limits
def retry_with_exponential_backoff(func, *args, max_retries=5):
    delay = 1
    for attempt in range(max_retries):
        try:
            return func(*args)
        except Exception as e:
            if "429" in str(e):  # Handle rate limit
                print(f"Rate limit hit. Retrying after {delay} seconds...")
                time.sleep(delay)
                delay *= 2  # Exponential backoff
            else:
                print(f"Error: {e}")
                break
    raise RuntimeError("Max retries exceeded.")

# Function to generate questions for all types
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    fill_prompt = prompt_template.format(paragraph=paragraph, question_type=question_type, num_questions=num_questions)
    print(f"Generated prompt:\n{fill_prompt}")

    # Assuming LLMChain or equivalent call here
    llm_chain = LLMChain(llm=llm, prompt=fill_prompt)
    response = retry_with_exponential_backoff(llm_chain.predict, paragraph, str(num_questions), question_type)
    return extract_valid_json(response)

def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    all_questions = []
    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, min(chunk_size, total_questions - i), prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Main question generation logic
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": "FACTUAL PROMPT",
        "Procedural": "PROCEDURAL PROMPT",
        "Conceptual": "CONCEPTUAL PROMPT",
        "Inferential": "INFERENTIAL PROMPT",
        "Reasoning-based": "REASONING PROMPT"
    }

    all_questions = []
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(generate_questions_in_chunks, paragraph, question_type, questions_per_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(futures):
            question_type = futures[future]
            try:
                questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                for qa in questions:
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": qa["question"],
                        "Answer": qa["answer"],
                        "Extracted From": qa["extracted"]
                    })
            except Exception as e:
                print(f"Error processing {question_type}: {e}")

    return all_questions[:total_questions]

def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    # Extract text and images
                    pdf_text = extract_text_from_pdf(pdf_path)
                    image_data = extract_images_from_pdf(pdf_path)

                    message_content = [{"type": "text", "text": pdf_text}] + [
                        {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_data}"}
                        for img_data in image_data
                    ]

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(message_content, total_questions, chunk_size)

                    # Save results to Excel (assuming save_to_excel function)
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")









import pandas as pd
import PyPDF2
import base64
import fitz  # PyMuPDF for image extraction
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage
import json

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() or ""  # Handle pages with None
    return text.strip()

# Function to extract images from a PDF and encode them in base64
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []

    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            # Convert image data to base64 and store it
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))

    return encoded_images

# Function to generate questions and answers using Azure OpenAI
def generate_questions_and_answers(text: str, prompt: str, azure_api_key: str, azure_endpoint: str, deployment_name: str) -> list:
    azure_openai = AzureChatOpenAI(
        temperature=0.7,
        api_version="2023-06-01-preview",
        azure_openai_api_key=azure_api_key,
        azure_openai_base_url=azure_endpoint,
        deployment_name=deployment_name
    )

    # Format the prompt
    formatted_prompt = prompt.format(text=text)

    # Send the request to Azure OpenAI
    response = azure_openai([HumanMessage(content=formatted_prompt)])

    try:
        # Parse the response as JSON
        q_and_a_list = json.loads(response.content.strip())
        return q_and_a_list
    except Exception as e:
        print("Error parsing Azure OpenAI response:", e)
        return []

# Function to save questions and answers to an Excel file
def save_questions_and_answers_to_excel(q_and_a_list, output_file: str):
    # Create a DataFrame
    df = pd.DataFrame(q_and_a_list)
    # Save to Excel
    df.to_excel(output_file, index=False)

# Function to handle the full extraction and saving process
def extract_and_save_qa(pdf_path: str, prompt: str, output_file: str, azure_api_key: str, azure_endpoint: str, deployment_name: str):
    # Step 1: Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)

    # Step 2: Generate questions and answers
    if pdf_text:
        questions_and_answers = generate_questions_and_answers(pdf_text, prompt, azure_api_key, azure_endpoint, deployment_name)

        # Step 3: Save the questions and answers to an Excel file
        if questions_and_answers:
            save_questions_and_answers_to_excel(questions_and_answers, output_file)
            print(f"Questions and answers saved to {output_file}.")
        else:
            print("No questions and answers were generated.")
    else:
        print("The PDF did not contain any extractable text.")


from qa_extractor import extract_and_save_qa

# User-provided inputs
pdf_path = "path/to/your/pdf_file.pdf"  # Replace with your actual PDF path
output_file = "questions_and_answers.xlsx"

prompt = """
Based on the following text, generate 5 questions and their corresponding answers:

{text}

Format the output as a list of dictionaries, each containing:
- "question": The question text
- "answer": The answer text
"""

# Azure OpenAI API configuration
azure_api_key = "YOUR_AZURE_OPENAI_API_KEY"  # Replace with your API key
azure_endpoint = "YOUR_AZURE_ENDPOINT"  # Replace with your Azure OpenAI endpoint
deployment_name = "ssgpt-40"  # Replace with your deployment name if different

# Run the extraction and save process
extract_and_save_qa(pdf_path, prompt, output_file, azure_api_key, azure_endpoint, deployment_name)










import pandas as pd
import PyPDF2
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage
import json

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() or ""  # Handle pages with None
    return text.strip()

# Function to generate questions and answers using Azure OpenAI
def generate_questions_and_answers(text: str, prompt: str, num_questions: int, azure_api_key: str, azure_endpoint: str, deployment_name: str) -> list:
    azure_openai = AzureChatOpenAI(
        temperature=0.7,
        api_version="2023-06-01-preview",
        azure_openai_api_key=azure_api_key,
        azure_openai_base_url=azure_endpoint,
        deployment_name=deployment_name
    )

    # Format the prompt with the text and number of questions
    formatted_prompt = prompt.format(text=text, num_questions=num_questions)

    # Send the request to Azure OpenAI
    response = azure_openai([HumanMessage(content=formatted_prompt)])

    try:
        # Parse as JSON
        q_and_a_list = json.loads(response.content.strip())
        return q_and_a_list
    except Exception as e:
        print("Error parsing Azure OpenAI response:", e)
        return []

# Function to save questions and answers to an Excel file
def save_questions_and_answers_to_excel(q_and_a_list, output_file: str):
    # Create a DataFrame
    df = pd.DataFrame(q_and_a_list)
    # Save to Excel
    df.to_excel(output_file, index=False)

# User inputs
pdf_path = "path/to/your/pdf_file.pdf"  # Replace with your actual PDF path
output_file = "questions_and_answers.xlsx"

prompt = """
Based on the following text, generate {num_questions} questions and their corresponding answers:

{text}

Format the output as a list of dictionaries, each containing:
- "question": The question text
- "answer": The answer text
"""

# Azure OpenAI API configuration (replace with your actual details)
azure_api_key = "YOUR_AZURE_OPENAI_API_KEY"  # Replace with your API key
azure_endpoint = "YOUR_AZURE_ENDPOINT"  # Replace with your Azure OpenAI endpoint
deployment_name = "ssgpt-40"  # Replace with your deployment name if different
num_questions = 5  # Number of questions to generate

# Step 1: Extract text from the PDF
pdf_text = extract_text_from_pdf(pdf_path)

# Step 2: Generate questions and answers
if pdf_text:
    questions_and_answers = generate_questions_and_answers(pdf_text, prompt, num_questions, azure_api_key, azure_endpoint, deployment_name)

    # Step 3: Save the questions and answers to an Excel file
    if questions_and_answers:
        save_questions_and_answers_to_excel(questions_and_answers, output_file)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No questions and answers were generated.")
else:
    print("The PDF did not contain any extractable text.")







pdf_path = "path/to/your/pdf_file.pdf"

# Define the prompt template and instructions
prompt_template = """
Please extract text and provide it in the specified format.
Ignore: {listIgnore}
Priority Instructions: {priority_instructions}
Format Instructions: {format_instructions}
"""

instructions = "Extract key points, maintain structure, and include images if available."

# Define the prompt for generating questions and answers
prompt = """
Based on the following text, generate {num_questions} questions and their corresponding answers:

{text}

Format the output as a list of dictionaries, each containing:
- "question": The question text
- "answer": The answer text
"""

# Set your Azure OpenAI API details
azure_api_key = "YOUR_AZURE_OPENAI_API_KEY"  # Replace with your API key
azure_endpoint = "YOUR_AZURE_ENDPOINT"  # Replace with your Azure OpenAI endpoint
deployment_name = "ssgpt-40"  # Replace with your deployment name if different

# Specify the number of questions you want to generate
num_questions = 5  # You can change this number as needed

# Generate the message content
message = create_message_with_qa(pdf_path, prompt_template, instructions, prompt, num_questions, azure_api_key, azure_endpoint, deployment_name)

# Output the content
for item in message:
    print(item)








import PyPDF2
import base64
import fitz  # PyMuPDF for image extraction
from langchain.chat_models import AzureChatOpenAI

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() or ""  # Handle pages with None
    return text

# Function to encode images from a PDF
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []

    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            # Convert image data to base64 and store it
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))

    return encoded_images

# Function to generate questions and answers using Azure OpenAI
def generate_questions_and_answers(text: str, prompt: str, num_questions: int, azure_api_key: str, azure_endpoint: str, deployment_name: str) -> list:
    azure_openai = AzureChatOpenAI(
        temperature=0.7,
        api_version="2023-06-01-preview",
        azure_openai_api_key=azure_api_key,  # Pass API key directly
        azure_openai_base_url=azure_endpoint,  # Pass Azure endpoint directly
        deployment_name=deployment_name
    )

    # Format the prompt with the text and number of questions
    formatted_prompt = prompt.format(text=text, num_questions=num_questions)

    # Properly formatted input for AzureChatOpenAI
    response = azure_openai([{"role": "user", "content": formatted_prompt}])

    try:
        # Parse the response and return as a list
        q_and_a_list = eval(response.content.strip())
        return q_and_a_list
    except Exception as e:
        print("Error parsing Azure OpenAI response:", e)
        return []

# Function to generate a message with text, images, and questions/answers
def create_message_with_qa(pdf_path: str, prompt_template: str, instructions: str, prompt: str, num_questions: int, azure_api_key: str, azure_endpoint: str, deployment_name: str) -> list:
    # Step 1: Extract text from PDF
    extracted_text = extract_text_from_pdf(pdf_path)
    # Step 2: Extract and encode images
    encoded_images = extract_images_from_pdf(pdf_path)
    # Step 3: Generate questions and answers from the text using the provided prompt and number of questions
    questions_and_answers = generate_questions_and_answers(extracted_text, prompt, num_questions, azure_api_key, azure_endpoint, deployment_name)

    # Step 4: Format the message
    message_content = [
        {
            "type": "text",
            "text": prompt_template.format(
                listIgnore=["Line number", "MSH Item", "SKU number"],
                priority_instructions=instructions,
                format_instructions="Extract in JSON format with sections and subsections."
            ) + "\n\nExtracted Text:\n" + extracted_text
        }
    ] + [
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{img_data}"}
        } for img_data in encoded_images
    ] + [
        {
            "type": "qa",
            "question": qa["question"],
            "answer": qa["answer"]
        } for qa in questions_and_answers
    ]

    return message_content










def generate_questions_and_answers(text: str) -> list:
    azure_openai = AzureChatOpenAI(
        temperature=0.7,
        api_version="2023-06-01-preview",  # Replace with your Azure API version if needed
        azure_openai_api_key="YOUR_AZURE_OPENAI_API_KEY",  # Replace with your Azure API key
        azure_openai_base_url="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    # Construct the prompt for generating Q&A
    prompt = f"""
    Based on the following text, generate 5 questions and their corresponding answers:

    {text}

    Format the output as a list of dictionaries, each containing:
    - "question": The question text
    - "answer": The answer text
    """

    # Properly formatted input for AzureChatOpenAI
    response = azure_openai([{"role": "user", "content": prompt}])

    try:
        # Parse the response and return as a list
        q_and_a_list = eval(response.content.strip())
        return q_and_a_list
    except Exception as e:
        print("Error parsing Azure OpenAI response:", e)
        return []











import PyPDF2
import base64
import fitz  # PyMuPDF for image extraction
from langchain.chat_models import AzureChatOpenAI

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() or ""  # Ensure to handle pages with None
    return text

# Function to encode images from a PDF
def extract_images_from_pdf(pdf_path: str):
    doc = fitz.open(pdf_path)
    encoded_images = []

    for page_number in range(len(doc)):
        for img_index, img in enumerate(doc[page_number].get_images(full=True)):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            # Convert image data to base64 and store it
            encoded_images.append(base64.b64encode(image_data).decode('utf-8'))

    return encoded_images

# Function to generate questions and answers using AzureChatOpenAI
def generate_questions_and_answers(text: str) -> list:
    azure_openai = AzureChatOpenAI(
        temperature=0.7,
        api_version="2023-06-01-preview",  # Replace with your Azure API version if needed
        azure_openai_api_key="YOUR_AZURE_OPENAI_API_KEY",  # Replace with your Azure API key
        azure_openai_base_url="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    # Construct the prompt for generating Q&A
    prompt = f"""
    Based on the following text, generate 5 questions and their corresponding answers:

    {text}

    Format the output as a list of dictionaries, each containing:
    - "question": The question text
    - "answer": The answer text
    """

    response = azure_openai.predict_messages([{"role": "user", "content": prompt}])
    
    try:
        # Parse the response and return as a list
        q_and_a_list = eval(response.content.strip())
        return q_and_a_list
    except Exception as e:
        print("Error parsing Azure OpenAI response:", e)
        return []

# Function to generate a message with text, images, and questions/answers
def create_message_with_qa(pdf_path: str, prompt_template: str, instructions: str) -> list:
    # Step 1: Extract text from PDF
    extracted_text = extract_text_from_pdf(pdf_path)
    # Step 2: Extract and encode images
    encoded_images = extract_images_from_pdf(pdf_path)
    # Step 3: Generate questions and answers from the text
    questions_and_answers = generate_questions_and_answers(extracted_text)

    # Step 4: Format the message
    message_content = [
        {
            "type": "text",
            "text": prompt_template.format(
                listIgnore=["Line number", "MSH Item", "SKU number"],
                priority_instructions=instructions,
                format_instructions="Extract in JSON format with sections and subsections."
            ) + "\n\nExtracted Text:\n" + extracted_text
        }
    ] + [
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{img_data}"}
        } for img_data in encoded_images
    ] + [
        {
            "type": "qa",
            "question": qa["question"],
            "answer": qa["answer"]
        } for qa in questions_and_answers
    ]

    return message_content

# Example usage of the functions
if __name__ == "__main__":
    pdf_path = "path/to/your/pdf_file.pdf"

    # Define the prompt template and instructions
    prompt_template = """
    Please extract text and provide it in the specified format.
    Ignore: {listIgnore}
    Priority Instructions: {priority_instructions}
    Format Instructions: {format_instructions}
    """

    instructions = "Extract key points, maintain structure, and include images if available."

    # Generate the message content
    message = create_message_with_qa(pdf_path, prompt_template, instructions)

    # Output the content
    for item in message:
        print(item)









import json

def merge_text_and_images(text_file, image_file):
    # Load the text data from JSON without worrying about metadata
    with open(text_file, 'r', encoding='utf-8') as file1:
        text_data = json.load(file1)

    # Load the image data from JSON
    with open(image_file, 'r', encoding='utf-8') as file2:
        image_data = json.load(file2)

    # Create a mapping of page_number to its image descriptions
    image_map = {}
    for image in image_data:
        page_num = image["page_number"]
        if page_num not in image_map:
            image_map[page_num] = []
        image_map[page_num].append(image["image_description"])

    # Append image descriptions as additional keys
    final_pages = []
    for page in text_data["pages"]:
        page_num = page["page_number"]
        page_flattened = {
            "page_number": page_num,
            "text": page["text"]
        }

        # Add image descriptions
        if page_num in image_map:
            for i, img_desc in enumerate(image_map[page_num]):
                page_flattened[f"image_description_{i + 1}"] = img_desc
        else:
            page_flattened["image_description_1"] = "No image descriptions available"

        final_pages.append(page_flattened)

    # Return the final pages section only
    return {"pages": final_pages}

# Example Usage
result = merge_text_and_images('text.json', 'image_data.json')
print(json.dumps(result, indent=4))






import json

def merge_text_and_images(text_file, image_file):
    # Load the text data from JSON with encoding to avoid decoding errors
    with open(text_file, 'r', encoding='utf-8') as file1:
        text_data = json.load(file1)

    # Load the image data from JSON with encoding
    with open(image_file, 'r', encoding='utf-8') as file2:
        image_data = json.load(file2)

    # Create a mapping of page_number to its image descriptions
    image_map = {}
    for image in image_data:
        page_num = image["page_number"]
        if page_num not in image_map:
            image_map[page_num] = []
        image_map[page_num].append(image["image_description"])

    # Append image descriptions as additional keys
    for page in text_data["pages"]:
        page_num = page["page_number"]
        if page_num in image_map:
            page["image_description"] = image_map[page_num]
        else:
            page["image_description"] = ["No image descriptions available"]

    # Flattening image descriptions into individual keys
    final_pages = []
    for page in text_data["pages"]:
        page_flattened = {
            "page_number": page["page_number"],
            "text": page["text"]
        }
        for i, img_desc in enumerate(page["image_description"]):
            page_flattened[f"image_description_{i + 1}"] = img_desc
        final_pages.append(page_flattened)

    # Replace the original pages with the final flattened version
    text_data["pages"] = final_pages

    # Return the merged result
    return text_data

# Example Usage
result = merge_text_and_images('text.json', 'image_data.json')
print(json.dumps(result, indent=4))









import json

def merge_text_and_images(text_file, image_file):
    # Load the text data from JSON
    with open(text_file, 'r') as file1:
        text_data = json.load(file1)

    # Load the image data from JSON
    with open(image_file, 'r') as file2:
        image_data = json.load(file2)

    # Create a mapping of page_number to its image descriptions
    image_map = {}
    for image in image_data:
        page_num = image["page_number"]
        if page_num not in image_map:
            image_map[page_num] = []
        image_map[page_num].append(image["image_description"])

    # Append image descriptions as additional keys
    for page in text_data["pages"]:
        page_num = page["page_number"]
        if page_num in image_map:
            page["image_description"] = image_map[page_num]
        else:
            page["image_description"] = ["No image descriptions available"]

    # Flattening image descriptions into individual keys
    final_pages = []
    for page in text_data["pages"]:
        page_flattened = {
            "page_number": page["page_number"],
            "text": page["text"]
        }
        for i, img_desc in enumerate(page["image_description"]):
            page_flattened[f"image_description_{i + 1}"] = img_desc
        final_pages.append(page_flattened)

    # Replace the original pages with the final flattened version
    text_data["pages"] = final_pages

    # Return the merged result
    return text_data

# Example Usage
result = merge_text_and_images('text.json', 'image_data.json')
print(json.dumps(result, indent=4))












Chat history
Open sidebar

ChatGPT 4o

You said:


## Understanding Index Points:

1. index index its represent the page number

2. text coresponding to represnt the text

3. image_description to reresent the index of page number and its corresponding image description
ChatGPT said:
ChatGPT
It seems like you are mapping or defining index points and their representation in the context of a document or a structured dataset. Here's a refined understanding based on your outline:

index: Represents the page number or sequential index in the document, making it a key reference point for navigation or retrieval.

text: Corresponds to the text present on the indexed page. This could include paragraphs, headings, or other important textual content.

image_description: Represents the description of the image located on the indexed page. This helps in associating visual content with the correct page and provides context for analysis.

Would you like help creating a structured table or data model around this idea?











Search


ChatGPT can make mistakes. Check important info.
?












## Objective:
You will be given a list of pages containing **combined text and image descriptions**. Your task is to analyze both sources of content to generate step-by-step Markdown-style answers for each procedural question while summarizing the understanding of each page through **understanding index points**.

---

## Understanding Index Points:

1. **Page {Page Number} - Text Content:**  
   Summarize key details from the text such as instructions, sections, and references to procedures, data, or processes.

2. **Page {Page Number} - Image Content:**  
   Provide details of the images on the page, including the **image name** and **image description**. Summarize any key data or information depicted in the image that supports the text or provides additional steps or validation.

3. **Cross-Reference:**  
   Explain how the text and image descriptions connect to provide a **complete understanding of the procedural steps**. If image descriptions contain important additional steps or validations, ensure they are incorporated into the relevant questions and answers.

---

## Pages:  
{pages}

---

## Steps:

1. Identify the relevant pages that help answer each procedural question.  
2. Generate **understanding index points** for each page by summarizing key textual and image details.  
3. Create a well-structured, step-by-step Markdown-style output that integrates both text and image descriptions.  
4. If any additional steps or validations are mentioned in the image descriptions, ensure they are included in the answers.

---

## Output Format:

```markdown
# Question: {question_text}

## Understanding Index Points:
1. **Page {Page Number} - Text Content:**  
   {Summarize the key details from the page’s text.}  

2. **Page {Page Number} - Image Content:**  
   - **Image Name:** {image_name}  
   - **Image Description:** {image_description}  
   {Summarize any important data, tables, or details from the image.}  

3. **Cross-Reference:**  
   {Explain how the text and image descriptions work together to provide a complete understanding of the task.}

## Step-by-Step Answer:
1. **Step 1:** {Clearly describe the first step using both text and image details.}  
2. **Step 2:** {Include any additional steps or validations from the image description, if applicable.}  
3. **Step 3:** {Continue with sequential instructions based on the input.}

## Relevant Pages Referenced:
- {List the relevant pages and sections}

## Key Tables, Calculations, or Notes:
- {Include any key tables or calculations if mentioned in the input}








## Understanding Index Points:
1. **Page {Page Number} - Text Content:** {Summarize key details from the text such as instructions, sections, or processes.}
2. **Page {Page Number} - Image Content:** {Summarize key details from the image descriptions, including tables or financial data.}
3. **Cross-Reference:** Explain how the text and image details connect to form a complete understanding of the task.




prompt = """
## Objective:
You will be given a list of questions and corresponding paragraphs containing combined textual and image content. Your task is to analyze the combined input and generate a step-by-step Markdown-style procedural answer for each question, ensuring all relevant details from both text and image descriptions are included.

## Questions:
{questions}

## Paragraphs:
The paragraphs provided may have numbers followed by spaces (e.g., "1 Section 8.6"). Ensure you correctly reference these sections and paragraphs without omitting key details.

{paragraphs}

---

## Steps:
1. For each question, identify the relevant paragraphs containing information needed to answer it, including those with numbered sections (e.g., "1 Section 8.6").
2. Use both the text and image descriptions to create a well-structured, step-by-step Markdown-style output.
3. Ensure key references to columns, tables, calculations, and section titles are preserved exactly as they appear in the input.
4. Integrate any tables, data fields, or notes from the image descriptions directly within the relevant steps.
5. If image descriptions provide essential details related to steps (e.g., adjustments, validations), include them in the output explicitly.
6. Avoid redundant information and ensure the steps are actionable.

---

## Output Format:
```markdown
# Question: {question_text}

## Step-by-Step Answer:
1. **Step 1:** {Clearly describe the first step, including referenced sections and key details from the text or image descriptions.}
2. **Step 2:** {Describe the next action or calculation required, integrating any relevant notes or tables from the image description.}
3. **Step 3:** {Continue the steps sequentially, ensuring logical progression. Include tables or field names when necessary.}

## Relevant Paragraphs Referenced:
- {List the paragraphs or sections that contributed to the steps}

## Key Tables, Calculations, or Notes:
- {Include any tables or formulas from the input when applicable}












You are a financial expert with expertise in corporate finance, investment strategies, and accounting.

Your task is to generate up to {num_questions} distinct procedural question-answer pairs by analyzing the combined input, which includes both textual content and image descriptions.

Ensure:
- Steps must be precise, detailed, and formatted correctly (e.g., column names, tables, and field names preserved).
- Include relevant information from both text and image descriptions.
- If image descriptions provide supporting tables, fields, or explanations, integrate them directly into the corresponding steps.

Input:
Combined content (from text and image description):
{paragraph}

Requirements:
1. Ensure each step is clearly defined and logically flows from the previous one.
2. If image descriptions contain critical values (e.g., field names, table data, or formulas), include them directly in the appropriate step.
3. Preserve key references like column names (e.g., Col E), table labels, and formulas exactly as stated in the input.
4. Ensure the output is formatted with numbered steps and explanations (no redundant or missing information).
5. Return a well-formatted response that captures both the procedural logic and any image-related clarifications.

Output format:
[
    {{
        "question": "What are the steps to complete the financial reconciliation process?",
        "answer": [
            "Step 1: Copy the NRM SIT RTS recon file of the previous month, rename it, and save it in the applicable working folder for the current month.",
            "Step 2: Update the date in the report to reflect the required period.",
            "Step 3: Ensure the formulas in Column E are correct as shown below.\n\nColumn E: MCH YTD realised G/L = 12,175,017.91\nColumn N: AAS25 YTD realised G/L = -1,067,845.97\nFormula: D35 + D34 - D36",
            "Step 4: Analyze and validate the following adjustments from the image description:\n\n- Column J (SEC G/L) = 11,818,266.28\n- Column N (AAS 25 adjustment-ccy) = 0\n- Column H (MCH Unrealised G/L) = -251,439,912.92\n- Columns E + H (MCH Total) = -239,264,895.01",
            "Step 5: Perform reasonability checks and validate cusips with AAS25 adjustments.\nExample: 'AAS25 adjustment-sec = -13,242,863.88'",
            "Step 6: Insert Part1 and Part2 tables from the SIT Rec file and ensure no differences exist between the totals and DMH data."
        ],
        "extracted": "text, image_description, or both"
    }}
]

Guidelines:
- Ensure that each step integrates all relevant details from text and images.
- When image descriptions provide additional data or tables, they must be referenced directly within the corresponding steps.
- Ensure the response is a valid JSON array.










You are a financial expert with expertise in corporate finance, investment strategies, and accounting.

Questions and answers must be distinct and should not repeat.

Your task is to generate up to {num_questions} distinct procedural question-answer pairs based on the given combined input, which includes both text and image descriptions.

Ensure:
- Carefully analyze both the text and image content together to extract key steps, data points, tables, columns, and relevant processes.
- No critical information from the text or image descriptions should be ignored.
- Integrate details from image descriptions into the steps when they provide context or are related to specific processes.

Input:
Combined content (from text and image description):
{paragraph}

Requirements:
1. Extract necessary details such as column names, tables, SWIFT codes, fund codes, URLs, or specific tools directly from the input.
2. Include any relevant information mentioned in the image descriptions (e.g., diagrams, tables, document references) that support or clarify the process.
3. Maintain a clear, step-by-step structure, covering data access, validation, handling exceptions (e.g., missing codes or 'CHECK FUND'), and final updates or escalations.
4. Preserve the exact terminology and references (e.g., columns A, B, tables, codes) as stated in the input.
5. Avoid redundant steps and ensure logical progression between them.

Output format:
[
    {{
        "question": "What are the steps to complete the financial data validation process?",
        "answer": [
            "Step 1: Log into the financial data portal (portal_name) using the provided URL (data_access_url) and valid credentials.",
            "Step 2: Retrieve the required report containing data for columns: Column A (Account Number), Column B (Fund Code), and Column C (SWIFT Code).",
            "Step 3: Check if any entry in Column B contains 'CHECK FUND'. If found, initiate an escalation to the compliance team following the escalation procedure described in the image description.",
            "Step 4: Validate the SWIFT codes in Column C using the lookup table provided in the report (Table_X). If missing, refer to the SWIFT table reference shown in the attached image.",
            "Step 5: Update validated records in the tracking sheet and note any exceptions or missing information based on details mentioned in the image descriptions."
        ],
        "extracted": "text, image_description, or both"
    }}
]

Guidelines:
- Ensure each procedural step includes critical details from both text and image content.
- Include image-based details if they explain or clarify specific steps (e.g., fund code validation tables, portal login instructions, document references).
- If fewer distinct steps are possible, return only the available ones.
- Ensure the response is a valid JSON array.







You are a financial expert with expertise in corporate finance, investment strategies, and accounting.

Questions and answers must be distinct and should not repeat.

Your task is to generate up to {num_questions} distinct procedural question-answer pairs based on the given combined input, which includes both textual and image-based content.

Ensure:
- Summarize key steps, tables, columns, and relevant data points naturally within the procedure.
- Maintain a logical, step-by-step flow, incorporating conditions (e.g., VLOOKUP, status checks) where needed.
- Use branching decisions for missing values or exceptions (e.g., handling cases like 'CHECK FUND').

Input:
Combined content (from text and image description):
{paragraph}

Requirements:
1. Each procedural step should clearly describe how to use the data, identify key fields, and execute checks or tasks.
2. Preserve references to columns, tables, fields, SWIFT codes, fund codes, or case IDs exactly as they appear in the input.
3. Include specific URLs, portals, or tools directly in the relevant steps if mentioned.
4. Ensure logical flow between steps (e.g., data access → validation → status updates → escalation).
5. Avoid redundant or repeated steps.

Output format:
[
    {{
        "question": "What are the steps to complete the financial data validation process?",
        "answer": [
            "Step 1: Log in to the data portal (portal_name) using your credentials at the specified URL (data_access_url).",
            "Step 2: Retrieve data from the financial report, focusing on columns: Column A (Account Number), Column B (Fund Code), and Column C (SWIFT Code).",
            "Step 3: If any entry in Column B contains the label 'CHECK FUND', initiate an escalation to the compliance team.",
            "Step 4: Validate all remaining fund codes using a VLOOKUP on the reference table provided (Table_X).",
            "Step 5: Update the status of validated records and document the outcome in the tracking sheet."
        ],
        "extracted": "text, image_description, or both"
    }}
]

Guidelines:
- Analyze both the text and image content for any relevant details (e.g., columns, tables, tools).
- If specific image-based information supports or clarifies a step, include it in the answer.
- Return only the available steps if fewer than {num_questions} steps are possible.
- Ensure the response is a valid JSON array.







You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.

Questions and answers must be distinct and should not repeat.

Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input, which contains both textual and image content combined.

Ensure:
- Questions focus on specific details, names, titles, or entities mentioned in the input text or image descriptions.
- Answers are precise and concise.
- Questions and answers must be distinct and should not repeat.
- When relevant, explicitly state if the question or answer is based on text, image descriptions, or both.

Input:
Combined content (from text and image description):
{paragraph}

Your goal is to:
1. Analyze the combined content from both text and image descriptions to generate factual questions.
2. Ensure the generated questions and answers reflect unique aspects of the provided input.
3. If fewer distinct questions are possible, return only the available ones.

Output format:
[
    {{
        "question": "What is the name of the entity mentioned in the input content?",
        "answer": "The name or entity mentioned in the input content.",
        "extracted": "text, image_description, or both"
    }},
    {{
        "question": "What detail is mentioned about the financial process described?",
        "answer": "Specific details from the text or image descriptions.",
        "extracted": "text, image_description, or both"
    }}
]

Ensure the response is a valid JSON array.








import json
import os

def extract_text_and_image_descriptions(json_file):
    with open(json_file, 'r', encoding='utf-8') as file:
        data = json.load(file)
    
    combined_paragraphs = []
    
    # Iterate through pages and combine text and image descriptions
    for page in data["pages"]:
        page_text = page.get("text", "").strip()
        image_descriptions = []

        # Collect image descriptions for the page
        if "images" in page:
            for image in page["images"]:
                image_desc = image.get("image_description", "").strip()
                if image_desc:
                    image_descriptions.append(image_desc)

        # Combine text and image descriptions into one paragraph per page
        combined_content = page_text
        if image_descriptions:
            combined_content += "\n\n" + "\n".join(image_descriptions)

        if combined_content.strip():
            combined_paragraphs.append(combined_content)
    
    return combined_paragraphs

def generate_questions(paragraph, question_type, num_questions, prompt_template):
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain
    from langchain.chat_models import ChatOpenAI

    # Load the prompt template
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_template
    )

    # Use OpenAI's LLM to generate questions
    llm = ChatOpenAI(temperature=0)
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    # Fill in the prompt
    fill_prompt = prompt.format(
        paragraph=paragraph,
        num_questions=num_questions,
        question_type=question_type
    )

    # Generate response and extract valid JSON
    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions),
            question_type=question_type
        )
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating questions: {e}")
        return []

def extract_valid_json(response):
    import json
    try:
        # Assuming JSON starts after the first valid structure and ends correctly
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError as e:
        print("Error: Invalid JSON detected.")
    return []

def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    all_questions = []

    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(
                paragraph=paragraph,
                question_type=question_type,
                num_questions=min(chunk_size, total_questions),
                prompt_template=prompt_template
            )
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue

    return all_questions

def generate_questions_by_all_types(paragraphs, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []

    questions_per_type = max(1, total_questions // len(question_types_and_prompts))

    for question_type, prompt_template in question_types_and_prompts.items():
        try:
            for paragraph in paragraphs:
                questions = generate_questions_in_chunks(paragraph, question_type, questions_per_type, prompt_template, chunk_size)
                all_questions.extend(questions)
        except Exception as e:
            print(f"Error processing {question_type}: {e}")
            continue

    return all_questions[:total_questions]

def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")

                    # Step 1: Extract text and metadata from the PDF
                    pdf_text = extract_text_and_metadata_pagewise(pdf_path)
                    text_json_path = "text.json"
                    with open(text_json_path, "w") as json_text:
                        json.dump(pdf_text, json_text, indent=4)

                    # Step 2: Extract descriptions from images
                    image_data = extract_text_from_pdf_images(pdf_path)
                    image_json_path = "image.json"
                    with open(image_json_path, "w") as json_file:
                        json.dump(image_data, json_file, indent=4)

                    # Step 3: Merge text and image descriptions
                    pdf_text = merge_json_files(text_json_path, image_json_path)

                    # Step 4: Extract combined content from JSON
                    combined_paragraphs = extract_text_and_image_descriptions("merged_text_image.json")

                    # Step 5: Generate questions
                    questions_and_answers = generate_questions_by_all_types(combined_paragraphs, total_questions, chunk_size)

                    # Step 6: Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)

                    save_to_excel(questions_and_answers, output_file)

                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")

# Sample prompt templates
FACTUAL_PROMPT = "Your factual prompt here..."
PROCEDURAL_PROMPT = "Your procedural prompt here..."
CONCEPTUAL_PROMPT = "Your conceptual prompt here..."
INFERENTIAL_PROMPT = "Your inferential prompt here..."
REASONING_PROMPT = "Your reasoning-based prompt here..."






import json

def restructure_json(input_file_path):
    # Read the input JSON from the file
    with open(input_file_path, "r") as input_file:
        input_json = json.load(input_file)

    # Ensure input_json is a list
    if not isinstance(input_json, list):
        raise ValueError("Expected a list as the root element, but got something else.")

    structured_data = {"pages": []}

    # Process each item in the list
    for page in input_json:
        if not isinstance(page, dict):
            print(f"Skipping invalid page entry: {page}")
            continue

        # Extract text and page number
        text = page.get("text", "").strip()
        page_number = page.get("page_number", 0)

        # Debugging print statement to identify empty or missing text
        if not text:
            print(f"[!] Warning: Empty or missing text on page {page_number}")

        structured_data["pages"].append({
            "page_number": page_number,
            "content": text if text else "[No content available]"  # Placeholder for empty content
        })

    return structured_data

# Example usage
input_file = "path/to/your/extracted_pdf_data.json"  # Replace with your JSON file path
restructured_data = restructure_json(input_file)
print(json.dumps(restructured_data, indent=2))







import json

def restructure_json(input_file_path):
    # Read the input JSON from the file
    with open(input_file_path, "r") as input_file:
        input_json = json.load(input_file)

    # Ensure input_json is a list
    if not isinstance(input_json, list):
        raise ValueError("Expected a list as the root element, but got something else.")

    structured_data = {"pages": []}

    # Process each item in the list
    for page in input_json:
        if not isinstance(page, dict):
            print(f"Skipping invalid page entry: {page}")
            continue

        # Extract text and page number
        text = page.get("text", "").strip()
        page_number = page.get("page_number", 0)

        structured_data["pages"].append({
            "page_number": page_number,
            "content": text
        })

    return structured_data

# Example usage
input_file = "path/to/your/extracted_pdf_data.json"  # Replace with your JSON file path
restructured_data = restructure_json(input_file)
print(json.dumps(restructured_data, indent=2))










import os
import json
from collections import defaultdict

def restructure_and_store_json(input_file_path, output_dir="outputs_sdr"):
    restructured_data = {
        "metadata": {},
        "pages": []
    }

    os.makedirs(output_dir, exist_ok=True)

    with open(input_file_path, "r") as input_file:
        input_json = json.load(input_file)

    # Ensure input_json is a list
    if not isinstance(input_json, list):
        raise ValueError("Expected a list of image entries, but got something else.")

    # Group images by page number
    page_images = defaultdict(list)
    for entry in input_json:
        page_number = entry.get("page_number", None)
        image_name = entry.get("image_name", entry.get("image name", "")).strip()
        image_description = entry.get("image_description", entry.get("image description", "")).strip()

        if page_number is not None and image_name:
            page_images[page_number].append({
                "image_name": image_name,
                "image_description": image_description
            })

    # Create page entries with images
    for page_number, images in page_images.items():
        restructured_data["pages"].append({
            "page_number": page_number,
            "text": f"Placeholder text for page {page_number}",  # Replace with actual page text if needed
            "images": images
        })

    # Define output file path and save JSON
    output_file_path = os.path.join(output_dir, "restructured_pdf_data.json")
    with open(output_file_path, "w") as output_file:
        json.dump(restructured_data, output_file, indent=2)
    print(f"[+] Restructured data saved to {output_file_path}")

    # Return the restructured data
    return restructured_data

# Example usage
input_file = "path/to/your/merged_text_image.json"  # Replace with the correct file path
restructured_data = restructure_and_store_json(input_file)

# Printing the returned data if needed
print(json.dumps(restructured_data, indent=2))










import os
import json

def save_json(data, output_file_path):
    with open(output_file_path, "w") as json_file:
        json.dump(data, json_file, indent=2)
    print(f"[+] Data saved to {output_file_path}")

def restructure_and_store_json(input_file_path, output_dir="outputs_sdr"):
    restructured_data = {
        "metadata": {},
        "pages": []
    }

    os.makedirs(output_dir, exist_ok=True)

    with open(input_file_path, "r") as input_file:
        input_json = json.load(input_file)

    # Copy metadata
    restructured_data["metadata"] = input_json.get("metadata", {})

    # Process pages
    for page in input_json.get("pages", []):
        text = page.get("text", "").strip()
        combined_content = text

        # Append image descriptions
        for image in page.get("images", []):
            image_description = image.get("image_description", image.get("image description", "")).strip()
            if image_description:
                combined_content += f" The image shows: {image_description}."

        restructured_data["pages"].append({
            "page_number": page.get("page_number", 0),
            "text": combined_content
        })

    # Define output file path and save JSON
    output_file_path = os.path.join(output_dir, "restructured_pdf_data.json")
    save_json(restructured_data, output_file_path)

    # Return the restructured data for further use if needed
    return restructured_data

# Example usage
input_file = "path/to/your/merged_text_image.json"  # Replace with your input JSON file path
restructured_data = restructure_and_store_json(input_file)








import os
import json

def save_json(data, output_file_path):
    """Save the given data to a JSON file."""
    with open(output_file_path, "w") as json_file:
        json.dump(data, json_file, indent=2)
    print(f"[+] Data saved to {output_file_path}")

def restructure_and_store_json(input_file_path, output_dir="outputs_sdr"):
    # Initialize the structure similar to your provided example
    restructured_data = {
        "metadata": {},
        "pages": []
    }

    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)

    try:
        # Read input JSON from the provided file path
        with open(input_file_path, "r") as input_file:
            input_json = json.load(input_file)

        # Copy metadata from the input JSON if available
        restructured_data["metadata"] = input_json.get("metadata", {})

        # Process pages
        pages = input_json.get("pages", [])
        for page in pages:
            # Ensure 'text' and 'page_number' are present
            text = page.get("text", "").strip()
            page_number = page.get("page_number", 0)

            # Initialize combined content with text
            combined_content = text

            # Check for images and append descriptions if available
            images = page.get("images", [])
            if isinstance(images, list):
                for image in images:
                    if isinstance(image, dict):
                        # Handle inconsistent keys ('image description' or 'image_description')
                        image_description = image.get("image_description", image.get("image description", "")).strip()
                        if image_description:
                            combined_content += f" The image shows: {image_description}."

            # Append the processed page data
            restructured_data["pages"].append({
                "page_number": page_number,
                "text": combined_content
            })

        # Define output file path
        output_file_path = os.path.join(output_dir, "restructured_pdf_data.json")

        # Save the restructured data using save_json function
        save_json(restructured_data, output_file_path)

    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
    except FileNotFoundError as e:
        print(f"Error: {e}")
    except ValueError as e:
        print(f"Error processing JSON: {e}")

# Example usage
input_file = "path/to/your/merged_text_image.json"  # Replace with your input JSON file path
restructure_and_store_json(input_file)










import os
import json
import PyPDF2

def extract_text_and_metadata_pagewise(file, output_dir="outputs_sdr"):
    pdf_data = {
        "metadata": {},
        "pages": []
    }

    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Open the PDF file and read metadata
    with open(file, 'rb') as pdf_file:
        reader = PyPDF2.PdfReader(pdf_file)

        # Extract metadata if available
        if reader.metadata:
            pdf_data["metadata"] = {k[1:]: v for k, v in reader.metadata.items()}
        else:
            pdf_data["metadata"] = {}

        # Extract text page by page
        for page_index, page in enumerate(reader.pages):
            page_text = page.extract_text() if page.extract_text() else ""
            pdf_data["pages"].append({
                "page_number": page_index + 1,
                "text": page_text.strip()
            })
            print(f"[+] Extracted text from page {page_index + 1}")

    # Save the result to a JSON file
    output_file = os.path.join(output_dir, "extracted_pdf_data.json")
    with open(output_file, "w") as json_file:
        json.dump(pdf_data, json_file, indent=2)
    print(f"[+] Data saved to {output_file}")

# Example usage:
file_path = "path/to/your/input.pdf"  # Replace with your PDF file path
extract_text_and_metadata_pagewise(file_path)








import json

def restructure_json(input_json):
    structured_data = {"pages": []}

    # Check if input_json has 'pages' key and it is a list
    pages = input_json.get("pages", [])
    if not isinstance(pages, list):
        raise ValueError("Expected 'pages' to be a list, but got something else.")

    for page in pages:
        # Ensure page is a dictionary
        if not isinstance(page, dict):
            raise ValueError(f"Expected a dictionary for each page, but got {type(page)}: {page}")

        # Access and validate 'text'
        text = page.get("text", "")
        if not isinstance(text, str):
            raise ValueError(f"Expected 'text' to be a string, but got {type(text)}")

        # Initialize combined content with the page text
        combined_content = text.strip()

        # Check for images and append descriptions
        images = page.get("images", [])
        if isinstance(images, list):
            for image in images:
                if not isinstance(image, dict):
                    print(f"Skipping invalid image entry: {image}")
                    continue  # Skip invalid image entries

                # Fix possible key inconsistency ('image description' -> 'image_description')
                image_description = image.get("image_description", image.get("image description", "")).strip()
                if image_description:
                    combined_content += f" The image shows: {image_description}."

        # Append structured page data
        structured_data["pages"].append({
            "page_number": page.get("page_number", 0),
            "content": combined_content.strip(),
            "source": "text and image_description"
        })

    return structured_data

# Read the JSON data from a file
file_path = "path/to/your/merged_text_image.json"

try:
    with open(file_path, "r") as file:
        input_json = json.load(file)  # Load JSON content from the file

    # Restructure the JSON
    restructured_json = restructure_json(input_json)
    print(json.dumps(restructured_json, indent=2))

except json.JSONDecodeError as e:
    print(f"Error decoding JSON: {e}")

except FileNotFoundError as e:
    print(f"Error: {e}")

except ValueError as e:
    print(f"Error processing JSON: {e}")













import json

def restructure_json(input_json):
    structured_data = {"pages": []}

    # Check if input_json has 'pages' key and it is a list
    pages = input_json.get("pages", [])
    if not isinstance(pages, list):
        raise ValueError("Expected 'pages' to be a list, but got something else.")

    for page in pages:
        # Ensure page is a dictionary
        if not isinstance(page, dict):
            raise ValueError(f"Expected a dictionary for each page, but got {type(page)}: {page}")

        # Access and validate 'text'
        text = page.get("text", "")
        if not isinstance(text, str):
            raise ValueError(f"Expected 'text' to be a string, but got {type(text)}")

        # Initialize combined content with the page text
        combined_content = text.strip()

        # Check for images and append descriptions
        images = page.get("images", [])
        if isinstance(images, list):
            for image in images:
                if isinstance(image, dict):
                    # Fixing possible key inconsistency ('image description' -> 'image_description')
                    image_description = image.get("image_description", image.get("image description", "")).strip()
                    if image_description:
                        combined_content += f" The image shows: {image_description}."

        # Append structured page data
        structured_data["pages"].append({
            "page_number": page.get("page_number", 0),
            "content": combined_content.strip(),
            "source": "text and image_description"
        })

    return structured_data

# Example input JSON based on your provided structure
input_json = {
    "metadata": {
        "Author": "Gilman-Cabral, Betsy S",
        "Keywords": "Limited Access",
        "Creator": "Microsoft Word",
        "CreationDate": "D:20241201170842+00'00'",
        "ModDate": "D:20241201170842+00'00'"
    },
    "pages": [
        {
            "page_number": 1,
            "text": "Information Classification: Limited Access.",
            "images": [
                {
                 












import json

def restructure_json(input_json):
    structured_data = {"pages": []}

    # Check if input is a list of pages
    if not isinstance(input_json, list):
        raise ValueError("Expected a list of pages, but got something else.")

    for page in input_json:
        # Ensure page is a dictionary
        if not isinstance(page, dict):
            raise ValueError(f"Expected a dictionary for each page, but got {type(page)}: {page}")

        # Access the 'text' field
        text = page.get("text", "")
        if not isinstance(text, str):
            raise ValueError(f"Expected 'text' to be a string, but got {type(text)}")

        # Start combining content
        combined_content = text

        # Check and append image descriptions
        images = page.get("images", [])
        if not isinstance(images, list):
            raise ValueError(f"Expected 'images' to be a list, but got {type(images)}")

        for image in images:
            if not isinstance(image, dict):
                raise ValueError(f"Expected each image to be a dictionary, but got {type(image)}")

            image_description = image.get("image_description", "")
            if image_description:
                combined_content += f" The image shows: {image_description}."

        # Append the structured data for this page
        structured_data["pages"].append({
            "page_number": page.get("page_number", 0),
            "content": combined_content.strip(),
            "source": "text and image_description"
        })

    return structured_data

# Example Input JSON
input_json = [
    {
        "page_number": 1,
        "text": "Information Classification: Limited Access.",
        "images": [
            {
                "image_name": "image_page1_1.png",
                "image_description": "The document appears to be a professional financial statement."
            }
        ]
    },
    {
        "page_number": 2,
        "text": "Hub Procedure Oversight Owner Names(s): Not Applicable.",
        "images": []
    },
    {
        "page_number": 3,
        "text": "1. DMH Data Maintenance - Ensure monthly book in MCH has been released.",
        "images": [
            {
                "image_name": "image_page3_1.png",
                "image_description": "The image depicts a detailed financial document."
            }
        ]
    }
]

try:
    # Restructure the JSON
    restructured_json = restructure_json(input_json)
    print(json.dumps(restructured_json, indent=2))
except ValueError as e:
    print(f"Error: {e}")










import json

def restructure_json(input_json):
    structured_data = {"pages": []}

    for page in input_json:  # Correct iteration over list of pages
        combined_content = page["text"]

        # Check and append image descriptions if available
        if "images" in page and isinstance(page["images"], list):
            for image in page["images"]:
                combined_content += f" The image shows: {image['image_description']}."

        # Append structured content for each page
        structured_data["pages"].append({
            "page_number": page["page_number"],
            "content": combined_content.strip(),
            "source": "text and image_description"
        })

    return structured_data

# Example Input JSON (list of pages, simplified)
input_json = [
    {
        "page_number": 1,
        "text": "Information Classification: Limited Access.",
        "images": [
            {
                "image_name": "image_page1_1.png",
                "image_description": "The document in question appears to be a professional financial statement."
            }
        ]
    },
    {
        "page_number": 2,
        "text": "Hub Procedure Oversight Owner Names(s): Not Applicable.",
        "images": []
    },
    {
        "page_number": 3,
        "text": "1. DMH Data Maintenance - Ensure monthly book in MCH has been released.",
        "images": [
            {
                "image_name": "image_page3_1.png",
                "image_description": "The image depicts a detailed financial document."
            },
            {
                "image_name": "image_page3_2.png",
                "image_description": "The image depicts a professional financial document."
            }
        ]
    }
]

# Restructure the JSON
restructured_json = restructure_json(input_json)

# Print the result
print(json.dumps(restructured_json, indent=2))










import json

def restructure_json(input_json):
    structured_data = {"pages": []}

    for page in input_json["pages"]:
        # Initialize combined content with page text
        combined_content = page["text"]

        # Add image descriptions if they exist
        if "images" in page and page["images"]:
            for image in page["images"]:
                combined_content += f" The image shows: {image['image_description']}."

        # Append structured page data
        structured_data["pages"].append({
            "page_number": page["page_number"],
            "content": combined_content.strip(),
            "source": "text and image_description"
        })

    return structured_data

# Example Input JSON (similar to yours)
input_json = {
    "pages": [
        {
            "page_number": 1,
            "text": "Information Classification: Limited Access.",
            "images": [
                {
                    "image_name": "image_page1_1.png",
                    "image_description": "The document in question appears to be a professional financial statement."
                }
            ]
        },
        {
            "page_number": 2,
            "text": "Hub Procedure Oversight Owner Names(s): Not Applicable.",
            "images": []
        },
        {
            "page_number": 3,
            "text": "1. DMH Data Maintenance - Ensure monthly book in MCH has been released.",
            "images": [
                {
                    "image_name": "image_page3_1.png",
                    "image_description": "The image depicts a detailed financial document."
                },
                {
                    "image_name": "image_page3_2.png",
                    "image_description": "The image depicts a professional financial document."
                }
            ]
        }
    ]
}

# Restructure the JSON
restructured_json = restructure_json(input_json)

# Print the result or save it to a file
print(json.dumps(restructured_json, indent=2))






from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

# Initialize Azure OpenAI LLM
llm = AzureChatOpenAI(
    temperature=0.7,
    api_version="azure_openai_api_version",
    azure_endpoint="azure_openai_endpoint",
    deployment_name="deployment_name"
)

def generate_questions(paragraph, question_type, num_questions, prompt_template):
    # Create the prompt template
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type"],
        template=prompt_template
    )

    # Fill the prompt using format
    fill_prompt = prompt.format(
        paragraph=paragraph, 
        num_questions=num_questions, 
        question_type=question_type
    )

    print(f"Generated the prompt:\n{fill_prompt}")

    # Create the LLM chain
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        # Generate questions using the LLM chain
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions),
            question_type=str(question_type)
        )

        # Extract and return valid JSON from the response
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating questions: {e}")
        return []

def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    all_questions = []

    for i in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(
                paragraph=paragraph, 
                question_type=question_type, 
                num_questions=min(chunk_size, total_questions - i), 
                prompt_template=prompt_template
            )
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue

    return all_questions

# Example usage (Replace with actual prompt and API settings)
paragraph = "Machine learning is a subset of artificial intelligence (AI) focused on building systems that learn from data."
question_type = "Factual"
total_questions = 25
prompt_template = (
    "Generate {num_questions} {question_type} questions based on the following paragraph:\n"
    "{paragraph}\n"
    "Provide the questions in JSON format."
)

questions = generate_questions_in_chunks(
    paragraph=paragraph,
    question_type=question_type,
    total_questions=total_questions,
    prompt_template=prompt_template
)

print("Generated Questions:", questions)








import fitz  # PyMuPDF for PDF parsing
import openai
from PIL import Image
import io
import os

# Initialize OpenAI API (replace with your actual API key)
openai.api_key = "43d6ab1448834574807eff7a195f76f3"

def extract_procedural_steps_with_images(pdf_path):
    # Open the PDF
    doc = fitz.open(pdf_path)
    procedure_steps = []

    # Iterate through the pages and extract relevant text and images
    for page_num in range(len(doc)):
        page = doc[page_num]
        text = page.get_text()

        # Identify procedural step headings and related text (using keyword heuristic)
        lines = text.split('\n')
        step_content = []

        for i, line in enumerate(lines):
            if line.lower().startswith("step"):
                step_content = [line.strip()]

                # Gather subsequent lines related to the step until another step or blank line is encountered
                for j in range(i + 1, len(lines)):
                    if lines[j].strip().lower().startswith("step") or lines[j].strip() == "":
                        break
                    step_content.append(lines[j].strip())

                step_text = "\n".join(step_content)

                # Check if the page contains any images related to this step
                images = page.get_images(full=True)
                image_data = []

                for img_index, img in enumerate(images):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image_ext = base_image["ext"]
                    image = Image.open(io.BytesIO(image_bytes))

                    # Save the image temporarily
                    image_filename = f"temp_image_{page_num}_{img_index}.{image_ext}"
                    image.save(image_filename)
                    image_data.append(image_filename)

                # Add the step and any associated images
                procedure_steps.append({"text": step_text, "images": image_data})

    return procedure_steps

def generate_procedural_qna_with_images(procedure_steps, prompt_template):
    qna_pairs = []

    for step_data in procedure_steps:
        step_text = step_data["text"]
        prompt = prompt_template.format(procedure_text=step_text)

        response = openai.Completion.create(
            engine="text-davinci-003",
            prompt=prompt,
            max_tokens=300
        )

        generated_text = response.choices[0].text.strip()

        # Split the generated output into question-answer pairs
        for line in generated_text.split('\n'):
            if '|' in line:
                question, answer = line.split('|', 1)
                qna_pairs.append({
                    "question": question.strip(),
                    "answer": answer.strip(),
                    "images": step_data["images"]
                })

    return qna_pairs

def save_qna_to_file(qna_pairs, output_file):
    with open(output_file, 'w') as f:
        for pair in qna_pairs:
            f.write(f"Question: {pair['question']}\nAnswer: {pair['answer']}\n")
            if pair["images"]:
                f.write(f"Related Images: {', '.join(pair['images'])}\n\n")

pdf_path = r"h:\Question and answer\Fund accounting (Resubmit_1025)36. VFMC AAS25 SOP 5.0.pdf"
output_file = r"h:\Question and answer\Fund accounting\procedural_qna_with_images.txt"

# Define a prompt template for procedural Q&A generation
prompt_template = (
    "You are an AI assistant tasked with generating procedural questions and answers. "
    "Given the following procedure step, generate questions that can help clarify the task, "
    "and provide an answer for each question in the format 'Question | Answer'.\n\n"
    "Procedure Step:\n{procedure_text}"
)

# Step 1: Extract procedural steps and associated images from the PDF
procedure_steps = extract_procedural_steps_with_images(pdf_path)
print(procedure_steps)

# Step 2: Generate procedural Q&A pairs
qna_pairs = generate_procedural_qna_with_images(procedure_steps, prompt_template)

# Step 3: Save Q&A pairs to the output file
save_qna_to_file(qna_pairs, output_file)

print(f"Generated Q&A pairs with images saved to {output_file}")















import json
import os
import re

def fix_and_load_json(json_file):
    """Fix any trailing commas and load JSON from a file."""
    with open(json_file, 'r', encoding="utf-8-sig") as file:
        json_content = file.read()

    # Basic fix for trailing commas before closing brackets
    fixed_json = re.sub(r',\s*([\]])', r'\1', json_content)

    try:
        return json.loads(fixed_json)
    except json.JSONDecodeError as e:
        print(f"JSON decoding error in file {json_file}: {e}")
        return None


def merge_json_files(text_json_path, image_json_path, output_dir="outputs_sdr"):
    """Merge text and image JSON files based on 'page_number'."""
    # Ensure the output directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Fix and load JSON data
    text_data = fix_and_load_json(text_json_path)
    image_data = fix_and_load_json(image_json_path)

    if text_data is None or image_data is None:
        print("Error in loading JSON files. Please check their format.")
        return None

    # Convert image JSON to a dictionary for quick lookup by page_number
    image_dict = {}

    for image in image_data.get("pages", []):  # Assuming image_data also has a 'pages' key
        page_number = image.get("page_number")
        if page_number not in image_dict:
            image_dict[page_number] = []
        image_dict[page_number].append({
            "image_name": image.get("image_name"),
            "image_description": image.get("image_description")
        })

    # Merge text and image data based on 'page_number'
    for page in text_data.get("pages", []):
        page_number = page.get("page_number")
        if page_number in image_dict:
            page["images"] = image_dict[page_number]

    # Save the merged JSON file
    save_json_image(output_dir, text_data)

    # Return the merged data (or other useful information)
    return text_data  # Returning the merged data


def save_json_image(output_dir, image_descriptions):
    """Save the merged JSON data to a file."""
    json_file_path = os.path.join(output_dir, "merged_text_image.json")
    with open(json_file_path, "w", encoding="utf-8") as json_file:
        json.dump(image_descriptions, json_file, indent=4)

    print(f"Merged JSON saved to: {json_file_path}")


# Example usage
text_json_path = r'h:\Q & A Note Book\final\outputs_sdr\text.json'
image_json_path = r'h:\Q & A Note Book\final\outputs_sdr\image.json'

# Now you can use the return value from the merge_json_files function
merged_data = merge_json_files(text_json_path, image_json_path)

# If you need to do something further with the returned data
if merged_data:
    print(f"Merged data: {merged_data}")
else:
    print("No merged data to process.")






import json
import os
import re

def fix_and_load_json(json_file):
    """Fix any trailing commas and load JSON from a file."""
    with open(json_file, 'r', encoding="utf-8-sig") as file:
        json_content = file.read()

    # Basic fix for trailing commas before closing brackets
    fixed_json = re.sub(r',\s*([\]])', r'\1', json_content)

    try:
        return json.loads(fixed_json)
    except json.JSONDecodeError as e:
        print(f"JSON decoding error in file {json_file}: {e}")
        return None


def merge_json_files(text_json_path, image_json_path, output_dir="outputs_sdr"):
    """Merge text and image JSON files based on 'page_number'."""
    # Ensure the output directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Fix and load JSON data
    text_data = fix_and_load_json(text_json_path)
    image_data = fix_and_load_json(image_json_path)

    if text_data is None or image_data is None:
        print("Error in loading JSON files. Please check their format.")
        return

    # Convert image JSON to a dictionary for quick lookup by page_number
    image_dict = {}

    for image in image_data.get("pages", []):  # Assuming image_data also has a 'pages' key
        page_number = image.get("page_number")
        if page_number not in image_dict:
            image_dict[page_number] = []
        image_dict[page_number].append({
            "image_name": image.get("image_name"),
            "image_description": image.get("image_description")
        })

    # Merge text and image data based on 'page_number'
    for page in text_data.get("pages", []):
        page_number = page.get("page_number")
        if page_number in image_dict:
            page["images"] = image_dict[page_number]

    # Save the merged JSON file
    save_json_image(output_dir, text_data)


def save_json_image(output_dir, image_descriptions):
    """Save the merged JSON data to a file."""
    json_file_path = os.path.join(output_dir, "merged_text_image.json")
    with open(json_file_path, "w", encoding="utf-8") as json_file:
        json.dump(image_descriptions, json_file, indent=4)

    print(f"Merged JSON saved to: {json_file_path}")


# Example usage
text_json_path = r'h:\Q & A Note Book\final\outputs_sdr\text.json'
image_json_path = r'h:\Q & A Note Book\final\outputs_sdr\image.json'

merge_json_files(text_json_path, image_json_path)







import json
import re
import os

def fix_and_load_json(json_file):
    with open(json_file, 'r', encoding='utf-8-sig') as file:
        json_content = file.read()
    
    # Basic fix for trailing commas before closing brackets
    fixed_json = re.sub(r',\s*([}\]])', r'\1', json_content)
    
    try:
        return json.loads(fixed_json)
    except json.JSONDecodeError as e:
        print(f"JSON decoding error in file {json_file}: {e}")
        return None

def merge_json_files(text_json_path, image_json_path, output_dir="outputs_sdr"):
    # Ensure the output directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Fix and load JSON data
    text_data = fix_and_load_json(text_json_path)
    image_data = fix_and_load_json(image_json_path)

    if text_data is None or image_data is None:
        print("Error in loading JSON files. Please check their format.")
        return

    # Convert image JSON to a dictionary for quick lookup by page_number
    image_dict = {}
    for image in image_data:
        page_number = image.get("page_number")
        if page_number not in image_dict:
            image_dict[page_number] = []
        image_dict[page_number].append({
            "image_name": image.get("image_name"),
            "description": image.get("description")
        })

    # Merge text and image data based on page_number
    for page in text_data["pages"]:
        page_number = page.get("page_number")
        if page_number in image_dict:
            page["images"] = image_dict[page_number]

    # Save the merged data to a new file in the output directory
    output_file_path = os.path.join(output_dir, "merged_data.json")
    with open(output_file_path, 'w', encoding='utf-8') as output_file:
        json.dump(text_data, output_file, indent=4)

    print(f"Merged JSON saved to: {output_file_path}")

# Example usage
text_json_path = 'path/to/text.json'  # Replace with your actual path
image_json_path = 'path/to/image.json'  # Replace with your actual path

merge_json_files(text_json_path, image_json_path)











import json
import re
import os

def fix_and_load_json(json_file):
    with open(json_file, 'r', encoding='utf-8-sig') as file:
        json_content = file.read()
    
    # Basic fix for trailing commas before closing brackets
    fixed_json = re.sub(r',\s*([}\]])', r'\1', json_content)
    
    try:
        return json.loads(fixed_json)
    except json.JSONDecodeError as e:
        print(f"JSON decoding error in file {json_file}: {e}")
        return None

def merge_json_files(text_json_path, image_json_path, output_path):
    # Fix and load JSON data
    text_data = fix_and_load_json(text_json_path)
    image_data = fix_and_load_json(image_json_path)

    if text_data is None or image_data is None:
        print("Error in loading JSON files. Please check their format.")
        return

    # Convert image JSON to a dictionary for quick lookup by page_number
    image_dict = {}
    for image in image_data:
        page_number = image.get("page_number")
        if page_number not in image_dict:
            image_dict[page_number] = []
        image_dict[page_number].append({
            "image_name": image.get("image_name"),
            "description": image.get("description")
        })

    # Merge text and image data based on page_number
    for page in text_data["pages"]:
        page_number = page.get("page_number")
        if page_number in image_dict:
            page["images"] = image_dict[page_number]

    # Create the output directory if it doesn't exist
    os.makedirs(output_path, exist_ok=True)

    # Save the merged data to a new file
    output_file_path = os.path.join(output_path, "merged_data.json")
    with open(output_file_path, 'w', encoding='utf-8') as output_file:
        json.dump(text_data, output_file, indent=4)

    print(f"Merged JSON saved to: {output_file_path}")

# Example usage
text_json_path = 'path/to/text.json'  # Replace with actual path
image_json_path = 'path/to/image.json'  # Replace with actual path
output_path = 'path/to/output_directory'  # Replace with desired output directory

merge_json_files(text_json_path, image_json_path, output_p











import json
import os

def merge_json_files(text_json_path, image_json_path, output_path):
    # Load the JSON data from the files with specified encoding
    with open(text_json_path, 'r', encoding='utf-8-sig') as text_file:
        text_data = json.load(text_file)
    
    with open(image_json_path, 'r', encoding='utf-8-sig') as image_file:
        image_data = json.load(image_file)

    # Convert image JSON to a dictionary for quick lookup by page_number
    image_dict = {}
    for image in image_data:
        page_number = image.get("page_number")
        if page_number not in image_dict:
            image_dict[page_number] = []
        image_dict[page_number].append({
            "image_name": image.get("image_name"),
            "description": image.get("description")
        })

    # Merge text and image data based on page_number
    for page in text_data["pages"]:
        page_number = page.get("page_number")
        if page_number in image_dict:
            page["images"] = image_dict[page_number]

    # Create the output directory if it doesn't exist
    os.makedirs(output_path, exist_ok=True)

    # Save the merged data to a new file
    output_file_path = os.path.join(output_path, "merged_data.json")
    with open(output_file_path, 'w', encoding='utf-8') as output_file:
        json.dump(text_data, output_file, indent=4)

    print(f"Merged JSON saved to: {output_file_path}")

# Example usage
text_json_path = 'path/to/text.json'  # Replace with actual path
image_json_path = 'path/to/image.json'  # Replace with actual path
output_path = 'path/to/output_directory'  # Replace with desired output directory

merge_json_files(text_json_path, image_json_path, output_path)










import json
import os

def merge_json_files(text_json_path, image_json_path, output_path):
    # Load the JSON data from the files
    with open(text_json_path, 'r') as text_file:
        text_data = json.load(text_file)
    
    with open(image_json_path, 'r') as image_file:
        image_data = json.load(image_file)

    # Convert image JSON to a dictionary for quick lookup by page_number
    image_dict = {}
    for image in image_data:
        page_number = image.get("page_number")
        if page_number not in image_dict:
            image_dict[page_number] = []
        image_dict[page_number].append({
            "image_name": image.get("image_name"),
            "description": image.get("description")
        })

    # Merge text and image data based on page_number
    for page in text_data["pages"]:
        page_number = page.get("page_number")
        if page_number in image_dict:
            page["images"] = image_dict[page_number]

    # Create the output directory if it doesn't exist
    os.makedirs(output_path, exist_ok=True)

    # Save the merged data to a new file
    output_file_path = os.path.join(output_path, "merged_data.json")
    with open(output_file_path, 'w') as output_file:
        json.dump(text_data, output_file, indent=4)

    print(f"Merged JSON saved to: {output_file_path}")

# Example usage
text_json_path = 'path/to/text.json'  # Replace with actual path
image_json_path = 'path/to/image.json'  # Replace with actual path
output_path = 'path/to/output_directory'  # Replace with desired output directory

merge_json_files(text_json_path, image_json_path, output_path)











# Prompt for generating step-wise answers combining text and image descriptions with the given output format

"""
Given a JSON input containing procedural steps with textual content and image descriptions, generate a structured output with step-by-step answers formatted as specified. Each step may reference text, images, or both. Ensure that:

1. Steps should follow the order mentioned in the input (Step 1, Step 2, etc.).
2. If a step includes both textual and image information, merge them into a single, coherent answer.
3. Image descriptions should be included where applicable (e.g., Step 3 or Step 5).
4. All critical details, such as addresses, instructions, and labeled components, should be included without omissions.
5. Include the source of the extracted content, either from the paragraph or image descriptions.

### **Input Format:**
- Text: A JSON object with a key "text" containing step-by-step instructions.
- Image description: A JSON object with keys like "Step 3", "Step 5", etc., containing visual descriptions.

### **Output Format:**
{
    "question": "What are the steps to achieve or complete this process?",
    "answer": [
        "Step 1: [Description of Step 1, including any textual or image-based content].",
        "Step 2: [Description of Step 2, with any relevant details from text or images].",
        "Step n: [Description of Step n, combining textual and image information where needed]."
    ],
    "extracted": "paragraph/image_content"
}

### **Example Input:**
- Text: 
{
    "text": "Step 1: Gather materials. Step 2: Visit 45 Pine Street to collect the tools. Step 3: Assemble the parts using the diagram. Step 4: Install the system. Step 5: Verify the setup visually."
}
- Image description: 
{
    "description": {
        "Step 3": "An image showing the components for assembly with labeled parts.",
        "Step 5": "An image of the fully assembled system."
    }
}

### **Expected Output:**
{
    "question": "What are the steps to achieve or complete this process?",
    "answer": [
        "Step 1: Gather the necessary materials before beginning the process.",
        "Step 2: Visit 45 Pine Street to collect the required tools.",
        "Step 3: Assemble the parts using the provided diagram. The image shows the components for assembly with labeled parts.",
        "Step 4: Install the system as described in the instructions.",
        "Step 5: Verify the setup visually using the image of the fully assembled system."
    ],
    "extracted": "paragraph/image_content"
}

Generate the output exactly in the structured format specified.
"""









import json

def merge_text_and_description(text_file_path, image_file_path, output_file_path):
    # Load the text JSON from the file with explicit encoding
    with open(text_file_path, 'r', encoding='utf-8') as text_file:
        text_json = json.load(text_file)

    # Load the image JSON from the file with explicit encoding
    with open(image_file_path, 'r', encoding='utf-8') as image_file:
        image_json = json.load(image_file)

    # Create a dictionary of images based on page_number for easy lookup
    image_dict = {image["page_number"]: image for image in image_json}

    # Initialize the merged JSON
    merged_json = {"metadata": text_json["metadata"], "pages": []}

    # Merge text and images based on page number
    for text_page in text_json["pages"]:
        page_number = text_page["page_number"]
        
        # Create a new page entry with text and image combined into 'text_and_description'
        merged_page = {
            "page_number": page_number,
            "text_and_description": str(text_page["text"])  # Ensure text is a string
        }
        
        # Add image description to 'text_and_description' if available for that page
        if page_number in image_dict:
            # Ensure description is a string
            merged_page["text_and_description"] += "\n" + str(image_dict[page_number]["description"])  # Concatenate description
        
        # Append merged page to the result
        merged_json["pages"].append(merged_page)

    # Save the merged JSON to the output file with explicit encoding
    with open(output_file_path, 'w', encoding='utf-8') as output_file:
        json.dump(merged_json, output_file, indent=2, ensure_ascii=False)

    # Optionally, return the merged JSON or print it
    return merged_json

# Example usage
merged_json = merge_text_and_description('text.json', 'images.json', 'merged.json')
print(json.dumps(merged_json, indent=2, ensure_ascii=False))











import json

def merge_text_and_description(text_file_path, image_file_path, output_file_path):
    # Load the text JSON from the file with explicit encoding
    with open(text_file_path, 'r', encoding='utf-8') as text_file:
        text_json = json.load(text_file)

    # Load the image JSON from the file with explicit encoding
    with open(image_file_path, 'r', encoding='utf-8') as image_file:
        image_json = json.load(image_file)

    # Create a dictionary of images based on page_number for easy lookup
    image_dict = {image["page_number"]: image for image in image_json}

    # Initialize the merged JSON
    merged_json = {"metadata": text_json["metadata"], "pages": []}

    # Merge text and images based on page number
    for text_page in text_json["pages"]:
        page_number = text_page["page_number"]
        
        # Create a new page entry with text and image combined into 'text_and_description'
        merged_page = {
            "page_number": page_number,
            "text_and_description": text_page["text"]  # Start with text field
        }
        
        # Add image description to 'text_and_description' if available for that page
        if page_number in image_dict:
            merged_page["text_and_description"] += "\n" + image_dict[page_number]["description"]  # Concatenate description
        
        # Append merged page to the result
        merged_json["pages"].append(merged_page)

    # Save the merged JSON to the output file with explicit encoding
    with open(output_file_path, 'w', encoding='utf-8') as output_file:
        json.dump(merged_json, output_file, indent=2, ensure_ascii=False)

    # Optionally, return the merged JSON or print it
    return merged_json

# Example usage
merged_json = merge_text_and_description('text.json', 'images.json', 'merged.json')
print(json.dumps(merged_json, indent=2, ensure_ascii=False))









import json

def merge_text_and_description(text_file_path, image_file_path, output_file_path):
    # Load the text JSON from the file
    with open(text_file_path, 'r') as text_file:
        text_json = json.load(text_file)

    # Load the image JSON from the file
    with open(image_file_path, 'r') as image_file:
        image_json = json.load(image_file)

    # Create a dictionary of images based on page_number for easy lookup
    image_dict = {image["page_number"]: image for image in image_json}

    # Initialize the merged JSON
    merged_json = {"metadata": text_json["metadata"], "pages": []}

    # Merge text and images based on page number
    for text_page in text_json["pages"]:
        page_number = text_page["page_number"]
        
        # Create a new page entry with text and image combined into 'text_and_description'
        merged_page = {
            "page_number": page_number,
            "text_and_description": text_page["text"]  # Start with text field
        }
        
        # Add image description to 'text_and_description' if available for that page
        if page_number in image_dict:
            merged_page["text_and_description"] += "\n" + image_dict[page_number]["description"]  # Concatenate description
        
        # Append merged page to the result
        merged_json["pages"].append(merged_page)

    # Save the merged JSON to the output file
    with open(output_file_path, 'w') as output_file:
        json.dump(merged_json, output_file, indent=2)

    # Optionally, return the merged JSON or print it
    return merged_json

# Example usage
merged_json = merge_text_and_description('text.json', 'images.json', 'merged.json')
print(json.dumps(merged_json, indent=2))











import json

# Load the text JSON from the file
with open('text.json', 'r') as text_file:
    text_json = json.load(text_file)

# Load the image JSON from the file
with open('images.json', 'r') as image_file:
    image_json = json.load(image_file)

# Create a dictionary of images based on page_number for easy lookup
image_dict = {image["page_number"]: image for image in image_json}

# Initialize the merged JSON
merged_json = {"metadata": text_json["metadata"], "pages": []}

# Merge text and images based on page number
for text_page in text_json["pages"]:
    page_number = text_page["page_number"]
    
    # Create a new page entry with text and image combined into 'text_and_description'
    merged_page = {
        "page_number": page_number,
        "text_and_description": text_page["text"]  # Start with text field
    }
    
    # Add image description to 'text_and_description' if available for that page
    if page_number in image_dict:
        merged_page["text_and_description"] += "\n" + image_dict[page_number]["description"]  # Concatenate description
    
    # Append merged page to the result
    merged_json["pages"].append(merged_page)

# Save the merged JSON to a new file
with open('merged.json', 'w') as output_file:
    json.dump(merged_json, output_file, indent=2)

# Optionally print the merged JSON
print(json.dumps(merged_json, indent=2))







import json

def generate_procedural_questions(text_json, image_json, num_questions=5):
    questions = []

    # Load both JSON files
    with open(text_json, 'r', encoding='utf-8') as f:
        text_data = json.load(f)

    with open(image_json, 'r', encoding='utf-8') as f:
        image_data = json.load(f)

    # Organize image descriptions by page number (convert page_number to string consistently)
    images_by_page = {}
    for image_entry in image_data:
        page_number = str(image_entry["page_number"])  # Convert to string
        if page_number not in images_by_page:
            images_by_page[page_number] = []
        images_by_page[page_number].append(image_entry["description"])

    # Generate questions by combining text and images dynamically
    for page in text_data["pages"]:
        page_number = str(page["page_number"])  # Convert to string
        page_text = page["text"].strip()

        # Skip pages with no meaningful text
        if not page_text or len(page_text) < 50:  # Threshold to ignore headers or empty pages
            continue

        # Combine with image descriptions if available
        combined_text = page_text
        if page_number in images_by_page:
            combined_text += "\n\n" + "\n".join(images_by_page[page_number])

        # Create the question-answer pair
        question_answer = {
            "question": f"What are the key procedural steps for page {page_number}?",
            "answer": generate_steps_from_text(combined_text),
            "extracted": combined_text
        }

        questions.append(question_answer)
        if len(questions) >= num_questions:
            break

    return questions

def generate_steps_from_text(combined_text):
    # Splitting the combined content into logical steps
    steps = []
    lines = combined_text.split("\n")
    
    step_number = 1
    for line in lines:
        line = line.strip()
        if len(line) > 20:  # Ignore short or meaningless lines
            steps.append(f"Step {step_number}: {line}")
            step_number += 1

    return steps

# Example usage
text_json = "path/to/text.json"
image_json = "path/to/image.json"
try:
    questions = generate_procedural_questions(text_json, image_json, num_questions=5)

    # Output the results to a JSON file
    with open("procedural_questions.json", "w", encoding="utf-8") as outfile:
        json.dump(questions, outfile, indent=4, ensure_ascii=False)

    print("Procedural questions generated successfully.")

except json.JSONDecodeError as e:
    print(f"JSON decoding error: {e}")
except FileNotFoundError as e:
    print(f"File not found: {e}")
except ValueError as e:
    print(f"ValueError: {e}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")










import json

def generate_procedural_questions(text_json, image_json, num_questions=5):
    questions = []

    # Load and validate JSON files
    with open(text_json, 'r', encoding='utf-8') as f:
        text_data = json.load(f)

    with open(image_json, 'r', encoding='utf-8') as f:
        image_data = json.load(f)

    # Validate JSON structure
    if not isinstance(text_data, dict) or "pages" not in text_data or not isinstance(text_data["pages"], list):
        raise ValueError("Invalid structure in text JSON. Expected a dictionary with a 'pages' key containing a list.")

    if not isinstance(image_data, list):
        raise ValueError("Invalid structure in image JSON. Expected a list of dictionaries.")

    # Organize image descriptions by page number (convert page_number to string consistently)
    images_by_page = {}
    for image_entry in image_data:
        if not isinstance(image_entry, dict) or "page_number" not in image_entry or "description" not in image_entry:
            raise ValueError(f"Invalid image entry format: {image_entry}")
        
        page_number = str(image_entry["page_number"])  # Convert to string
        if page_number not in images_by_page:
            images_by_page[page_number] = []
        images_by_page[page_number].append(image_entry["description"])

    # Generate questions by combining text and images dynamically
    for page in text_data["pages"]:
        if not isinstance(page, dict) or "page_number" not in page or "text" not in page:
            raise ValueError(f"Invalid page entry format: {page}")

        page_number = str(page["page_number"])  # Convert to string
        page_text = page["text"].strip()

        # Skip pages with no meaningful text
        if not page_text or len(page_text) < 50:  # Threshold to ignore headers or empty pages
            continue

        # Combine with image descriptions if available
        combined_text = page_text
        if page_number in images_by_page:
            combined_text += "\n\n" + "\n".join(images_by_page[page_number])

        # Create the question-answer pair
        question_answer = {
            "question": "What are the key procedural steps for page {}?".format(page_number),
            "answer": generate_steps_from_text(combined_text),
            "extracted": combined_text
        }

        questions.append(question_answer)
        if len(questions) >= num_questions:
            break

    return questions

def generate_steps_from_text(combined_text):
    # Splitting the combined content into logical steps
    steps = []
    lines = combined_text.split("\n")
    
    step_number = 1
    for line in lines:
        line = line.strip()
        if len(line) > 20:  # Ignore short or meaningless lines
            steps.append(f"Step {step_number}: {line}")
            step_number += 1

    return steps

# Example usage
text_json = "path/to/text.json"
image_json = "path/to/image.json"
try:
    questions = generate_procedural_questions(text_json, image_json, num_questions=5)

    # Output the results to a JSON file
    with open("procedural_questions.json", "w", encoding="utf-8") as outfile:
        json.dump(questions, outfile, indent=4, ensure_ascii=False)

    print("Procedural questions generated successfully.")

except json.JSONDecodeError as e:
    print(f"JSON decoding error: {e}")
except FileNotFoundError as e:
    print(f"File not found: {e}")
except ValueError as e:
    print(f"ValueError: {e}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")







import json

def generate_procedural_questions(text_json, image_json, num_questions=5):
    questions = []

    # Load both JSON files
    with open(text_json, 'r', encoding='utf-8') as f:
        text_data = json.load(f)

    with open(image_json, 'r', encoding='utf-8') as f:
        image_data = json.load(f)

    # Organize image descriptions by page number (convert page_number to string consistently)
    images_by_page = {}
    for image_entry in image_data:
        try:
            page_number = str(image_entry["page_number"])  # Convert to string
            if page_number not in images_by_page:
                images_by_page[page_number] = []
            images_by_page[page_number].append(image_entry["description"])
        except KeyError as e:
            print(f"KeyError in image data: {e}")
        except Exception as e:
            print(f"Unexpected error in image processing: {e}")

    # Generate questions by combining text and images dynamically
    for page in text_data["pages"]:
        try:
            page_number = str(page["page_number"])  # Convert to string
            page_text = page["text"].strip()

            # Skip pages with no meaningful text
            if not page_text or len(page_text) < 50:  # Threshold to ignore headers or empty pages
                continue

            # Combine with image descriptions if available
            combined_text = page_text
            if page_number in images_by_page:
                combined_text += "\n\n" + "\n".join(images_by_page[page_number])

            # Create the question-answer pair
            question_answer = {
                "question": "What are the key procedural steps for page {}?".format(page_number),
                "answer": generate_steps_from_text(combined_text),
                "extracted": combined_text
            }

            questions.append(question_answer)
            if len(questions) >= num_questions:
                break
        except KeyError as e:
            print(f"KeyError in text data: {e}")
        except Exception as e:
            print(f"Unexpected error in text processing: {e}")

    return questions

def generate_steps_from_text(combined_text):
    # Splitting the combined content into logical steps
    steps = []
    lines = combined_text.split("\n")
    
    step_number = 1
    for line in lines:
        line = line.strip()
        if len(line) > 20:  # Ignore short or meaningless lines
            steps.append(f"Step {step_number}: {line}")
            step_number += 1

    return steps

# Example usage
text_json = "path/to/text.json"
image_json = "path/to/image.json"
try:
    questions = generate_procedural_questions(text_json, image_json, num_questions=5)

    # Output the results to a JSON file
    with open("procedural_questions.json", "w", encoding="utf-8") as outfile:
        json.dump(questions, outfile, indent=4, ensure_ascii=False)

    print("Procedural questions generated successfully.")

except json.JSONDecodeError as e:
    print(f"JSON decoding error: {e}")
except FileNotFoundError as e:
    print(f"File not found: {e}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")











import json

def generate_procedural_questions(text_json, image_json, num_questions=5):
    questions = []

    # Load both JSON files
    with open(text_json, 'r', encoding='utf-8') as f:
        text_data = json.load(f)

    with open(image_json, 'r', encoding='utf-8') as f:
        image_data = json.load(f)

    # Organize image descriptions by page number
    images_by_page = {}
    for image_entry in image_data:
        page_number = image_entry["page_number"]
        if page_number not in images_by_page:
            images_by_page[page_number] = []
        images_by_page[page_number].append(image_entry["description"])

    # Generate questions by combining text and images dynamically
    for page in text_data["pages"]:
        page_number = page["page_number"]
        page_text = page["text"].strip()

        # Skip pages with no meaningful text
        if not page_text or len(page_text) < 50:  # Threshold to ignore headers or empty pages
            continue

        # Combine with image descriptions if available
        combined_text = page_text
        if page_number in images_by_page:
            combined_text += "\n\n" + "\n".join(images_by_page[page_number])

        # Create the question-answer pair
        question_answer = {
            "question": "What are the key procedural steps for page {}?".format(page_number),
            "answer": generate_steps_from_text(combined_text),
            "extracted": combined_text
        }

        questions.append(question_answer)
        if len(questions) >= num_questions:
            break

    return questions

def generate_steps_from_text(combined_text):
    # Splitting the combined content into logical steps
    steps = []
    lines = combined_text.split("\n")
    
    step_number = 1
    for line in lines:
        line = line.strip()
        if len(line) > 20:  # Ignore short or meaningless lines
            steps.append(f"Step {step_number}: {line}")
            step_number += 1

    return steps

# Example usage
text_json = "path/to/text.json"
image_json = "path/to/image.json"
questions = generate_procedural_questions(text_json, image_json, num_questions=5)

# Output the results to a JSON file
with open("procedural_questions.json", "w", encoding="utf-8") as outfile:
    json.dump(questions, outfile, indent=4, ensure_ascii=False)

print("Procedural questions generated successfully.")







# Procedural Prompt for LLM: Ensure Image Content Is Always Included in the Generated QA Pairs

"""
You are a financial expert with expertise in corporate finance, investment strategies, and accounting. 
Generate distinct and meaningful procedural question-answer pairs by combining the provided paragraph text and image descriptions. 
Questions and answers must be unique, logical, and must always seamlessly include relevant image content in the answer.

---

### Key Instructions:

1. **Combine Text and Image Content:**
    - For each section of text, identify and merge the corresponding image description if the image belongs to the same or adjacent page.
    - Always reference key visual details, descriptions of tables, interfaces, or processes from the image description in the answers.

2. **Summarize Relevant Details:**
    - Extract and summarize necessary tables, data fields, financial documents, or interface instructions from both the text and image descriptions.
    - Ensure image-based data, such as descriptions of financial tables or software interfaces, are directly integrated into the steps.

3. **Maintain Step-Wise Logical Flow:**
    - Provide a step-by-step explanation of the task or process, incorporating VLOOKUPs, missing data checks, and decision-making steps.
    - Where applicable, explain how image-related information guides or supports each step.

4. **Mandatory Image Integration:**
    - If an image contains tables, fields, or interface actions, describe them explicitly and explain how they influence or support the process.
    - Avoid generic instructions; each answer must incorporate visual content to enhance or clarify the steps.

5. **Include Branching or Conditional Logic:**
    - Include conditions (e.g., handling missing fund codes or errors). Reference images that depict relevant screens, documents, or fields for decision-making.

---

### Input Format:

- **Text:** {paragraph}
- **Image descriptions:** {image_content}

---

### Output Format:
Generate a JSON array with entries structured as follows:

[
  {
    "question": "What are the steps to complete this task involving financial oversight and job monitoring?",
    "answer": [
      "Step 1: Review the job monitoring data mentioned in the text and refer to the image description of the job monitoring tool. Double-click the job code 'AAS_MC_UPDATE_CLIENT_VFM' as depicted in the image.",
      "Step 2: Verify the job status using the Job Monitor interface (refer to the image description showing the monitoring system). If an issue is detected, consult any warnings or messages displayed on the screen.",
      "Step 3: Perform a VLOOKUP on job codes using the corresponding financial tables shown in the text and the financial statement depicted in the image description. Address any discrepancies, especially those involving missing or invalid fund codes.",
      "Step 4: Cross-reference the extracted job monitoring logs and ensure completion using the UI shown in the job monitor interface described in the image. If the system raises a flag (e.g., ‘CHECK FUND’), investigate by referring to details in the financial document image."
    ],
    "extracted": "Combined content from text and image descriptions for job monitoring, oversight, and financial review."
  }
]

---

### Handling Image Content:

- Always include and reference relevant images in each answer step. Ensure that if an image shows a tool, financial document, or process, it is directly linked to the respective instructions.
- Integrate images logically to provide enhanced clarity or guidance for each procedural step.
- If an image description contains key tables or financial data, explain their importance in relation to the task.

Ensure the LLM generates comprehensive answers by integrating both textual and image-based information seamlessly.
"""





# Procedural Prompt for LLM: Combining Text and Image Content for QA Generation

"""
You are a financial expert with expertise in corporate finance, investment strategies, and accounting. 
Generate distinct and meaningful procedural question-answer pairs by combining the provided paragraph text and image descriptions. 
Questions and answers must be unique, logical, and should seamlessly include relevant image content.

---

Key Instructions:

1. Combine Text and Image Content:
    - For each section of the text, identify and merge the corresponding image description if the image belongs to the same or adjacent page.  
    - Integrate relevant image details such as interface descriptions, financial documents, or job-monitoring tools into the question and answer content.

2. Summarize Relevant Details:
    - Extract tables, data points, financial details, or interface descriptions mentioned in both text and images to provide enhanced answers.  
    - If a step involves navigating through a tool or UI, use image descriptions to explain the user interaction.

3. Maintain Step-Wise Logical Flow:
    - Provide a step-wise explanation of the process, incorporating VLOOKUPs, status updates, fund checks, or any missing/invalid codes.  
    - Use conditions and branching logic (e.g., instructions when a fund code is labeled ‘CHECK FUND’).

4. Include Visual Instructions:
    - If an image shows a table, field, or action, describe how it relates to the corresponding step in the text.  
    - Avoid redundancy by merging image content directly into relevant steps.

5. Integrate Tables, Columns, and Conditions:
    - Reference any specific tables, column headers, job monitors, or data points as described in either the text or image description.  
    - Ensure each step meaningfully contributes to achieving the overall task.

---

### Input Format:

- Text: {paragraph}
- Image descriptions: {image_content}

---

### Output Format:
Generate a JSON array with entries structured as follows:

[
  {
    "question": "What are the steps to complete this task involving financial oversight and job monitoring?",
    "answer": [
      "Step 1: Review the provided job monitoring data, ensuring that relevant jobs (e.g., 'AAS_MC_UPDATE_CLIENT_VFM') are correctly identified in the Job Monitor interface.",
      "Step 2: Refer to the interface as depicted in the image description for the job monitoring tool. Double-click the job and verify the status or completion logs.",
      "Step 3: Use VLOOKUP functions where necessary to match missing or incorrect job codes (cross-referencing image content from the job monitoring tool).",
      "Step 4: Investigate missing or invalid fund codes using any tables or fields described in the financial document (refer to descriptions from the financial document image)."
    ],
    "extracted": "Combined content from text and image descriptions for job monitoring, oversight, and financial review."
  }
]

---

### Handling Image Content:
- Ensure image details (e.g., UI components, financial documents, job monitors) are included as part of relevant procedural steps.
- Where image descriptions describe actions (e.g., double-clicking a job), place the actions in the respective steps.
- Use image-based instructions for visual cues to enhance or clarify actions.

Ensure each answer integrates both text and image content logically and provides comprehensive procedural guidance.
"""







PROCEDURAL PROMPT

You are a financial expert with expertise in corporate finance, investment strategies, and accounting. Questions and answers must be distinct and should not repeat.

Generate up to (num_questions) distinct procedural question-answer pairs by dynamically combining content from **pdf_text** (containing text content) and **image_text** (containing image descriptions). Match image descriptions to the corresponding pages by their **page_number** and integrate them seamlessly into the procedural steps. **Do not specify or mention page numbers in either the questions or the answers.**

---

### Key Instructions:
1. **Dynamic Linking by Page Number:** For each section of text in `pdf_text`, search for any corresponding images in `image_text` based on **page_number**. If matching images are found, combine the image descriptions with the text content.
2. **Text and Image Integration:** Integrate image descriptions naturally within the relevant procedural steps, ensuring the instructions remain cohesive and logical.
3. **Logical Flow and Conditions:** Maintain logical order by including conditions (e.g., VLOOKUP checks, status updates, investigation logs) and instructions for handling issues like missing fund codes or process failures.

---

### Content Requirements:
- **Combine Text and Image Content:** Seamlessly combine text content from `pdf_text` and relevant image descriptions from `image_text` to create detailed procedural instructions.
- **Include Specific Data:** Reference any specific tables, columns, portals, or tools directly where applicable.
- **Branching Logic:** Include instructions for handling errors, conditions, or branching logic as necessary.

---

### Handling Edge Cases:
- **If an image is found for a section:** Include its description within or after the relevant text-based instructions.
- **If no image is found for a section:** Base the instructions solely on the text content.
- **If an image exists without meaningful text:** Create image-based instructions independently.

---

### Output Format:
Generate a JSON array with each entry in the following format:
```json
[
    {
        "question": "What are the steps to complete this process?",
        "answer": [
            "Step 1: [Step description from the combined content].",
            "Step 2: [Next logical step, integrating text and images].",
            "Step n: [Subsequent steps, branching if necessary]."
        ],
        "extracted": "Combined content from text and image descriptions"
    }
]












PROCEDURAL PROMPT

You are a financial expert with expertise in corporate finance, investment strategies, and accounting. Questions and answers must be distinct and should not repeat.

Generate up to (num_questions) distinct procedural question-answer pairs by dynamically combining content from **pdf_text** (containing text content) and **image_text** (containing image descriptions). Match **image descriptions** to the corresponding pages by their **page_number** and integrate them into the procedural steps naturally.

---

### Key Instructions:
1. **Dynamic Linking by Page Number:** For each page in `pdf_text`, search for any corresponding images in `image_text` based on **page_number**. If matching images are found, combine the image descriptions with the text to generate comprehensive procedural questions and answers.
2. **Text and Image Integration:** Integrate image descriptions directly into the relevant text-based instructions. Ensure the image content complements the text logically and flows smoothly.
3. **Logical Flow and Conditions:** Maintain logical order by including conditions (e.g., VLOOKUP checks, status updates) and instructions for handling issues like missing fund codes, incorrect values, or process failures.

---

### Content Requirements:
- **Combine Text and Image Content:** Use text content from `pdf_text` and related image descriptions from `image_text` to create rich procedural instructions.
- **Include Specific Data:** If any specific tables, columns, tools, or portals are mentioned in the text or images, reference them directly.
- **Branching Logic:** Include instructions for handling issues such as missing or incorrect data when needed.

---

### Handling Cases:
- **If an image is found for a page:** Include its description after or within the relevant text instructions.
- **If no image is found for a page:** Base the instructions solely on the text content from that page.
- **If an image exists without meaningful text:** Create image-based instructions independently.

---

### Output Format:
Generate a JSON array with each entry in the following format:
```json
[
    {
        "question": "What are the key procedural steps for page X?",
        "answer": [
            "Step 1: [Step description from combined text and images].",
            "Step 2: [Next step, integrating any conditions or checks].",
            "Step n: [Subsequent steps]."
        ],
        "extracted": "Combined content from text and image descriptions"
    }
]












PROCEDURAL PROMPT

You are a financial expert with expertise in corporate finance, investment strategies, and accounting. Questions and answers must be distinct and should not repeat.

Generate up to (num_questions) distinct procedural question-answer pairs by dynamically combining content from **`text.json`** and **`image.json`**. Match image descriptions to the corresponding pages by their **`page_number`**. Integrate the matched image content with the text to form comprehensive questions and answers.

### Key Instructions for Dynamic Matching:
1. **Page Number Linking:** For each page in `text.json`, search for images in `image.json` with the same **`page_number`**. If matching images are found, incorporate their descriptions directly into the steps.
2. **Text and Image Integration:** When summarizing, combine the image descriptions with the corresponding page text. Place relevant image-based details after or within the related text instructions for context.
3. **Ensure Logical Flow:** Maintain logical order by interleaving text and image-based details naturally within the procedural steps.

### Content Requirements:
- **Combine Text and Image Data:** Use text content from `text.json` and related image descriptions from `image.json` to create rich procedural instructions.
- **Include Tables, Columns, and Tools:** If any specific tables, columns, tools, SWIFT codes, or portals are mentioned in the text or images, directly reference them.
- **Handle Conditions and Branching:** Include branching logic (e.g., VLOOKUP checks, status updates) when required, based on the combined content.

### Example Logic for Integration:
- For **`page_number` X**, include instructions derived from both text and image content.  
  - **Text:** Provide steps for the procedure based on the page’s text content.  
  - **Image:** If an image on the same page is related to the text (e.g., a financial table, job monitoring interface), append its description to the corresponding step.

---

### Output Format:
Generate a JSON array with each entry in the following format:
```json
[
    {
        "question": "What are the steps to achieve or complete this process?",
        "answer": [
            "Step 1: [Integrate text from the page and image content if applicable].",
            "Step 2: [Next logical step, referencing text and image details as necessary].",
            "Step n: [Subsequent steps, incorporating any conditions or branching logic]."
        ],
        "extracted": "Combined content from text and image descriptions on the relevant pages"
    }
]






PROCEDURAL PROMPT

You are a financial expert with expertise in corporate finance, investment strategies, and accounting. Questions and answers must be distinct and should not repeat.

Generate up to (num_questions) distinct procedural question-answer pairs by dynamically combining content from **`text.json`** and **`image.json`**. Match image descriptions to the corresponding pages by their **`page_number`**. Integrate the matched image content with the text to form comprehensive questions and answers.

### Key Instructions for Dynamic Matching:
1. **Page Number Linking:** For each page in `text.json`, search for images in `image.json` with the same **`page_number`**. If matching images are found, incorporate their descriptions directly into the steps.
2. **Text and Image Integration:** When summarizing, combine the image descriptions with the corresponding page text. Place relevant image-based details after or within the related text instructions for context.
3. **Ensure Logical Flow:** Maintain logical order by interleaving text and image-based details naturally within the procedural steps.

### Content Requirements:
- **Combine Text and Image Data:** Use text content from `text.json` and related image descriptions from `image.json` to create rich procedural instructions.
- **Include Tables, Columns, and Tools:** If any specific tables, columns, tools, SWIFT codes, or portals are mentioned in the text or images, directly reference them.
- **Handle Conditions and Branching:** Include branching logic (e.g., VLOOKUP checks, status updates) when required, based on the combined content.

### Example Logic for Integration:
- For **`page_number` X**, include instructions derived from both text and image content.  
  - **Text:** Provide steps for the procedure based on the page’s text content.  
  - **Image:** If an image on the same page is related to the text (e.g., a financial table, job monitoring interface), append its description to the corresponding step.

---

### Output Format:
Generate a JSON array with each entry in the following format:
```json
[
    {
        "question": "What are the steps to achieve or complete this process?",
        "answer": [
            "Step 1: [Integrate text from the page and image content if applicable].",
            "Step 2: [Next logical step, referencing text and image details as necessary].",
            "Step n: [Subsequent steps, incorporating any conditions or branching logic]."
        ],
        "extracted": "Combined content from text and image descriptions on the relevant pages"
    }
]











PROCEDURAL PROMPT

You are a financial expert with expertise in corporate finance, investment strategies, and accounting. Questions and answers must be distinct and should not repeat.

Generate up to (num_questions) distinct procedural question-answer pairs based on the input text and images. Match content from **image descriptions** to the relevant pages using the **page numbers** from both `text.json` and `image.json`. Integrate related text and image content seamlessly into the questions and answers.

Key Instructions:

1. For each page, if an image description is present, combine the textual content with the image description to provide enhanced context.
2. Summarize and extract necessary columns, tables, or data fields from both the text and image content on the corresponding page.
3. Maintain step-wise logical flow, including conditions such as VLOOKUP checks, status updates, missing fund codes, or investigation logs.
4. Ensure that each step clearly describes how to use the combined input, referencing any tables, columns, data points, conditions, or decision-making steps.
5. Use branching logic or conditions when necessary (e.g., what to do if a fund code is missing or labeled as 'CHECK FUND').

### Requirements:

- If specific details (e.g., columns, SWIFT codes, fund codes, case IDs, tabs) are provided in either the text or image content, include them directly in the appropriate steps.
- Include tables, columns, and data points exactly as described in the text or image content.
- Reference portals, URLs, or tools mentioned directly in the process.
- For each page, append relevant image descriptions to the corresponding text section when summarizing.

### Handling Images:
- If an image contains tables, tools, or specific fields, mention them explicitly in the relevant steps.
- Maintain logical flow by placing image-based instructions after their corresponding textual content.
  
### Specific Instructions:

1. Summarize the combined content from both the text and images logically.
2. Avoid redundancy or step repetition, and ensure that each step contributes meaningfully to the overall process.
3. For conditions such as VLOOKUP or identifying missing values, describe the check and the corresponding next step.
4. Include relevant tables, field names, column headers, tabs, or codes exactly as described in either input file.
5. Match image content to its corresponding page in the text and integrate both inputs seamlessly.

### Output Format:
Generate a JSON array with each entry in the following format:
```json
[
    {
        "question": "What are the steps to achieve or complete this process?",
        "answer": [
            "Step 1: [Description of first step, including text and image content if present].",
            "Step 2: [Next logical step in the process, referencing any relevant tables, fields, or conditions].",
            "Step n: [Subsequent steps, incorporating images and text as necessary]."
        ],
        "extracted": "Combined content from text and image descriptions on relevant pages"
    }
]









def extract_text_and_metadata_pagewise(file, output_dir="outputs_sdr"):
    pdf_data = {
        "metadata": {},
        "pages": []
    }

    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Open the PDF file and read metadata
    with open(file, 'rb') as pdf_file:
        reader = PyPDF2.PdfReader(pdf_file)
        pdf_data["metadata"] = {k[1:]: v for k, v in reader.metadata.items()} if reader.metadata else {}

        # Extract text page by page
        for page_index, page in enumerate(reader.pages):
            page_text = page.extract_text() if page.extract_text() else ""
            pdf_data["pages"].append({
                "page_number": page_index + 1,
                "text": page_text.strip()
            })
            print(f"[+] Extracted text from page {page_index + 1}")

    # Save the result to JSON
    save_json(output_dir, pdf_data)

    return pdf_data

def save_json(output_dir, pdf_data):
    json_file_path = os.path.join(output_dir, "text.json")
    with open(json_file_path, "w", encoding="utf-8") as json_file:
        json.dump(pdf_data, json_file, indent=4, ensure_ascii=False)
    print(f"[+] JSON saved to: {json_file_path}")

# Example pipeline usage
pdf_path = r"H:\Question and answer\Fund accounting\example.pdf"
output_folder = r"H:\Question and answer\Fund accounting"

pdf_text = extract_text_and_metadata_pagewise(pdf_path, output_folder)
print("Step 1 Extracted text from PDF completed\n")






# Function to process a single image
def process_image(pdf_file, page_index, image_index, img, output_dir):
    xref = img[0]
    base_image = pdf_file.extract_image(xref)
    image_bytes = base_image["image"]
    image_ext = base_image["ext"]

    # Save the image to the output directory
    image_name = f"image_page{page_index+1}_{image_index}.{image_ext}"
    image_path = os.path.join(output_dir, image_name)
    with open(image_path, "wb") as image_file:
        image_file.write(image_bytes)

    # Convert the image to Base64
    image_data = base64.b64encode(image_bytes).decode("utf-8")

    # Get image description
    description = img_description(image_data)

    # Return page number along with the image details
    return {
        "page_number": page_index + 1,
        "image_name": image_name,
        "description": description
    }



# Function to extract metadata from PDF
def extract_metadata(pdf_file):
    metadata = pdf_file.metadata
    # Clean up metadata (remove '/' from keys)
    return {key.strip('/'): value for key, value in metadata.items() if value}









import json
import PyPDF2
import os

# Function to save JSON data
def save_json(output_dir, pdf_data):
    json_file_path = os.path.join(output_dir, "text_metadata_sop.json")
    with open(json_file_path, "w", encoding="utf-8") as json_file:
        json.dump(pdf_data, json_file, indent=4, ensure_ascii=False)
    print(f"[+] JSON saved to: {json_file_path}")

# Function to extract text and metadata
def extract_text_and_metadata_pagewise(file, output_dir="outputs_sdr"):
    pdf_data = {
        "metadata": {},
        "pages": []
    }

    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Open the PDF file and read metadata
    with open(file, 'rb') as pdf_file:
        reader = PyPDF2.PdfReader(pdf_file)
        pdf_data["metadata"] = {k[1:]: v for k, v in reader.metadata.items()} if reader.metadata else {}

        # Extract text page by page
        for page_index, page in enumerate(reader.pages):
            page_text = page.extract_text() if page.extract_text() else ""
            pdf_data["pages"].append({
                "page_number": page_index + 1,
                "text": page_text.strip()
            })
            print(f"[+] Extracted text from page {page_index + 1}")

    # Save the result to JSON using the custom save_json function
    save_json(output_dir, pdf_data)

    return pdf_data

# Example usage
pdf_path = "example.pdf"
extract_text_and_metadata_pagewise(pdf_path)










import json
import PyPDF2
import os

def save_json(output_dir, data, json_filename):
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, json_filename)
    with open(output_file, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file, indent=4, ensure_ascii=False)
    print(f"[+] JSON saved to: {output_file}")

def extract_text_and_metadata_pagewise(file, output_dir="outputs_sdr"):
    pdf_data = {
        "metadata": {},
        "pages": []
    }

    # Open the PDF file and read metadata
    with open(file, 'rb') as pdf_file:
        reader = PyPDF2.PdfReader(pdf_file)
        pdf_data["metadata"] = {k[1:]: v for k, v in reader.metadata.items()} if reader.metadata else {}

        # Extract text page by page
        for page_index, page in enumerate(reader.pages):
            page_text = page.extract_text() if page.extract_text() else ""
            pdf_data["pages"].append({
                "page_number": page_index + 1,
                "text": page_text.strip()
            })
            print(f"[+] Extracted text from page {page_index + 1}")

    # Determine JSON filename based on PDF name
    base_name = os.path.splitext(os.path.basename(file))[0]
    json_filename = f"{base_name}.json"

    # Save the result to JSON
    save_json(output_dir, pdf_data, json_filename)

    return pdf_data

# Example usage
pdf_path = "example.pdf"
extract_text_and_metadata_pagewise(pdf_path)











import json
import PyPDF2

def extract_text_and_metadata_pagewise(pdf_path, json_output_path):
    pdf_data = {
        "metadata": {},
        "pages": []
    }

    # Open the PDF file
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)

        # Extract metadata
        metadata = reader.metadata
        pdf_data["metadata"] = {k[1:]: v for k, v in metadata.items()}  # Strip leading '/' from keys

        # Extract text page by page
        for page_number, page in enumerate(reader.pages):
            page_text = page.extract_text() if page.extract_text() else ""
            pdf_data["pages"].append({
                "page_number": page_number + 1,
                "text": page_text.strip()
            })

    # Write the result to a JSON file
    with open(json_output_path, 'w', encoding='utf-8') as json_file:
        json.dump(pdf_data, json_file, indent=4, ensure_ascii=False)

# Example usage:
pdf_path = "example.pdf"
json_output_path = "output_pagewise.json"
extract_text_and_metadata_pagewise(pdf_path, json_output_path)





import json
import PyPDF2

def extract_text_and_metadata_from_pdf(pdf_path, json_output_path):
    pdf_text = ""
    metadata = {}

    # Open the PDF file
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)

        # Extract text from all pages
        for page in reader.pages:
            pdf_text += page.extract_text() if page.extract_text() else ""

        # Extract metadata
        metadata = reader.metadata

    # Create a dictionary with text and metadata
    result = {
        "metadata": {k[1:]: v for k, v in metadata.items()},  # Strip leading '/' from keys
        "text": pdf_text
    }

    # Write the result to a JSON file
    with open(json_output_path, 'w', encoding='utf-8') as json_file:
        json.dump(result, json_file, indent=4, ensure_ascii=False)

# Example usage:
pdf_path = "example.pdf"
json_output_path = "output.json"
extract_text_and_metadata_from_pdf(pdf_path, json_output_path)










ChatGPT 4o

You said:
My prompt

Specific Instructions:

-Ensure that any tables, specific data references, or fields mentioned in the input (e.g., column names, codes, identifiers, etc.) are preserved directly in the relevant steps.

-If the input includes details about portals, storage, or tools, embed them directly into the steps.

-Avoid redundancy or step repetition.

-while answering look for image in the same page or next page from which page the text part is coming. If specific data, columns, tables, or conditions are mentioned in the image description, they must be directly included in the answer.

Replace "[specific process/task derived from the input]" with a task-specific description.

For example:

If the task involves updating transaction records, use "the transaction update process."

If related to SWIFT codes, use "the SWIFT code validation process."

Ensure to match the corresponding image with answer text.



"""My image input 
Example
image_description data

"image_name": "image3_1.png",

Street Corporation, specifically a "Browse Calendar File Detail" s End Date: 20190821, 20190731, 20190630\n **Valuation Status:**

{ "description": "The image appears to be a detailed financial document from State 31, 20200630\n**Prior:***\n -**Start Date: 20190821, 20190701, 20180701\n e disclaimers or terms and conditions at the bottom or in a separate section.\n- **Professional Appearance:** The document has a formal and professional appe

"image_name": "image5_2.png",

<

{ "description": "The image depicts a software interface, specifically a job monitoring system, which is divided into two main sections: "Job Status" E pattern, status, time zone, pool size, updated date, and the user who updated it. In The \"Status\" column shows that all jobs have \"succeeded," and th ed.\n The second row is highlighted with a red border, possibly indicating a selected or important entry.\n Similar to the "Job Status\" table, this t oring or scheduling software. \n- The use of colors is minimal, with light blue for headers and section dividers, and red for highlighting important, elements.
"" another in put 
We are providing whole text 
Can u change the specifuc instruction clearly

ChatGPT said:
ChatGPT
Revised Specific Instructions:
Preserve Critical Data: Maintain any tables, columns, data fields, codes, identifiers, or conditions mentioned in the input as they are. Ensure that all referenced details, such as column names, transaction records, or SWIFT codes, are directly embedded into the answer.

Image-Text Correlation: If the task refers to content from an image, ensure that the description and corresponding text from that image are accurately captured. Always match the image content to the relevant section in the answer.

Context-Specific Descriptions: Replace placeholders like "[specific process/task derived from the input]" with clear task descriptions. For example:

If updating records, mention "the record update process".
If related to validation, specify "the SWIFT code validation process".
When managing financial dates or reports, describe "the financial report review process".
Avoid Redundancy: Ensure the steps or instructions do not repeat unnecessarily unless required for clarity.

Multiple Images: If details are spread across multiple images, cross-reference text from the corresponding images on the same or next page. Ensure that any tables or sections found in images are integrated seamlessly into the explanation.

Professional Tone and Presentation: Maintain formal, clear, and concise writing. Ensure any disclaimers, terms, or descriptions from the input are appropriately included without altering their intended meaning.

Example Clarification:
If the input includes a table from a financial report or job monitoring interface, integrate details such as job status, start and end dates, or financial valuations in the answer as follows:

"The job status update process ensures that records reflecting jobs with 'succeeded' status are marked for verification before proceeding."
This should match the relevant image or table content exactly as described.






You said:
Give like as my for. At
ChatGPT said:
ChatGPT
Here’s the revised version of the instructions following your preferred format:

Specific Instructions:
Preserve Critical Data:

Ensure any tables, columns, specific data fields, codes, identifiers, or conditions from the input are directly embedded in the answer without modifications.
Examples include transaction details, column names, financial dates, SWIFT codes, or valuation statuses.
Image-Text Matching:

Look for relevant images from the same or next page of the text reference.
Match and include tables, conditions, or fields from the image description into the answer where applicable.
Task-Specific Descriptions:

Replace placeholders like "[specific process/task derived from the input]" with clear task-based terms:
For updating transaction records, use "the transaction update process".
For validating SWIFT codes, use "the SWIFT code validation process".
For managing financial data, specify "the financial valuation process".
Avoid Redundancy:

Do not repeat steps unless necessary. Ensure concise and non-repetitive instructions.
Multiple Image References:

If the task spans multiple images, ensure proper cross-referencing and integration of data or tables.
Professional Tone:

Maintain formal and structured language. Include terms, conditions, and disclaimers from the input where necessary, keeping their original intent.
Example: If provided a table from a financial report or job monitoring system:

"The job status verification process ensures that jobs marked as 'succeeded' are reviewed for compliance, referencing data such as 'Updated Date' and 'User' from the corresponding image tables."










Search


ChatGPT can make mistakes. Check important info














ChatGPT 4o

You said:
PROCEDURAL PROMPT

You are a financial expert with expertise in corporate finance, investment strategies, and accounting. Questions and answers must be distinct and should not repeat.

Ensure that answers reference or describe relevant images within the image description. If an image contains text, diagrams or key visual elements, include a description of those elements in the answer. If an image supports the context of the answer,summarize its contents concisely and explain how it relates to the question.

Maintain clarity and completeness in response. Generate up to (num_questions) distinct procedural question-answer pairs based on the input text and image description data.

Key Instructions:

Do not treat as the image description data and text as separate, please be treated as a single input.

The image description data (provided as a dictionary) must be summarized,nd relevant details must be incorporated into the process steps where necessary.

Summarize and extracted necessary columns, tables, or data fields from the image content and integrate them naturally into the steps.

Maintain step-wise logical flow, including conditions vlookup check, status updates, missing fun codes, or investigation logs etc. Ensure that each step clearly describes how to use the combined input, referencing any tables, columns, data points, conditions, or

decision-making steps.

If specific details (e.g., columns, SWIFT codes, fund codes, case IDs, or tabs) are provided in either the image or text, they should be included directly in the appropriate steps. I

Use branching logic or conditions when necessary (e.g., what to do if a fund code is missing or labeled as 'CHECK FUND").

Instructions for Process Description:**

Replace "[specific process/task derived from the input]" with a task-specific description.

For example:

If the task involves updating transaction records, use "the transaction update process."

If related to SWIFT codes, use "the SWIFT code validation process."

Requirements:

Incorporate both the text and image content into the procedural steps. If specific data, columns, tables, or conditions are mentioned in the image description, they must be directly included in the answer.

Ensure that each step clearly describes how the data is used, any conditions (e.g., vlookup), and branching
decisions such as what to do when specific values (e..

CHECK FUND) are encountered. Give me the a complete answer which combined both text image summarization.

Preserve data references, column labels (A, C, etc.), tabs, SWIFT codes, fund codes, and case IDs exactly as they appear in the input.

portals, URLs, or specific tools are mentioned in the process, include them directly in the relevant steps. If Each step should logically follow the process of identifying, copying, checking, updating, and escalating tasks.

Preserve any references to columns (A, B, C, etc.), tabs (PD, globaluaxl), tables, fields, and codes exactly as provided in the input.

the process involves conditions (e.g., vlookup, identifying missing information), clearly describe the check and the corresponding next step.

If If decisions or branching logic exists, include clear instructions on what to do in each cases.

Use exact terminology provided in the input, and don't generalize steps unnecessarily.

Specific Instructions:

-Ensure that any tables, specific data references, or fields mentioned in the input (e.g., column names, codes, identifiers, etc.)

are preserved directly in the relevant steps.

-If the input includes details about portals, storage, or tools, embed them directly into the steps.

-Avoid redundancy or step repetition.

Input:

-Text: (paragraph)

-Image description data: The following images are provided in image description: {image_content)

Output format:

[

{{ "question": "What are the steps to achieve or complete this process?",

"answer": [

"Step 1: Access the data via portal_name at data_access_url. Ensure you have the correct credentials: authentication_details.",

"Step 2: Retrieve the relevant files stored at storage_location within database_name.",

"Step n: Process the data according to the required methodology."
]
"extracted":"paragraph, image_content or paragraph/image_content"

]

}},

Ensure the response is a valid JSON array.
"""
" you can instruct the prompt prepare the question combinedly both text and image content treated as single input file and also while preapring the question that corresponding answer if it available or not in image deacription u can check that image include in the answer also 
ChatGPT said:
ChatGPT
Understood! Here's a structured JSON output combining the procedural prompt's requirements with an integrated question-answer approach, derived from both the provided text and image descriptions:

[
    {
        "question": "What are the steps to validate and update SWIFT transactions with missing or incorrect codes?",
        "answer": [
            "Step 1: Access the portal at 'portal_name' using the provided link 'data_access_url' and ensure you have the necessary credentials.",
            "Step 2: Navigate to the 'Pending SWIFT Validation' tab, and retrieve the transaction file stored in 'Global_SWIFT_Transactions.xls'.",
            "Step 3: Open the file, and cross-check Column A (Transaction ID) and Column C (Fund Code) against the external master reference file using a VLOOKUP function.",
            "Step 4: If Column C displays 'CHECK FUND', escalate the record by logging it into the 'Error Log' sheet within the same file and tag it with the current date in Column D (Escalation Date).",
            "Step 5: For transactions with valid SWIFT codes, mark them as 'VALIDATED' in Column E, and save the changes.",
            "Step 6: Upload the updated file to the designated portal location 'Updated_SWIFT_Records'. Confirm successful upload by checking the system-generated status message.",
            "Step 7: Notify the respective team via email, including the updated SWIFT file and the logged error cases."
        ],
        "extracted": "Combined content from paragraph and image description."
    },
    {
        "question": "How should missing fund codes (displaying 'CHECK FUND') be handled within the validation process?",
        "answer": [
            "Step 1: Identify rows with 'CHECK FUND' in Column C of the transaction data.",
            "Step 2: Document these cases in the 'Error Log' tab, specifying the corresponding Transaction ID, Fund Code, and validation date.",
            "Step 3: Investigate the missing fund code using the 'Fund Reference Database', located in the 'globaluaxl' tab.",
            "Step 4: If the correct code is found, update the original SWIFT record and revalidate using VLOOKUP.",
            "Step 5: For unresolved cases, escalate by creating a case ID in the 'Fund Investigation Portal' and assign it to the Fund Review Team.",
            "Step 6: Monitor for status updates within the investigation log and ensure that any corrected cases are reprocessed promptly."
        ],
        "extracted": "Integrated from both input text and image description."
    },
    {
        "question": "What is the branching logic if errors are encountered during the SWIFT validation process?",
        "answer": [
            "Branch 1: If VLOOKUP identifies no errors, proceed to Step 6 and mark the record as 'VALIDATED'.",
            "Branch 2: If Column C indicates 'CHECK FUND', document the case in the 'Error Log' and follow the fund investigation steps.",
            "Branch 3: If a missing or mismatched SWIFT code is found, escalate it to the SWIFT Review Team using 'Case Tracker.xls'.",
            "Branch 4: For system-related upload errors, retry the upload using the backup file from 'Backup_SWIFT_Records'. If the issue persists, escalate to IT Support."
        ],
        "extracted": "Branching derived from both text and image references."
    }
]
This JSON ensures the integration of procedural steps, conditions, branching logic, and the necessary data extraction from both the text and image content as specified.






You said:
U can give me the prompt according to my requirements 
ChatGPT said:
ChatGPT
Here’s the prompt tailored to meet your requirements perfectly:

Prompt:

You are a financial expert with expertise in corporate finance, investment strategies, and accounting. Questions and answers must be distinct and should not repeat.

Ensure that answers reference or describe relevant images within the image description. If an image contains text, diagrams, or key visual elements, include a description of those elements in the answer. If an image supports the context of the answer, summarize its contents concisely and explain how it relates to the question.

Maintain clarity and completeness in the response. Generate up to (num_questions) distinct procedural question-answer pairs based on the input text and image description data.

Key Instructions:

Treat the text and image description as a single input. Do not separate them during question-answer creation.
Summarize and extract necessary columns, tables, or data fields from the image content and integrate them naturally into the steps.
Maintain a logical, step-wise flow, including VLOOKUP checks, status updates, missing fund codes, or investigation logs. Ensure that each step clearly describes how to use the combined input, referencing any tables, columns, data points, conditions, or decision-making steps.
If specific details (e.g., columns, SWIFT codes, fund codes, case IDs, or tabs) are provided in either the text or image, they should be included directly in the steps.
Use branching logic or conditions when necessary (e.g., what to do if a fund code is missing or labeled as ‘CHECK FUND’).
Replace [specific process/task derived from the input] with task-specific descriptions:
For example:

If the task involves updating transaction records, use "the transaction update process."
If related to SWIFT codes, use "the SWIFT code validation process."
Procedural Steps Format:

Ensure data references (columns, tabs, codes) are preserved exactly as provided in the input.
If the process involves conditions (e.g., VLOOKUP, identifying missing information), describe checks and corresponding next steps clearly.
If decisions or branching logic exist, include detailed instructions on handling each case.
Embed any portals, URLs, or tools mentioned in the input directly into the steps.
Input Format:

Text: (paragraph)
Image description data: {image_content}
Output Format (JSON):

[
    {
        "question": "What are the steps to achieve or complete this process?",
        "answer": [
            "Step 1: [Access the relevant portal and navigate to specified location.]",
            "Step 2: [Retrieve files as indicated and perform validation checks using provided conditions.]",
            "Step n: [Document errors, update cases, and escalate issues when required.]"
        ],
        "extracted": "[paragraph, image_content, or paragraph/image_content]"
    }
]
Additional Notes:

Ensure that tables, fields, columns, and portals mentioned in the input are preserved exactly in the output.
Handle any specific conditions or missing values as described in the input text and image.
Provide distinct, non-repetitive questions with clear answers for every generated pair.











ChatGPT can make mistakes. Check important 







**Instructions for Process Description:**  
- Replace **"[specific process/task derived from the input]"** with a task-specific description. For example:  
  - If the task involves updating transaction records, use "the transaction update process."  
  - If related to SWIFT codes, use "the SWIFT code validation process."  







prompt_template = (
        "You are provided with descriptions of financial documents that may contain details such as company "
        "names, dates, transaction codes, tables, levels, cycle dates, and other structured information. "
        "Your task is to generate a comprehensive summary that highlights the following:\n"
        "- Key entities such as company names, transaction codes, and funds.\n"
        "- Important dates and timelines (start dates, end dates, cycle dates).\n"
        "- Any detailed levels and their corresponding statuses (BL, S1, S2, etc.).\n"
        "- Calculations, tables, or formulas related to financial transactions.\n"
        "- Additional metadata or structured information present in the descriptions.\n\n"
        "Descriptions:\n{descriptions}\n\n"
        "Summarize this information comprehensively while ensuring no key structured details are missed."
    )





import json
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

def generate_summary_from_json(json_data, output_file="summary_output.json"):
    # Step 1: Extract and concatenate descriptions
    descriptions = " ".join([entry["description"] for entry in json_data])

    # Step 2: Configure the Azure OpenAI Chat model
    llm = AzureChatOpenAI(
        temperature=0.5,
        api_version="azure_openai_api_version",  # Replace with your API version
        azure_openai_api_key="YOUR_AZURE_API_KEY",  # Replace with your API key
        azure_openai_base="azure_openai_endpoint",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    # Step 3: Define the prompt template
    prompt_template = (
        "Summarize the following financial document descriptions into a concise summary "
        "highlighting key themes and recurring information:\n\n{descriptions}"
    )

    # Create the prompt template
    prompt = PromptTemplate(
        input_variables=["descriptions"],
        template=prompt_template
    )

    # Step 4: Create the LLM chain
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        # Step 5: Generate the response using the LLM chain
        response = llm_chain.predict(descriptions=descriptions)

        # Format the output
        summary_data = {
            "Summary": response.strip()
        }

        # Step 6: Save to JSON file
        with open(output_file, "w") as outfile:
            json.dump(summary_data, outfile, indent=4)

        print(f"Summary successfully saved to {output_file}")

        # Return the output as a dictionary for optional use
        return summary_data
    
    except Exception as e:
        print(f"Error generating summary: {e}")
        return {"Error": str(e)}

# Sample JSON input
json_data = [
    {"image_name": "image1_1.png", "description": "The document in question appears to be a professional financial statement or bank statement."},
    {"image_name": "image3_1.png", "description": "The image depicts a professional financial document, likely a bank statement, with a structured format."},
    {"image_name": "image3_2.png", "description": "The image depicts a professional financial document, likely a bank statement or account summary."},
    {"image_name": "image4_1.png", "description": "The image depicts a professional document titled 'Global Transactions and Controls.'"},
    {"image_name": "image4_2.png", "description": "The image depicts a section of a financial report interface, likely from a banking or investment platform."},
    {"image_name": "image5_1.png", "description": "The image depicts a detailed table, likely from a financial document or a transaction report."}
]

# Call the function and display the result
result = generate_summary_from_json(json_data)






import json
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

def generate_summary_from_json(json_data):
    # Step 1: Extract and concatenate descriptions
    descriptions = " ".join([entry["description"] for entry in json_data])

    # Step 2: Configure the Azure OpenAI Chat model
    llm = AzureChatOpenAI(
        temperature=0.5,
        api_version="azure_openai_api_version",  # Replace with your API version
        azure_openai_api_key="YOUR_AZURE_API_KEY",  # Replace with your API key
        azure_openai_base="azure_openai_endpoint",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    # Step 3: Define the prompt template
    prompt_template = (
        "Summarize the following financial document descriptions into a concise summary "
        "highlighting key themes and recurring information:\n\n{descriptions}"
    )

    # Create the prompt template
    prompt = PromptTemplate(
        input_variables=["descriptions"],
        template=prompt_template
    )

    # Step 4: Create the LLM chain
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        # Step 5: Generate the response using the LLM chain
        response = llm_chain.predict(descriptions=descriptions)

        # If response contains valid JSON, return it; otherwise, return text as is
        return {"Summary": response.strip()}
    
    except Exception as e:
        print(f"Error generating summary: {e}")
        return {"Error": str(e)}

# Sample JSON input
json_data = [
    {"image_name": "image1_1.png", "description": "The document in question appears to be a professional financial statement or bank statement."},
    {"image_name": "image3_1.png", "description": "The image depicts a professional financial document, likely a bank statement, with a structured format."},
    {"image_name": "image3_2.png", "description": "The image depicts a professional financial document, likely a bank statement or account summary."},
    {"image_name": "image4_1.png", "description": "The image depicts a professional document titled 'Global Transactions and Controls.'"},
    {"image_name": "image4_2.png", "description": "The image depicts a section of a financial report interface, likely from a banking or investment platform."},
    {"image_name": "image5_1.png", "description": "The image depicts a detailed table, likely from a financial document or a transaction report."}
]

# Call the function and display the result
result = generate_summary_from_json(json_data)
print(json.dumps(result, indent=4))





import json
import openai

def generate_summary_from_json(json_data, api_key, model="text-davinci-003", max_tokens=150):
    """
    Generate a summary from image descriptions in JSON using an LLM API.

    Args:
    - json_data: List of dictionaries containing 'image_name' and 'description'.
    - api_key: OpenAI API key as a string.
    - model: The LLM model to use (default: 'text-davinci-003').
    - max_tokens: Max tokens for the response (default: 150).

    Returns:
    - A dictionary containing the summary.
    """
    # Step 1: Extract and concatenate all descriptions
    descriptions = " ".join([entry["description"] for entry in json_data])

    # Step 2: Set OpenAI API key
    openai.api_key = api_key

    # Step 3: Define the prompt
    prompt = f"Summarize the following financial document descriptions into a concise summary highlighting key themes and recurring information:\n\n{descriptions}"

    # Step 4: Call the OpenAI GPT model
    response = openai.Completion.create(
        engine=model,
        prompt=prompt,
        max_tokens=max_tokens,
        temperature=0.5
    )

    # Step 5: Extract the summary
    summary = response['choices'][0]['text'].strip()

    # Return the final summary as JSON
    return {
        "Summary": summary
    }

# Sample JSON input
json_data = [
    {"image_name": "image1_1.png", "description": "The document in question appears to be a professional financial statement or bank statement."},
    {"image_name": "image3_1.png", "description": "The image depicts a professional financial document, likely a bank statement, with a structured format."},
    {"image_name": "image3_2.png", "description": "The image depicts a professional financial document, likely a bank statement or account summary."},
    {"image_name": "image4_1.png", "description": "The image depicts a professional document titled 'Global Transactions and Controls.'"},
    {"image_name": "image4_2.png", "description": "The image depicts a section of a financial report interface, likely from a banking or investment platform."},
    {"image_name": "image5_1.png", "description": "The image depicts a detailed table, likely from a financial document or a transaction report."}
]

# Call the function
api_key = 'YOUR_OPENAI_API_KEY'  # Replace with your API key
result = generate_summary_from_json(json_data, api_key)

# Display the result
print(json.dumps(result, indent=4))






import json
import re

def extract_and_reorder(image_text):
    # Custom sorting function to extract and sort by primary and secondary numbers
    def extract_numeric_key(entry):
        # Extract primary and secondary numbers (e.g., "image2_3.png" -> [2, 3])
        match = re.findall(r'\d+', entry['image_name'])
        return [int(num) for num in match] if match else [float('inf')]  # Handle cases where no digits are found

    # Sort based on the extracted numeric keys
    sorted_images = sorted(image_text, key=extract_numeric_key)

    return sorted_images

# Example input
image_text = [
    {"image_name": "image1_1.png", "description": "The document in question appears to be a professional financial statement."},
    {"image_name": "image10_1.png", "description": "The image depicts a software interface, likely a dashboard for managing."},
    {"image_name": "image2_1.png", "description": "The image depicts a financial document, likely a report for review."},
    {"image_name": "image3_2.png", "description": "The image shows a detailed document section, likely for transaction records."},
    {"image_name": "image2_2.png", "description": "The image is a continuation of the financial report, showing structured data."},
    {"image_name": "image1_2.png", "description": "The second section of the financial statement."},
    {"image_name": "image2_3.png", "description": "The image contains a summary table for year-end financial data."},
    {"image_name": "image3_1.png", "description": "The image shows a summary of account activities for the previous quarter."}
]

# Reorder the images
sorted_output = extract_and_reorder(image_text)

# Specify the file location where the JSON will be saved
file_location = "/path/to/your/output/sorted_images.json"

# Save the sorted output to a JSON file
with open(file_location, "w") as outfile:
    json.dump(sorted_output, outfile, indent=4)

print(f"Sorted output saved to: {file_location}")




import json
import re

def extract_and_reorder(image_text):
    # Custom sorting function to extract and sort by primary and secondary numbers
    def extract_numeric_key(entry):
        # Extract primary and secondary numbers (e.g., "image2_3.png" -> [2, 3])
        match = re.findall(r'\d+', entry['image_name'])
        return [int(num) for num in match] if match else [float('inf')]  # Handle cases where no digits are found

    # Sort based on the extracted numeric keys
    sorted_images = sorted(image_text, key=extract_numeric_key)

    # Convert to JSON for output
    return json.dumps(sorted_images, indent=4)

# Example input
image_text = [
    {"image_name": "image1_1.png", "description": "The document in question appears to be a professional financial statement."},
    {"image_name": "image10_1.png", "description": "The image depicts a software interface, likely a dashboard for managing."},
    {"image_name": "image2_1.png", "description": "The image depicts a financial document, likely a report for review."},
    {"image_name": "image3_2.png", "description": "The image shows a detailed document section, likely for transaction records."},
    {"image_name": "image2_2.png", "description": "The image is a continuation of the financial report, showing structured data."},
    {"image_name": "image1_2.png", "description": "The second section of the financial statement."},
    {"image_name": "image2_3.png", "description": "The image contains a summary table for year-end financial data."},
    {"image_name": "image3_1.png", "description": "The image shows a summary of account activities for the previous quarter."}
]

# Get the reordered output
print(extract_and_reorder(image_text))









import json

def extract_and_reorder(image_text):
    # Sort the list to ensure "image1_1.png" appears first
    sorted_images = sorted(image_text, key=lambda x: (x['image_name'] != 'image1_1.png', x['image_name']))

    # Convert to JSON for output
    return json.dumps(sorted_images, indent=4)

# Simulating the result from the function and reordering
print(extract_and_reorder(image_text))








import re
import json

# Input data (cleaned version)
input_data = '''{
"image_name": "image4_2.png",
"description": "The image depicts a section of a financial report interface, likely from a banking or investment document."

"image_name": "image1_1.png",
"description": "The document in question appears to be a professional financial statement or bank statement."

"image_name": "image3_1.png",
"description": "The image depicts a professional financial document, likely a bank statement."

"image_name": "image3_2.png",
"description": "The image depicts a professional financial document, likely a bank statement."

"image_name": "image4_1.png",
"description": "The image depicts a professional document titled 'Global Transactions and Controls.'"
}'''

def extract_and_reorder(input_text):
    # Regular expression to extract image_name and description pairs
    pattern = r'"image_name":\s*"([^"]+)",\s*"description":\s*"([^"]+)"'
    matches = re.findall(pattern, input_text)

    # Sort to place "image1_1.png" first, and maintain the order for others
    matches.sort(key=lambda x: (x[0] != "image1_1.png", x[0]))

    # Building output with no modification to descriptions
    output = [{"image_name": image_name, "description": description.strip()} for image_name, description in matches]
    return json.dumps(output, indent=4)

# Generate the ordered JSON output
print(extract_and_reorder(input_data))









PROCEDURAL PROMPT = """
You are a financial expert with expertise in corporate finance, investment strategies, and accounting.

Generate up to (num_questions) distinct procedural question-answer pairs using the **combined context of text and image descriptions** provided below.

Treat the input as one unified source of information, not as separate components. Ensure no detail is missed from either the text or image description.  

Use the following format:

"question": "What are the steps to achieve or complete this process?",

"answer": [
    "Step 1: [Detailed step referencing relevant text/image data]",
    "Step 2: [Detailed step using text/image data with conditions or decisions]",
    "...",
    "Final step: Ensure outputs meet requirements by checking [text/image details]."
]

**Input Context:**  
- **Text Content:** {paragraph}  
- **Image Description:** {image_content}

Key Instructions:  
1. Incorporate both the text and image content directly into procedural steps.  
2. Reference specific details from columns, tables, or fields mentioned in either input.  
3. If data references are found only in the image (e.g., fund codes, column labels), ensure they are used exactly in the corresponding steps.  
4. Use branching logic when needed (e.g., checks for missing values, special conditions).  
5. Clearly describe how data is retrieved, processed, checked, updated, or escalated.  
6. Avoid any repetition of steps and provide concise procedural instructions.  

Ensure your output adheres to this format with unified text/image references.

Example Output:

"question": "What are the steps to process missing SWIFT codes in a transaction report?",

"answer": [
    "Step 1: Access the transaction report in Tab 'globaluaxl' under the SWIFT_Transactions sheet.",
    "Step 2: Identify missing SWIFT codes using Column A, which lists case IDs requiring validation.",
    "Step 3: Cross-check the case IDs against the provided fund codes listed in {image_content}. If 'CHECK FUND' is displayed, escalate to supervisor.",
    "Final Step: Ensure all updates are reflected in Tab PD under column C before final submission."
]

**Extracted Context:** "{paragraph, image_content}"
"""









Key Instructions:
Do not treat the text and image descriptions as separate inputs. They are to be processed together as a single source of information to ensure no detail is missed.
Ensure that each step clearly describes how to use the combined input, referencing any tables, columns, data points, conditions, or decision-making steps.
If specific details (e.g., columns, SWIFT codes, fund codes, case IDs, or tabs) are provided in either the image or text, they should be included directly in the appropriate steps.
Use branching logic or conditions when necessary (e.g., what to do if a fund code is missing or labeled as 'CHECK FUND').
INPUT FORMAT:
Combined Input: (Provide the combined content from text and images here.)
OUTPUT FORMAT:
json
Copy code
[
    {
        "question": "What are the steps to process unadvised cash items and allocate them correctly?",
        "answer": [
            "Step 1: Identify unadvised cash items using the combined data from the table (refer to the 'Fund Code', 'Case ID', and 'Purchase Amount' columns as described in the input). Copy the identified items into the 'PD' tab.",
            "Step 2: Check the status of the cash items and update comments where necessary. Use data from both text and image descriptions (e.g., transaction details, SWIFT codes, and amounts).",
            "Step 3: Copy all the downloaded data to the 'globaluaxl' tab, covering columns C through T. Include references to transaction-specific columns (e.g., 'Fund Code', 'Case ID', and 'Branch').",
            "Step 4: In the 'globaluaxl' tab, perform a vlookup on column A to identify entries labeled 'CHECK FUND'. Cross-reference this with details from both the image and text input (e.g., 'MT2322000067 SYD', 'AUD', or 'SWIFT codes').",
            "Step 5: For entries labeled 'CHECK FUND', raise an IMT log for the GCO investigation team to allocate the cash to funds or match with non-receipt cash.",
            "Step 6: If a fund code is not identifiable, search for the corresponding SWIFT reference in IBS using combined details (e.g., from the table and additional fields).",
            "Step 7: Send an email to the related team for clarification, providing any missing fund codes or transaction details extracted from the combined input (e.g., 'SWIFT code AU1230804075552', 'Case ID').",
            "Step 8: Request the GCO investigation team to allocate the cash based on further investigation or follow up as needed."
        ]
    }
]
Clarified Instructions:
Treat the text and image content as a single unified input.
Preserve and reference any tables, column names, or transaction details mentioned in either input.
Ensure the final output includes all necessary steps, decisions, and checks while providing clear references to data extracted from the combined source.
Would this approach meet your requirements perfectly?













ChatGPT can make mistakes. 
Key Requirements:
Incorporate both the text and image information into the procedural steps. If specific data, columns, tables, or conditions are mentioned in the image description, they must be directly included in the answer.
Ensure that each step clearly describes how the data is used, any conditions (e.g., vlookup), and branching decisions such as what to do when specific values (e.
CHECK FUND') are encountered.
Preserve data references, column labels (A, C, etc.), tabs, SWIFT codes, fund codes, and case IDs exactly as they appear in the input.
If portals, URLs, or specific tools are mentioned in the process, include them directly in the relevant steps.





Requirements:
Each step should logically follow the process of identifying, copying, checking, updating, and escalating tasks.
Preserve any references to columns (A, B, C, etc.), tabs (PD, globaluaxl), tables, fields, and codes exactly as provided in the input.
If the process involves conditions (e.g., vlookup, identifying missing information), clearly describe the check and the corresponding next step.
If decisions or branching logic exists, include clear instructions on what to do in each case.
Use exact terminology provided in the input, and don’t generalize steps unnecessarily.
If any portal, link, or storage location is mentioned, directly include it in the appropriate step.
INPUT FORMAT:
Image Description: {image_content}
Text: {paragraph}
OUTPUT FORMAT:
json
Copy code
[
    {
        "question": "What are the steps to process unadvised cash items and allocate them to funds?",
        "answer": [
            "Step 1: Identify unadvised cash items based on fund codes and copy the data into the 'PD' tab.",
            "Step 2: Check the status of the cash items and update the comments where necessary.",
            "Step 3: Copy all the downloaded data to the 'globaluaxl' tab, covering columns C to T.",
            "Step 4: In the 'globaluaxl' tab, use vlookup to check column A for entries labeled 'CHECK FUND'.",
            "Step 5: For entries where 'CHECK FUND' is identified, raise an IMT log for the GCO investigation team to allocate the cash to funds, if possible, or match with non-receipt cash.",
            "Step 6: If a fund code is not identifiable, search for the corresponding SWIFT reference in IBS, review the additional columns for transaction details, and contact the related team via email for clarification on the fund code.",
            "Step 7: Request the GCO investigation team to allocate the cash after further investigation, if necessary."
        ]
    }
]
Example Breakdown of Table References (Preserve table-like formatting where necessary):
Fund Code Check Example:
Column A: 'CHECK FUND' (Use vlookup to identify)
Relevant transaction data spans from columns C to T, ensure the appropriate checks are made.
Cross-check cases like AU1230804075552 (or similar) using both SWIFT and fund identifiers as indicated in the table.
This version will guarantee table-specific instructions, checks, and decision points are retained without losing any of the context.













ChatGPT can make mistakes. Check important info.









You are a financial expert with expertise in corporate finance, investment strategies, and accounting.

Generate distinct procedural question-answer pairs from the provided input text and image descriptions.

Requirements:
Questions must focus on the process, methods, or logical sequences.
Answers should break down the entire process step-by-step, ensuring each step is complete and does not miss critical details.
Steps should clearly explain tasks like copying data, verifying conditions, or taking decisions, with clear references to any columns, fields, or data points mentioned.
Specific Instructions:
Ensure that any tables, specific data references, or fields mentioned in the input (e.g., column names, codes, identifiers, etc.) are preserved directly in the relevant steps.
If the input includes details about portals, storage, or tools, embed them directly into the steps.
Avoid redundancy or step repetition.
INPUT:
Image Description: {image_content}
Text: {paragraph}
OUTPUT FORMAT:
json
Copy code
[
    {
        "question": "What are the steps to complete this process?",
        "answer": [
            "Step 1: Identify unadvised cash items by referencing fund codes and copying the data into the 'PD' tab.",
            "Step 2: Check the status and update any relevant comments. Copy all downloaded data to the 'globaluaxl' tab, covering columns C through T.",
            "Step 3: In the 'globaluaxl' tab, review column A to identify any instances labeled 'CHECK FUND' via vlookup.",
            "Step 4: Raise an IMT log for the GCO investigation team if identifiable cash can be allocated to specific funds or matched with non-receipt cash.",
            "Step 5: If the fund code is not identifiable, search for the corresponding SWIFT code in IBS, review the additional columns for details about the transaction nature, and notify the relevant team via email.",
            "Step 6: Request the GCO investigation team to allocate the cash based on further investigation if necessary."
        ]
    }
]
By rephrasing the prompt with the above structure, this should meet your expected step-wise breakdown seamlessly.
















Ensure:
Questions focus on processes, methods, or sequences of steps.
Answers provide detailed, step-by-step instructions with clear descriptions for each step, ensuring no step is missed.
Each question and answer must be unique and should not repeat previous questions.
If fewer distinct questions are possible, return only the available ones.
Analyze both the text and images together to generate procedural questions.
Include unrelated procedural questions (e.g., unrelated topics like cooking, weather) if the input contains off-topic content.
Do not generate questions for strikethrough text.
Ensure that when an address, portal, or URL is relevant to a step, it is included directly in that step rather than listed separately.
Input:
json
Copy code
{
    "text": "{paragraph}",
    "image_description_data": "{image_content}",
    "portal": "{portal_name}",
    "url": "{data_access_url}",
    "storage_address": "{storage_location}",
    "database_name": "{database_name}",
    "access_credentials": "{authentication_details}",
    "strikethrough_content": "{strikethrough_text}"
}
Output Format (Valid JSON Array):
json
Copy code
[
    {
        "question": "What are the steps to achieve or complete this process?",
        "answer": [
            "Step 1: Access the data via {portal_name} at {data_access_url}. Ensure you have the correct credentials: {authentication_details}.",
            "Step 2: Retrieve the relevant files stored at {storage_location} within {database_name}.",
            "Step 3: Process the data according to the required methodology."
        ]
    }
]













ChatGPT can make mistakes. Check important








import os
import pandas as pd

def consolidate_excel_files(base_folder, output_file):
    # Create an empty DataFrame to store consolidated data
    consolidated_data = pd.DataFrame()

    # Walk through the base folder and its subfolders
    for root, dirs, files in os.walk(base_folder):
        for file in files:
            if file.endswith('.xlsx') or file.endswith('.xls'):  # Check for Excel files
                file_path = os.path.join(root, file)
                print(f"Processing file: {file_path}")
                try:
                    # Read the Excel file into a DataFrame
                    df = pd.read_excel(file_path)
                    # Add a column to track the file name (optional)
                    df['Source_File'] = file
                    # Append the data to the consolidated DataFrame
                    consolidated_data = pd.concat([consolidated_data, df], ignore_index=True)
                except Exception as e:
                    print(f"Error reading {file_path}: {e}")

    # Save the consolidated data to an output Excel file
    consolidated_data.to_excel(output_file, index=False)
    print(f"Consolidated data saved to {output_file}")

# Specify the base folder and output file path
base_folder = r"path_to_your_folders"  # Replace with the folder containing Excel files
output_file = r"path_to_output_file/consolidated.xlsx"  # Replace with the output file path

# Call the function to consolidate Excel files
consolidate_excel_files(base_folder, output_file)








def generate_questions_by_all_types(paragraph, image_data, total_questions, chunk_size=10):
    """
    Generate questions of all types in parallel.
    """
    # Define question types and their corresponding prompt templates
    question_types_and_prompts = {
        "Factual": "Generate factual questions from the given text.",
        "Procedural": "Generate procedural questions from the given text.",
        "Conceptual": "Generate conceptual questions from the given text.",
        "Inferential": "Generate inferential questions from the given text.",
        "Reasoning-based": "Generate reasoning-based questions from the given text.",
    }

    all_questions = []
    question_counts = {q_type: 0 for q_type in question_types_and_prompts}

    # Ensure paragraph has sufficient content
    if not paragraph or len(paragraph.split()) < total_questions:
        print("Warning: The input paragraph may not have enough content to generate the requested number of questions.")

    # Adjust chunk size dynamically
    chunk_size = min(chunk_size, max(1, total_questions // len(paragraph.split("."))))

    # Helper function to generate questions for a single type
    def generate_questions_for_type(question_type, prompt, questions_to_generate):
        try:
            # Generate questions for the current type
            questions = generate_questions_in_chunks(
                paragraph,
                image_data,
                question_type,
                questions_to_generate,
                prompt,
                chunk_size
            )
            # Remove duplicates
            questions = remove_duplicates(questions)
            print(f"Generated {len(questions)} questions for {question_type}")
            return question_type, questions
        except Exception as e:
            print(f"Error generating questions for {question_type}: {e}")
            return question_type, []

    # Divide the total questions equally among question types
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    remaining_questions = total_questions

    with ThreadPoolExecutor() as executor:
        # Submit tasks to generate questions for each type
        futures = {
            executor.submit(
                generate_questions_for_type,
                question_type,
                prompt,
                questions_per_type
            ): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        # Collect results from the executor
        for future in futures:
            try:
                question_type, questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                    continue
                all_questions.extend(questions)
                question_counts[question_type] += len(questions)
                remaining_questions -= len(questions)
            except Exception as e:
                print(f"Error processing {futures[future]}: {e}")

    # Log question counts
    for q_type, count in question_counts.items():
        print(f"{q_type} questions: {count}")

    # Warn if the total is less than requested
    if len(all_questions) < total_questions:
        print(f"Warning: Only {len(all_questions)} questions generated out of the requested {total_questions}.")

    # Ensure the total number of questions matches the requested amount
    return all_questions[:total_questions]









def generate_questions_in_chunks(paragraph, image_data, question_type, total_questions, prompt_template, chunk_size=10):
    """
    Generate questions in chunks to avoid exceeding token limits.
    """
    all_questions = []

    for i in range(0, total_questions, chunk_size):
        try:
            # Generate questions for the current chunk
            chunk_questions = generate_questions(paragraph, image_data, question_type, chunk_size, prompt_template)
            
            # Ensure chunk_questions is a valid list
            if not chunk_questions or not isinstance(chunk_questions, list):
                print(f"Warning: No questions generated for {question_type} in chunk {i // chunk_size + 1}")
                continue
            
            # Remove duplicates in the current chunk
            chunk_questions = remove_duplicates(chunk_questions)
            
            # Append unique questions to all_questions
            all_questions.extend(chunk_questions)

        except Exception as e:
            print(f"Error generating chunk for {question_type} (chunk {i // chunk_size + 1}): {e}")
            continue

    # Ensure all_questions is a valid list
    return all_questions if all_questions else []







from concurrent.futures import ThreadPoolExecutor
import math

# Placeholder functions for prompts and processing
FACTUAL_PROMPT = "Factual question generation prompt"
PROCEDURAL_PROMPT = "Procedural question generation prompt"
CONCEPTUAL_PROMPT = "Conceptual question generation prompt"
INFERENTIAL_PROMPT = "Inferential question generation prompt"
REASONING_PROMPT = "Reasoning-based question generation prompt"

def generate_questions_in_chunks(paragraph, image_data, question_type, questions_to_generate, prompt, chunk_size):
    # Mock implementation for generating questions
    print(f"Generating questions for {question_type} with {questions_to_generate} questions.")
    questions = [{"question": f"Sample {question_type} question {i}", "answer": f"Sample {question_type} answer {i}"} for i in range(questions_to_generate)]
    print(f"Generated {len(questions)} questions for {question_type}.")
    return questions if questions else []

def remove_duplicates(questions):
    if not questions:
        return []  # Return an empty list if input is None or empty
    print(f"Removing duplicates from {len(questions)} questions.")
    seen = set()
    unique_questions = []
    for question in questions:
        if question["question"] not in seen:
            unique_questions.append(question)
            seen.add(question["question"])
    print(f"{len(unique_questions)} unique questions remain after deduplication.")
    return unique_questions

def calculate_tokens(text):
    # Mock implementation for token calculation
    return len(text.split())

def generate_questions_by_all_types(paragraph, image_data, total_questions, chunk_size=10):
    # Define question types and corresponding prompts
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    # Initialize variables
    all_questions = []
    question_counts = {q_type: 0 for q_type in question_types_and_prompts}

    # Check if inputs are valid
    if not paragraph or not image_data:
        print("Error: Paragraph or image data is empty!")
        return []

    # Helper function for generating questions for a specific type
    def generate_questions_for_type(question_type, prompt, questions_to_generate):
        try:
            questions = generate_questions_in_chunks(
                paragraph,
                image_data,
                question_type,
                questions_to_generate,
                prompt,
                chunk_size
            )
            if questions is None:
                print(f"Error: generate_questions_in_chunks returned None for {question_type}")
                questions = []
            questions = remove_duplicates(questions)
            if questions is None:
                print(f"Error: remove_duplicates returned None for {question_type}")
                questions = []
            print(f"Generated {len(questions)} questions for {question_type}.")
            return question_type, questions
        except Exception as e:
            print(f"Error generating questions for {question_type}: {e}")
            return question_type, []

    # Divide the total questions equally among question types
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        # Submit tasks to generate questions for each type
        futures = {
            executor.submit(
                generate_questions_for_type,
                question_type,
                prompt,
                questions_per_type
            ): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        # Collect results as they are completed
        for future in futures:
            try:
                question_type, questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                # Add metadata and append to all_questions
                for qa in questions:
                    question_text = qa["question"]
                    answer_text = qa["answer"]
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": question_text,
                        "Answer": answer_text,
                        "Question Length (Chars)": len(question_text),
                        "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                        "Question Tokens": calculate_tokens(question_text),
                        "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else "".join(answer_text)),
                    })
                    question_counts[question_type] += 1
            except Exception as e:
                print(f"Error processing {futures[future]}: {e}")

    # Log final question counts
    for q_type, count in question_counts.items():
        print(f"{q_type} questions: {count}")

    # Ensure the final output is exactly total_questions
    return all_questions[:total_questions]

# Example usage
paragraph = "This is a sample paragraph for question generation."
image_data = "Sample image data"  # Replace with actual image data
total_questions = 50
result = generate_questions_by_all_types(paragraph, image_data, total_questions, chunk_size=10)
print("Generated Questions:")
print(result)











from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import json


# Utility: Extract valid JSON from response
def extract_valid_json(response):
    try:
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON: {e}")
        return []


# Utility: Remove duplicate questions and answers
def remove_duplicates(questions):
    """
    Remove duplicate questions and answers.
    """
    seen_questions = set()
    seen_answers = set()
    unique_questions = []

    for qa in questions:
        question_text = qa["question"]
        answer_text = str(qa["answer"])  # Convert list-type answers to strings for comparison

        if question_text not in seen_questions and answer_text not in seen_answers:
            seen_questions.add(question_text)
            seen_answers.add(answer_text)
            unique_questions.append(qa)

    return unique_questions


# Utility: Calculate token count for a text
def calculate_tokens(text):
    """
    Placeholder function for token calculation. Update this with your tokenization logic.
    """
    return len(text.split())


# Function: Generate questions in chunks
def generate_questions_in_chunks(paragraph, image_data, question_type, total_questions, prompt_template, chunk_size=10):
    """
    Generate questions in chunks to avoid exceeding token limits.
    """
    all_questions = []

    for start in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(
                paragraph=paragraph,
                image_content=image_data,
                question_type=question_type,
                num_questions=min(chunk_size, total_questions - start),
                prompt_template=prompt_template
            )
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue

    return all_questions


# Function: Generate questions using LLM
def generate_questions(paragraph, image_content, question_type, num_questions, prompt_template):
    """
    Generate questions using Azure OpenAI with a structured prompt template.
    """
    # Initialize Azure OpenAI LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="azure_openai_api_version",  # Replace with your Azure OpenAI API version
        azure_endpoint="azure_openai_endpoint",  # Replace with your Azure OpenAI endpoint
        deployment_name="deployment_name"        # Replace with your deployment name
    )

    # Create the prompt template
    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions", "question_type", "image_content"],
        template=prompt_template
    )

    # Create the LLM chain
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        # Generate questions using the LLM chain
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions),
            question_type=str(question_type),
            image_content=str(image_content)
        )
        # Extract and return valid JSON from the response
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []


# Main Function: Generate questions by all types
def generate_questions_by_all_types(paragraph, image_data, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, ensuring uniqueness.
    """
    question_types_and_prompts = {
        "Factual": "FACTUAL_PROMPT",
        "Procedural": "PROCEDURAL_PROMPT",
        "Conceptual": "CONCEPTUAL_PROMPT",
        "Inferential": "INFERENTIAL_PROMPT",
        "Reasoning-based": "REASONING_PROMPT",
    }

    all_questions = []

    # Parallel processing for question generation
    def process_question_type(question_type, prompt_template):
        return question_type, generate_questions_in_chunks(
            paragraph, image_data, question_type, total_questions, prompt_template, chunk_size
        )

    with ThreadPoolExecutor() as executor:
        future_to_question_type = {
            executor.submit(process_question_type, question_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(future_to_question_type):
            question_type, questions = future.result()
            for qa in questions:
                question_text = qa["question"]
                answer_text = qa["answer"]
                all_questions.append({
                    "Question Type": question_type,
                    "Question": question_text,
                    "Answer": answer_text,
                    "Question Length (Chars)": len(question_text),
                    "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                    "Question Tokens": calculate_tokens(question_text),
                    "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else "".join(answer_text)),
                })

            # Stop early if the desired number of questions is reached
            if len(all_questions) >= total_questions:
                break

    # Remove duplicates and return the top N unique questions
    all_questions = remove_duplicates(all_questions)
    return all_questions[:total_questions]












def generate_questions_in_chunks(paragraph, image_data, question_type, total_questions, prompt_template, chunk_size=10):
    """
    Generate questions in chunks to avoid exceeding token limits.

    Args:
        paragraph (str): Input paragraph for question generation.
        image_data (any): Image data for context (if required).
        question_type (str): Type of question to generate (e.g., "Factual", "Inferential").
        total_questions (int): Total number of questions to generate.
        prompt_template (str): Template or prompt for generating questions.
        chunk_size (int): Number of questions to generate per chunk.

    Returns:
        list: A list of generated questions.
    """
    all_questions = []

    for start in range(0, total_questions, chunk_size):
        try:
            # Generate questions for the current chunk
            chunk_questions = generate_questions(
                paragraph=paragraph,
                image_data=image_data,
                question_type=question_type,
                total_questions=min(chunk_size, total_questions - start),
                prompt_template=prompt_template
            )
            # Add the questions to the overall list
            all_questions.extend(chunk_questions)
        except Exception as e:
            # Handle errors during chunk generation
            print(f"Error generating chunk for {question_type}: {e}")
            continue

    return all_questions






from concurrent.futures import ThreadPoolExecutor
import math

def generate_questions_by_all_types(paragraph, image_data, total_questions, chunk_size=10):
    # Define question types and corresponding prompts
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    # Initialize variables
    all_questions = []
    question_counts = {q_type: 0 for q_type in question_types_and_prompts}

    # Define a helper function for parallel processing
    def generate_questions_for_type(question_type, prompt, questions_to_generate):
        questions = generate_questions_in_chunks(
            paragraph,
            image_data,
            question_type,
            questions_to_generate,
            prompt,
            chunk_size
        )
        if questions is None:
            print(f"Error: generate_questions_in_chunks returned None for {question_type}")
            questions = []
        questions = remove_duplicates(questions)
        if questions is None:
            print(f"Error: remove_duplicates returned None for {question_type}")
            questions = []
        return question_type, questions

    # Divide the total questions equally among question types
    questions_per_type = max(1, math.ceil(total_questions / len(question_types_and_prompts)))

    with ThreadPoolExecutor() as executor:
        # Submit tasks to generate questions for each type in parallel
        futures = {
            executor.submit(
                generate_questions_for_type,
                question_type,
                prompt,
                questions_per_type
            ): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        # Collect results as they are completed
        for future in futures:
            try:
                question_type, questions = future.result()
                if not questions:
                    print(f"No questions generated for {question_type}")
                # Add metadata and append to all_questions
                for qa in questions:
                    question_text = qa["question"]
                    answer_text = qa["answer"]
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": question_text,
                        "Answer": answer_text,
                        "Question Length (Chars)": len(question_text),
                        "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                        "Question Tokens": calculate_tokens(question_text),
                        "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else "".join(answer_text)),
                    })
                    # Update counters
                    question_counts[question_type] += 1
            except Exception as e:
                print(f"Error processing {futures[future]}: {e}")

    # Log final question counts per type
    for q_type, count in question_counts.items():
        print(f"{q_type} questions: {count}")

    # Ensure the final output is exactly total_questions
    return all_questions[:total_questions]









from concurrent.futures import ThreadPoolExecutor, as_completed

def generate_questions_in_chunks(paragraph, image_data, question_type, total_questions, prompt, chunk_size):
    # Placeholder for your existing function that generates questions in chunks
    pass

def remove_duplicates(questions):
    # Placeholder for your duplicate removal logic
    return list({q['question']: q for q in questions}.values())

def calculate_tokens(text):
    # Placeholder for your token calculation logic
    return len(text.split())

def generate_questions_by_all_types(paragraph, image_data, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": "FACTUAL_PROMPT",
        "Procedural": "PROCEDURAL_PROMPT",
        "Conceptual": "CONCEPTUAL_PROMPT",
        "Inferential": "INFERENTIAL_PROMPT",
        "Reasoning-based": "REASONING_PROMPT",
    }

    all_questions = []

    def process_question_type(question_type, prompt):
        questions = generate_questions_in_chunks(paragraph, image_data, question_type, total_questions, prompt, chunk_size)
        return question_type, questions

    with ThreadPoolExecutor() as executor:
        future_to_question_type = {
            executor.submit(process_question_type, question_type, prompt): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        for future in as_completed(future_to_question_type):
            question_type, questions = future.result()
            for qa in questions:
                question_text = qa["question"]
                answer_text = qa["answer"]
                all_questions.append({
                    "Question Type": question_type,
                    "Question": question_text,
                    "Answer": answer_text,
                    "Question Length (Chars)": len(question_text),
                    "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                    "Question Tokens": calculate_tokens(question_text),
                    "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else "".join(answer_text)),
                })
            
            # Break if the desired number of questions is reached
            if len(all_questions) >= total_questions:
                break

    # Remove duplicates and return the top N questions
    all_questions = remove_duplicates(all_questions)
    return all_questions[:total_questions]








from concurrent.futures import ThreadPoolExecutor
import math

def generate_questions_by_all_types(paragraph, image_data, total_questions, chunk_size=10):
    # Define question types and corresponding prompts
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    # Initialize variables
    all_questions = []
    question_counts = {q_type: 0 for q_type in question_types_and_prompts}  # Track questions per type

    # Define a helper function for parallel processing
    def generate_questions_for_type(question_type, prompt, questions_to_generate):
        questions = generate_questions_in_chunks(
            paragraph,
            image_data,
            question_type,
            questions_to_generate,
            prompt,
            chunk_size
        )
        questions = remove_duplicates(questions)
        print(f"{question_type} questions generated: {len(questions)}")  # Debug log
        return question_type, questions

    # Divide the total questions equally among question types
    questions_per_type = math.ceil(total_questions / len(question_types_and_prompts))

    with ThreadPoolExecutor() as executor:
        # Submit tasks to generate questions for each type in parallel
        futures = {
            executor.submit(
                generate_questions_for_type,
                question_type,
                prompt,
                questions_per_type
            ): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        # Collect results as they are completed
        for future in futures:
            question_type, questions = future.result()
            # Add metadata and append to all_questions
            for qa in questions:
                question_text = qa["question"]
                answer_text = qa["answer"]
                all_questions.append({
                    "Question Type": question_type,
                    "Question": question_text,
                    "Answer": answer_text,
                    "Question Length (Chars)": len(question_text),
                    "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                    "Question Tokens": calculate_tokens(question_text),
                    "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else "".join(answer_text)),
                })
                # Update counters
                question_counts[question_type] += 1

    # Log final question counts per type
    for q_type, count in question_counts.items():
        print(f"{q_type} questions: {count}")

    # Ensure the final output is exactly total_questions
    return all_questions[:total_questions]









import os
import fitz
import base64
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

# Function to describe an image
def img_description(image_data):
    message = [
        {
            "type": "text",
            "text": "Describe an image in detail, including its overall structure, such as headers, footers, and sections, along with the placement of the logo, name, address, and contact information. Include background features like watermarks, barcodes, or security marks. Focus on tables within the document that display account details, transaction records, dates, descriptions, debit/credit amounts, and balances, specifying the number of rows and columns, headers, borders, alignment, and any highlights like bold totals or shaded cells. Mention account details such as the account holder's name, account number, branch name, and IFSC code, along with details about account types or interest rates. Provide information on transaction lists or tables, including details like dates, descriptions, transaction types (credit/debit), amounts, and balances, highlighting any significant entries. Include additional elements like handwritten notes, signatures (manual or digital), stamps, or seals for authentication, as well as reference numbers, document IDs, or case numbers for tracking. Describe the formatting, including font styles, alignment, and the use of bold or italicized text for emphasis. Mention any legal or formal information, such as disclaimers, terms, and conditions, or privacy policies, often found at the bottom of the document, and note the use of formal language and financial terminology. Finally, describe the professional appearance of the document, including the use of colors (e.g., black-and-white or highlights) and whether it appears printed or scanned."
        },
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},
        },
    ]
    response = "AzureChatOpenAI"  # Placeholder for Azure OpenAI function call
    return response  # Replace with actual function call logic

# Function to save JSON data
def save_json(output_dir, image_descriptions):
    json_file_path = os.path.join(output_dir, "image_data_isvc.json")
    with open(json_file_path, "w") as json_file:
        json.dump(image_descriptions, json_file, indent=4)

# Function to process a single image
def process_image(pdf_file, page_index, image_index, img, output_dir):
    xref = img[0]
    base_image = pdf_file.extract_image(xref)
    image_bytes = base_image["image"]
    image_ext = base_image["ext"]

    # Save the image to the outputs folder
    image_name = f"image{page_index+1}_{image_index}.{image_ext}"
    image_path = os.path.join(output_dir, image_name)
    with open(image_path, "wb") as image_file:
        image_file.write(image_bytes)

    # Convert the image to Base64
    image_data = base64.b64encode(image_bytes).decode("utf-8")

    # Get image description
    description = img_description(image_data)

    return {"image_name": image_name, "description": description}

# Main function for processing PDF
def extract_text_from_pdf_images(file, output_dir="outputs_sop"):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    pdf_file = fitz.open(file)
    image_descriptions = []

    # Using ThreadPoolExecutor for parallel processing
    with ThreadPoolExecutor() as executor:
        futures = []

        for page_index in range(len(pdf_file)):
            page = pdf_file.load_page(page_index)
            image_list = page.get_images(full=True)

            if image_list:
                print(f"[+] Found a total of {len(image_list)} images on page {page_index}")
                for image_index, img in enumerate(image_list, start=1):
                    futures.append(executor.submit(process_image, pdf_file, page_index, image_index, img, output_dir))
            else:
                print(f"[1] No images found on page {page_index}")

        # Collect results from parallel tasks
        for future in as_completed(futures):
            image_descriptions.append(future.result())

    save_json(output_dir, image_descriptions)
    return image_descriptions

# Usage
file = r"h:\Question and answer\Fund accounting\(Resubmit_1025)36. VFMC AAS25 SOP 5.0.pdf"
image_text = extract_text_from_pdf_images(file)
print(image_text)











import tiktoken

def calculate_tokens(text, model="gpt-3.5-turbo"):
    """
    Calculate the number of tokens in a given text using tiktoken.

    Args:
        text (str): Input text.
        model (str): Model name to determine the encoding (default: "gpt-3.5-turbo").

    Returns:
        int: Number of tokens in the text.
    """
    try:
        # Choose encoding based on the model
        if model in ["gpt-3.5-turbo", "gpt-4"]:
            encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")  # This will use cl100k_base
        elif model == "text-davinci-003":
            encoding = tiktoken.get_encoding("p50k_base")
        elif model == "text-davinci-002":
            encoding = tiktoken.get_encoding("r50k_base")
        else:
            raise ValueError(f"Unsupported model: {model}")

        # Calculate token count
        return len(encoding.encode(text))
    
    except Exception as e:
        print(f"Error calculating tokens: {e}")
        return None

# Example usage
t = calculate_tokens("hi", model="gpt-3.5-turbo")
print(t)








import tiktoken

def calculate_tokens(text, model="gpt-3.5-turbo", encoding_path=None):
    """
    Calculate the number of tokens in a given text using tiktoken.

    Args:
        text (str): Input text.
        model (str): Model name to determine the encoding (default: "gpt-3.5-turbo").
        encoding_path (str): Optional path to a manually downloaded encoding file.

    Returns:
        int: Number of tokens in the text.
    """
    try:
        # Load encoding from a manually downloaded file if specified
        if encoding_path:
            with open(encoding_path, "r") as f:
                encoding = tiktoken.load(f.read())
        else:
            # Use predefined encodings based on the model
            if model in ["gpt-3.5-turbo", "gpt-4"]:
                encoding = tiktoken.get_encoding("cl100k_base")
            elif model == "text-davinci-003":
                encoding = tiktoken.get_encoding("p50k_base")
            elif model == "text-davinci-002":
                encoding = tiktoken.get_encoding("r50k_base")
            else:
                raise ValueError(f"Unsupported model: {model}")
        
        # Calculate token count
        return len(encoding.encode(text))
    
    except Exception as e:
        print(f"Error calculating tokens: {e}")
        return None

# Example usage with a manually downloaded encoding file
manual_path = r"c:\Users\P872643\Downloads\cl100k_base.tiktoken"  # Update this path as needed
t = calculate_tokens("hi", encoding_path=manual_path)
print(t)






This project focuses on automating the analysis of financial and legal documents by leveraging Azure and AI technologies. It securely retrieves documents from Azure Blob Storage, organizes and structures their content, and uses Azure Document Intelligence to parse key elements. Regex filtering and Azure OpenAI models are employed to extract, refine, and interpret critical information like management fees and key terms. The workflow ensures accurate, context-specific insights and simplified interpretations for informed decision-making. By combining secure access, advanced AI, and structured processing, the system delivers a comprehensive and efficient document analysis solution.





You said:
project name
ChatGPT said:
ChatGPT
IntelliDoc Analyzer





You said:
give me a cutting edge like
ChatGPT said:
ChatGPT
NeuralEdge Document Intelligence
















def extract_text_from_pdf_images(file, output_dir="outputs_sop"):
    # Ensure the outputs directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Function to process a single page
    def process_page(page_index):
        page = pdf_file.load_page(page_index)  # Load the page
        image_list = page.get_images(full=True)  # Get images on the page
        
        # Check if images are found
        if not image_list:
            print(f"[!] No images found on page {page_index}")
            return {"page_index": page_index, "descriptions": []}

        print(f"[+] Found a total of {len(image_list)} images on page {page_index}")
        page_descriptions = []

        for image_index, img in enumerate(image_list, start=1):
            # Get the XREF of the image
            xref = img[0]
            
            # Extract the image bytes
            base_image = pdf_file.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]  # Get the image extension
            
            # Save the image to the outputs folder
            image_name = f"image_{page_index + 1}_{image_index}.{image_ext}"
            image_path = os.path.join(output_dir, image_name)
            with open(image_path, "wb") as image_file:
                image_file.write(image_bytes)
            print(f"[+] Image saved as {image_path}")
            
            # Convert the image to Base64
            image_data = base64.b64encode(image_bytes).decode("utf-8")
            
            # Append image description
            page_descriptions.append({
                "image_name": image_name,
                "description": img_description(image_data)
            })
        
        return {"page_index": page_index, "descriptions": page_descriptions}

    # Function to describe the image
    def img_description(image_data):
        # Simulated description (replace with actual API call if needed)
        return f"Simulated description for image data: {image_data[:30]}..."

    # Save JSON data
    def save_json(output_dir, image_descriptions):
        json_file_path = os.path.join(output_dir, "image_data_isvc.json")
        with open(json_file_path, "w") as json_file:
            json.dump(image_descriptions, json_file, indent=4)

    # Open the PDF file
    pdf_file = fitz.open(file)
    image_descriptions = []

    # Process pages in parallel
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(process_page, range(len(pdf_file))))

    # Sort results by page index to maintain sequence
    results.sort(key=lambda x: x["page_index"])

    # Collect sorted descriptions
    for result in results:
        image_descriptions.extend(result["descriptions"])

    # Save the combined descriptions to JSON
    save_json(output_dir, image_descriptions)
    return image_descriptions

# Usage example
# file = r"h:\Question and answer\Fund accounting (Resubmit_1025)36. VFMC AAS25 SOP 5.0.pdf"
# image_text = extract_text_from_pdf_images(file)













import os
import fitz  # PyMuPDF
import base64
import json
from concurrent.futures import ThreadPoolExecutor

def extract_text_from_pdf_images(file, output_dir="outputs_sop"):
    # Ensure the outputs directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Function to process a single page
    def process_page(page_index):
        page = pdf_file.load_page(page_index)  # Load the page
        image_list = page.get_images(full=True)  # Get images on the page
        
        # Check if images are found
        if not image_list:
            print("[!] No images found on page", page_index)
            return []

        print(f"[+] Found a total of {len(image_list)} images on page {page_index}")
        page_descriptions = []

        for image_index, img in enumerate(image_list, start=1):
            # Get the XREF of the image
            xref = img[0]
            
            # Extract the image bytes
            base_image = pdf_file.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]  # Get the image extension
            
            # Save the image to the outputs folder
            image_name = f"image_{page_index + 1}_{image_index}.{image_ext}"
            image_path = os.path.join(output_dir, image_name)
            with open(image_path, "wb") as image_file:
                image_file.write(image_bytes)
            print(f"[+] Image saved as {image_path}")
            
            # Convert the image to Base64
            image_data = base64.b64encode(image_bytes).decode("utf-8")
            
            # Append image description
            page_descriptions.append({
                "image_name": image_name,
                "description": img_description(image_data)
            })
        
        return page_descriptions

    # Function to describe the image
    def img_description(image_data):
        message = {
            "type": "text",
            "text": "Describe an image in detail, including its overall structure, such as headers, footers, and sections, along with authentication, as well as reference numbers, document IDs, or case numbers for tracking. Describe the formatting, including font styles, alignment, etc."
        }
        response = {
            "type": "image_url",
            "image_url": f"data:image/jpeg;base64,{image_data}"
        }
        # Simulated response (replace with actual API call if needed)
        return f"Simulated description for image data: {image_data[:30]}..."

    # Save JSON data
    def save_json(output_dir, image_descriptions):
        json_file_path = os.path.join(output_dir, "image_data_isvc.json")
        with open(json_file_path, "w") as json_file:
            json.dump(image_descriptions, json_file, indent=4)

    # Open the PDF file
    pdf_file = fitz.open(file)
    image_descriptions = []

    # Process pages in parallel
    with ThreadPoolExecutor() as executor:
        results = executor.map(process_page, range(len(pdf_file)))

    # Collect results
    for page_descriptions in results:
        if page_descriptions:
            image_descriptions.extend(page_descriptions)

    # Save the combined descriptions to JSON
    save_json(output_dir, image_descriptions)
    return image_descriptions

# Usage example
# file = r"h:\Question and answer\Fund accounting (Resubmit_1025)36. VFMC AAS25 SOP 5.0.pdf"
# image_text = extract_text_from_pdf_images(file)











import os
import fitz  # PyMuPDF
import base64
import json
from concurrent.futures import ThreadPoolExecutor

# Ensure the output directory exists
def ensure_output_dir(output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

# Generate a description for an image
def img_description(image_data, api_endpoint, api_key):
    """Describe an image using the Azure ChatGPT model."""
    import requests

    # API request payload
    message = {
        "messages": [
            {
                "role": "user",
                "content": f"Describe this image in detail: data:image/jpeg;base64,{image_data}"
            }
        ],
        "temperature": 0.5,
    }

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    response = requests.post(api_endpoint, json=message, headers=headers)

    if response.status_code == 200:
        return response.json()["choices"][0]["message"]["content"]
    else:
        raise Exception(f"API Request Failed: {response.status_code}, {response.text}")

# Save the JSON file
def save_json(output_dir, image_descriptions):
    json_file_path = os.path.join(output_dir, "image_data.json")
    with open(json_file_path, "w") as json_file:
        json.dump(image_descriptions, json_file, indent=4)

# Process a single image
def process_image(image, page_index, image_index, pdf_file, output_dir, api_endpoint, api_key):
    xref = image[0]
    base_image = pdf_file.extract_image(xref)
    image_bytes = base_image["image"]
    image_ext = base_image["ext"]

    # Save the image locally
    image_name = f"image{page_index+1}_{image_index}.{image_ext}"
    image_path = os.path.join(output_dir, image_name)

    with open(image_path, "wb") as image_file:
        image_file.write(image_bytes)

    # Convert to Base64
    image_data = base64.b64encode(image_bytes).decode("utf-8")
    
    # Get the description
    description = img_description(image_data, api_endpoint, api_key)

    return {"image_name": image_name, "description": description}

# Extract text from PDF images
def extract_text_from_pdf_images(file, output_dir="outputs_sop", api_endpoint="", api_key=""):
    ensure_output_dir(output_dir)

    pdf_file = fitz.open(file)
    image_descriptions = []

    def process_page(page_index):
        page = pdf_file.load_page(page_index)
        image_list = page.get_images(full=True)

        if not image_list:
            return []

        results = []
        for image_index, img in enumerate(image_list, start=1):
            result = process_image(img, page_index, image_index, pdf_file, output_dir, api_endpoint, api_key)
            results.append(result)
        return results

    # Use threading for parallel processing
    with ThreadPoolExecutor() as executor:
        all_results = executor.map(process_page, range(len(pdf_file)))

    # Flatten the results
    for page_results in all_results:
        image_descriptions.extend(page_results)

    # Save to JSON
    save_json(output_dir, image_descriptions)

    return image_descriptions

# Usage Example
file = r"path_to_pdf.pdf"
api_endpoint = "https://your_azure_openai_endpoint/v1/chat/completions"
api_key = "your_api_key"

try:
    image_text = extract_text_from_pdf_images(file, output_dir="outputs_sop", api_endpoint=api_endpoint, api_key=api_key)
    print(image_text)
except Exception as e:
    print(f"Error: {e}")










import os
import fitz  # PyMuPDF
import base64
import json
from concurrent.futures import ThreadPoolExecutor

def ensure_output_dir(output_dir):
    """Ensure the outputs directory exists."""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

def img_description(image_data):
    """Mockup for describing an image in detail."""
    # Replace this with actual logic if necessary (e.g., using an AI model).
    return f"Description of image with data size: {len(image_data)} bytes"

def save_json(output_dir, image_descriptions):
    """Save image descriptions to a JSON file."""
    json_file_path = os.path.join(output_dir, "image_data.json")
    with open(json_file_path, "w") as json_file:
        json.dump(image_descriptions, json_file, indent=4)

def process_image(image, page_index, image_index, pdf_file, output_dir):
    """Process a single image: save and describe it."""
    xref = image[0]
    base_image = pdf_file.extract_image(xref)
    image_bytes = base_image["image"]
    image_ext = base_image["ext"]

    image_name = f"image{page_index+1}_{image_index}.{image_ext}"
    image_path = os.path.join(output_dir, image_name)

    # Save the image
    with open(image_path, "wb") as image_file:
        image_file.write(image_bytes)

    # Convert to Base64
    image_data = base64.b64encode(image_bytes).decode("utf-8")
    description = img_description(image_data)

    return {"image_name": image_name, "description": description}

def extract_text_from_pdf_images(file, output_dir="outputs_sop"):
    """Extract text and descriptions from PDF images."""
    ensure_output_dir(output_dir)

    pdf_file = fitz.open(file)
    image_descriptions = []

    def process_page(page_index):
        """Process all images on a single page."""
        page = pdf_file.load_page(page_index)
        image_list = page.get_images(full=True)

        if not image_list:
            return []

        results = []
        for image_index, img in enumerate(image_list, start=1):
            result = process_image(img, page_index, image_index, pdf_file, output_dir)
            results.append(result)
        return results

    # Use threading for processing pages in parallel
    with ThreadPoolExecutor() as executor:
        all_results = executor.map(process_page, range(len(pdf_file)))

    # Flatten results from all pages
    for page_results in all_results:
        image_descriptions.extend(page_results)

    # Save descriptions to JSON
    save_json(output_dir, image_descriptions)

    return image_descriptions

# Usage
# file = r"path/to/your/document.pdf"
# image_text = extract_text_from_pdf_images(file)










from concurrent.futures import ThreadPoolExecutor
import math

def generate_questions_by_all_types(paragraph, image_data, total_questions, chunk_size=10):
    # Define question types and corresponding prompts
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    # Initialize variables
    all_questions = []
    question_counts = {q_type: 0 for q_type in question_types_and_prompts}  # Track questions per type

    # Define a helper function for parallel processing
    def generate_questions_for_type(question_type, prompt, questions_to_generate):
        questions = generate_questions_in_chunks(
            paragraph,
            image_data,
            question_type,
            questions_to_generate,
            prompt,
            chunk_size
        )
        questions = remove_duplicates(questions)
        return question_type, questions

    # Divide the total questions equally among question types
    questions_per_type = math.ceil(total_questions / len(question_types_and_prompts))

    with ThreadPoolExecutor() as executor:
        # Submit tasks to generate questions for each type in parallel
        futures = {
            executor.submit(
                generate_questions_for_type, 
                question_type, 
                prompt, 
                questions_per_type
            ): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        # Collect results as they are completed
        for future in futures:
            question_type, questions = future.result()

            # Add metadata and append to all_questions
            for qa in questions:
                question_text = qa["question"]
                answer_text = qa["answer"]
                all_questions.append({
                    "Question Type": question_type,
                    "Question": question_text,
                    "Answer": answer_text,
                    "Question Length (Chars)": len(question_text),
                    "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                    "Question Tokens": calculate_tokens(question_text),
                    "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else "".join(answer_text))
                })

            # Update counters
            question_counts[question_type] += len(questions)

    # Ensure the final output is exactly `total_questions`
    return all_questions[:total_questions]











import multiprocessing as mp


def generate_questions_for_type(paragraph, image_data, question_type, num_questions, prompt):
    """
    Generates questions for a single question type.
    """
    questions = generate_questions_in_chunks(
        paragraph,
        image_data,
        question_type,
        num_questions,
        prompt,
        chunk_size=10  # Customize chunk size if necessary
    )
    return remove_duplicates(questions)


def worker_task(args):
    """
    Worker task for multiprocessing. Accepts arguments as a tuple.
    """
    paragraph, image_data, question_type, num_questions, prompt = args
    try:
        questions = generate_questions_for_type(paragraph, image_data, question_type, num_questions, prompt)
        return {
            "question_type": question_type,
            "questions": questions
        }
    except Exception as e:
        print(f"Error processing {question_type}: {e}")
        return {
            "question_type": question_type,
            "questions": []
        }


def generate_questions_by_all_types_multiprocessing(paragraph, image_data, total_questions):
    """
    Generates questions of all types using multiprocessing.
    """
    # Define question types and corresponding prompts
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    # Divide total questions among question types
    num_types = len(question_types_and_prompts)
    questions_per_type = total_questions // num_types
    remaining_questions = total_questions % num_types

    # Prepare arguments for each process
    tasks = [
        (
            paragraph,
            image_data,
            question_type,
            questions_per_type + (1 if i < remaining_questions else 0),  # Distribute leftovers
            prompt
        )
        for i, (question_type, prompt) in enumerate(question_types_and_prompts.items())
    ]

    # Use multiprocessing Pool to parallelize tasks
    with mp.Pool(processes=mp.cpu_count()) as pool:
        results = pool.map(worker_task, tasks)

    # Combine results from all processes
    all_questions = []
    for result in results:
        question_type = result["question_type"]
        for qa in result["questions"]:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"],
                "Question Length (Chars)": len(qa["question"]),
                "Answer Length (Chars)": len(str(qa["answer"])) if isinstance(qa["answer"], str) else sum(len(str(a)) for a in qa["answer"]),
                "Question Tokens": calculate_tokens(qa["question"]),
                "Answer Tokens": calculate_tokens(qa["answer"] if isinstance(qa["answer"], str) else "".join(qa["answer"]))
            })

    # Return the top N unique questions
    return all_questions[:total_questions]










from concurrent.futures import ThreadPoolExecutor, as_completed

def generate_questions_by_type(paragraph, image_data, question_type, prompt, chunk_size, token_limit):
    """
    Generate questions for a specific type with token limit handling.
    """
    questions = []
    total_tokens_used = 0
    while True:
        # Generate questions in chunks
        chunk_questions = generate_questions_in_chunks(
            paragraph, image_data, question_type, chunk_size, prompt, chunk_size
        )
        
        # Break if no more questions are generated
        if not chunk_questions:
            break

        # Calculate tokens for generated questions
        for qa in chunk_questions:
            question_text = qa["question"]
            answer_text = qa["answer"]
            question_tokens = calculate_tokens(question_text)
            answer_tokens = calculate_tokens(answer_text if isinstance(answer_text, str) else "".join(answer_text))
            total_tokens = question_tokens + answer_tokens

            # Stop processing if token limit is reached
            if total_tokens_used + total_tokens > token_limit:
                return questions  # Return all questions generated so far

            # Append the question and update token usage
            questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text,
                "Question Length (Chars)": len(question_text),
                "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                "Question Tokens": question_tokens,
                "Answer Tokens": answer_tokens
            })
            total_tokens_used += total_tokens

    return questions


def generate_questions_parallel(paragraph, image_data, total_questions, chunk_size, token_limit):
    """
    Generate questions across all types in parallel while respecting token limits.
    """
    # Define question types and corresponding prompts
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    # Use ThreadPoolExecutor for parallel processing
    all_questions = []
    with ThreadPoolExecutor() as executor:
        # Submit tasks for parallel processing
        futures = {
            executor.submit(
                generate_questions_by_type,
                paragraph,
                image_data,
                question_type,
                prompt,
                chunk_size,
                token_limit
            ): question_type
            for question_type, prompt in question_types_and_prompts.items()
        }

        # Collect results as they complete
        for future in as_completed(futures):
            try:
                questions = future.result()  # Get questions from completed thread
                all_questions.extend(questions)  # Append results to all_questions
                if len(all_questions) >= total_questions:
                    break  # Stop if total question limit is reached
            except Exception as e:
                print(f"Error generating {futures[future]} questions: {e}")

    # Ensure the final output is exactly `total_questions`
    return all_questions[:total_questions]












from concurrent.futures import ThreadPoolExecutor, as_completed

def generate_questions_for_type(paragraph, image_data, question_type, prompt, remaining_questions, chunk_size):
    """
    Generate questions for a specific question type.
    """
    questions = generate_questions_in_chunks(
        paragraph,
        image_data,
        question_type,
        remaining_questions,
        prompt,
        chunk_size
    )
    return {
        "question_type": question_type,
        "questions": remove_duplicates(questions)
    }

def generate_questions_parallel(paragraph, image_data, total_questions, chunk_size=10):
    # Define question types and corresponding prompts
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    # Remaining questions to generate
    remaining_questions = total_questions
    all_questions = []

    # Use ThreadPoolExecutor for parallel processing
    with ThreadPoolExecutor(max_workers=len(question_types_and_prompts)) as executor:
        futures = []
        for question_type, prompt in question_types_and_prompts.items():
            futures.append(executor.submit(
                generate_questions_for_type,
                paragraph,
                image_data,
                question_type,
                prompt,
                remaining_questions,
                chunk_size
            ))

        # Collect results as they complete
        for future in as_completed(futures):
            result = future.result()
            question_type = result["question_type"]
            questions = result["questions"]

            # Add questions to the main list
            for qa in questions:
                question_text = qa["question"]
                answer_text = qa["answer"]
                all_questions.append({
                    "Question Type": question_type,
                    "Question": question_text,
                    "Answer": answer_text,
                    "Question Length (Chars)": len(question_text),
                    "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                    "Question Tokens": calculate_tokens(question_text),
                    "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else "".join(answer_text))
                })

            # Update remaining questions
            remaining_questions -= len(questions)

    # Return the top N unique questions
    return all_questions[:total_questions]











def generate_questions_by_all_types(paragraph, image_data, total_questions, chunk_size=10):
    # Define question types and corresponding prompts
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    # Initialize variables
    all_questions = []
    types_covered = set()  # Track covered types
    remaining_questions = total_questions  # Track remaining questions to generate

    # Iterate over question types in a loop until all are covered
    while remaining_questions > 0 and len(types_covered) < len(question_types_and_prompts):
        for question_type, prompt in question_types_and_prompts.items():
            if question_type in types_covered:
                continue  # Skip already fully covered types
            
            # Generate questions for the current type
            questions = generate_questions_in_chunks(
                paragraph,
                image_data,
                question_type,
                remaining_questions,  # Remaining slots for questions
                prompt,
                chunk_size
            )
            
            # Remove duplicates
            questions = remove_duplicates(questions)

            # Add metadata and append to all_questions
            for qa in questions:
                question_text = qa["question"]
                answer_text = qa["answer"]
                all_questions.append({
                    "Question Type": question_type,
                    "Question": question_text,
                    "Answer": answer_text,
                    "Question Length (Chars)": len(question_text),
                    "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                    "Question Tokens": calculate_tokens(question_text),
                    "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else "".join(answer_text))
                })

            # Update remaining questions and track completed types
            remaining_questions -= len(questions)
            if len(questions) == 0:
                types_covered.add(question_type)  # Mark this type as fully generated

            # Break the loop if enough questions are generated
            if len(all_questions) >= total_questions:
                break

    # Return the top N unique questions
    return all_questions[:total_questions]







import tiktoken

def calculate_tokens(text, model="gpt-3.5-turbo"):
    """
    Calculate the number of tokens in a given text using tiktoken.

    Args:
        text (str): Input text.
        model (str): Model name to determine the encoding (default: "gpt-3.5-turbo").

    Returns:
        int: Number of tokens in the text.
    """
    try:
        # Preload the encoding locally to avoid HTTPS timeouts
        if model in ["gpt-3.5-turbo", "gpt-4"]:
            encoding_name = "cl100k_base"
        elif model == "text-davinci-003":
            encoding_name = "p50k_base"
        else:
            raise ValueError(f"Unsupported model: {model}")

        # Load the encoding
        encoding = tiktoken.encoding_for_model(model) if tiktoken.encoding_for_model else tiktoken.get_encoding(encoding_name)
        
        # Return the number of tokens
        return len(encoding.encode(text))

    except Exception as e:
        print(f"Error calculating tokens: {e}")
        return None



"Describe an image of a formal bank document in detail, including its overall structure, such as headers, footers, and sections, along with the placement of the bank’s logo, name, address, and contact information. Include background features like watermarks, barcodes, or security marks. Focus on tables within the document that display account details, transaction records, dates, descriptions, debit/credit amounts, and balances, specifying the number of rows and columns, headers, borders, alignment, and any highlights like bold totals or shaded cells. Mention account details such as the account holder’s name, account number, branch name, and IFSC code, along with details about account types or interest rates. Provide information on transaction lists or tables, including details like dates, descriptions, transaction types (credit/debit), amounts, and balances, highlighting any significant entries. Include additional elements like handwritten notes, signatures (manual or digital), stamps, or seals for authentication, as well as reference numbers, document IDs, or case numbers for tracking. Describe the formatting, including font styles, alignment, and the use of bold or italicized text for emphasis. Mention any legal or formal information, such as disclaimers, terms, and conditions, or privacy policies, often found at the bottom of the document, and note the use of formal language and financial terminology. Finally, describe the professional appearance of the document, including the use of colors (e.g., black-and-white or highlights) and whether it appears printed or scanned."













ChatGPT can make mistakes. Check important info.






from azure.ai.openai import ChatGPT
import json


def generate_questions(paragraph, question_type, num_questions, prompt_template, base64_images=None):
    """
    Generate questions using Azure LLM based on a paragraph, question type, prompt template, and Base64 images.

    Args:
        paragraph (str): The input text for generating questions.
        question_type (str): The type of questions to generate (e.g., "Factual", "Procedural").
        num_questions (int): Number of questions to generate.
        prompt_template (str): Template for the LLM prompt.
        base64_images (dict): Optional dictionary of image names and their Base64 representations.

    Returns:
        list: A list of generated questions and answers, or an empty list in case of errors.
    """
    # Initialize the Azure OpenAI ChatGPT
    llm = ChatGPT(
        temperature=0,
        api_version="<azure_openai_api_version>",
        azure_endpoint="<azure_openai_endpoint>",
        deployment_name="ssgpt-40"
    )

    # Construct the prompt dynamically based on inputs
    prompt = prompt_template.format(paragraph=paragraph, num_questions=num_questions, question_type=question_type)

    # Add Base64 images to the payload if provided
    images_payload = [{"name": name, "base64": image} for name, image in base64_images.items()] if base64_images else []

    try:
        # Call the LLM with text and Base64 images
        response = llm.invoke([
            {
                "role": "user",
                "content": {
                    "type": "structured",
                    "inputs": {
                        "text": prompt,
                        "images": images_payload
                    },
                    "instruction": f"Generate {num_questions} {question_type} questions and answers based on the provided text and images. Provide the output in JSON format."
                }
            }
        ])

        # Parse and return JSON response
        return extract_valid_json(response.content)

    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []


# Helper function to extract valid JSON from LLM response
def extract_valid_json(response):
    """
    Extract valid JSON from the LLM response.

    Args:
        response (str): The response string from the LLM.

    Returns:
        list: Parsed JSON list of questions and answers, or an empty list if parsing fails.
    """
    try:
        # Parse the response as JSON
        return json.loads(response)
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected in LLM response.")
        return []








import fitz  # PyMuPDF
import os
import base64
import re
import json
from azure.ai.openai import ChatGPT

# Extract images from PDF
def extract_images_with_pymupdf(pdf_path, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)
    for page_number in range(len(doc)):
        page = doc[page_number]
        images = page.get_images(full=True)
        if not images:
            print(f"No images found on page {page_number + 1}")
            continue

        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]
            image_filename = os.path.join(
                output_folder, f"page_{page_number + 1}_{img_index + 1}.{image_ext}"
            )
            with open(image_filename, "wb") as image_file:
                image_file.write(image_bytes)
            print(f"Saved image: {image_filename}")

# Sort files numerically
def sort_numerically(file_list):
    """Sort files numerically by extracting numbers from filenames."""
    def extract_numbers(file_name):
        return [int(num) for num in re.findall(r'\d+', file_name)]
    return sorted(file_list, key=extract_numbers)

# Convert images to Base64
def images_to_base64(folder_path):
    base64_images = {}
    # Get all files and sort them numerically
    file_list = sort_numerically(os.listdir(folder_path))
    for image_file in file_list:
        if image_file.endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, image_file)
            with open(image_path, "rb") as file:
                encoded_string = base64.b64encode(file.read()).decode("utf-8")
                base64_images[image_file] = encoded_string
    return base64_images

# Call Azure OpenAI LLM with images
def call_llm_with_images(base64_images):
    responses = []
    messages_template = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Extract the text from the image"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": ""  # Placeholder, updated in loop
                    }
                }
            ]
        }
    ]

    # Configure AzureChatOpenAI
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="<azure_openai_api_version>",
        azure_endpoint="<azure_openai_endpoint>",
        deployment_name="ssgpt-40",
        seed=1
    )

    for image_name, base64_image in base64_images.items():
        try:
            messages = messages_template.copy()
            messages[0]["content"][1]["image_url"]["url"] = f"data:image/jpeg;base64,{base64_image}"
            response = llm.invoke(messages)
            responses.append({
                "image_name": image_name,
                "base64": base64_image,
                "response": response.content
            })
        except Exception as e:
            print(f"Error calling LLM for {image_name}: {e}")
            responses.append({
                "image_name": image_name,
                "base64": base64_image,
                "response": f"Error: {e}"
            })
    return responses

# Main Workflow
pdf_path = r"h:\Question and answer\Fund accounting (Resubmit_1025)36. VFMC AAS25 SOP 5.8.pdf"  # Replace with your PDF file
output_folder = r"h:\Question and answer Fund accounting\pdf_images"

# Step 1: Extract images from the PDF
extract_images_with_pymupdf(pdf_path, output_folder)

# Step 2: Convert extracted images to Base64
base64_images = images_to_base64(output_folder)

# Step 3: Call LLM with Base64-encoded images and get JSON response
responses = call_llm_with_images(base64_images)

# Step 4: Generate JSON output with Base64 and LLM responses
output_json = {"responses": responses}

# Save JSON to a file
json_output_file = "output_responses_with_base64.json"
with open(json_output_file, "w") as json_file:
    json.dump(output_json, json_file, indent=4)

# Print JSON output
print("Generated JSON Object:")
print(json.dumps(output_json, indent=4))










import fitz  # PyMuPDF
import os
import base64
import re
import json
from azure.ai.openai import ChatGPT

# Extract images from PDF
def extract_images_with_pymupdf(pdf_path, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)
    for page_number in range(len(doc)):
        page = doc[page_number]
        images = page.get_images(full=True)
        if not images:
            print(f"No images found on page {page_number + 1}")
            continue

        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]
            image_filename = os.path.join(
                output_folder, f"page_{page_number + 1}_{img_index + 1}.{image_ext}"
            )
            with open(image_filename, "wb") as image_file:
                image_file.write(image_bytes)
            print(f"Saved image: {image_filename}")

# Sort files numerically
def sort_numerically(file_list):
    """Sort files numerically by extracting numbers from filenames."""
    def extract_numbers(file_name):
        return [int(num) for num in re.findall(r'\d+', file_name)]
    return sorted(file_list, key=extract_numbers)

# Convert images to Base64
def images_to_base64(folder_path):
    base64_images = {}
    # Get all files and sort them numerically
    file_list = sort_numerically(os.listdir(folder_path))
    for image_file in file_list:
        if image_file.endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, image_file)
            with open(image_path, "rb") as file:
                encoded_string = base64.b64encode(file.read()).decode("utf-8")
                base64_images[image_file] = encoded_string
    return base64_images

# Call Azure OpenAI LLM with images
def call_llm_with_images(base64_images):
    responses = []
    messages_template = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Extract the text from the image"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": ""  # Placeholder, updated in loop
                    }
                }
            ]
        }
    ]

    # Configure AzureChatOpenAI
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="<azure_openai_api_version>",
        azure_endpoint="<azure_openai_endpoint>",
        deployment_name="ssgpt-40",
        seed=1
    )

    for image_name, base64_image in base64_images.items():
        try:
            messages = messages_template.copy()
            messages[0]["content"][1]["image_url"]["url"] = f"data:image/jpeg;base64,{base64_image}"
            response = llm.invoke(messages)
            responses.append({"image_name": image_name, "response": response.content})
        except Exception as e:
            print(f"Error calling LLM for {image_name}: {e}")
            responses.append({"image_name": image_name, "response": f"Error: {e}"})
    return responses

# Main Workflow
pdf_path = r"h:\Question and answer\Fund accounting (Resubmit_1025)36. VFMC AAS25 SOP 5.8.pdf"  # Replace with your PDF file
output_folder = r"h:\Question and answer Fund accounting\pdf_images"

# Step 1: Extract images from the PDF
extract_images_with_pymupdf(pdf_path, output_folder)

# Step 2: Convert extracted images to Base64
base64_images = images_to_base64(output_folder)
print("Base64 Encoded Images:", base64_images)

# Step 3: Call LLM with Base64-encoded images
responses = call_llm_with_images(base64_images)

# Step 4: Generate JSON output
output_json = {"responses": responses}

# Save JSON to a file
json_output_file = "output_responses.json"
with open(json_output_file, "w") as json_file:
    json.dump(output_json, json_file, indent=4)

# Print JSON output
print("Generated JSON Object:")
print(json.dumps(output_json, indent=4))











import os
import base64
import json
import fitz  # PyMuPDF
from azure.ai.openai import ChatGPT

# Function to extract images from PDF
def extract_images_with_pymupdf(pdf_path, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)
    for page_number in range(len(doc)):
        page = doc[page_number]
        images = page.get_images(full=True)
        if not images:
            print(f"No images found on page {page_number + 1}")
            continue

        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]
            image_filename = os.path.join(
                output_folder, f"page_{page_number + 1}_{img_index + 1}.{image_ext}"
            )
            with open(image_filename, "wb") as image_file:
                image_file.write(image_bytes)
            print(f"Saved image: {image_filename}")

# Function to convert images to Base64
def images_to_base64(folder_path):
    base64_images = {}
    for image_file in os.listdir(folder_path):
        if image_file.endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, image_file)
            with open(image_path, "rb") as file:
                encoded_string = base64.b64encode(file.read()).decode("utf-8")
                base64_images[image_file] = encoded_string
    return base64_images

# Function to prepare JSON payload
def generate_json_payload(base64_images):
    """
    Generates a JSON payload containing image names and base64-encoded data.
    """
    images_payload = {
        "images": [{"name": image_name, "data": image_data} for image_name, image_data in base64_images.items()]
    }
    return json.dumps(images_payload)

# Function to call LLM with Base64-encoded images
def call_llm_with_json_payload(json_payload, llm_endpoint):
    """
    Sends the JSON payload to the LLM API and retrieves responses.
    """
    headers = {"Content-Type": "application/json"}
    response = requests.post(llm_endpoint, headers=headers, data=json_payload)
    return response.json()

# Main Workflow
pdf_path = r"h:\Question and answer\Fund accounting (Resubmit_1025)36. VFMC AAS25 SOP 5.8.pdf"  # Replace with your PDF file
output_folder = r"h:\Question and answer Fund accounting\pdf_images"
llm_endpoint = "http://your-llm-api-endpoint"  # Replace with your LLM API endpoint

# Step 1: Extract images from the PDF
extract_images_with_pymupdf(pdf_path, output_folder)

# Step 2: Convert extracted images to Base64
base64_images = images_to_base64(output_folder)
print("Base64 Encoded Images:", base64_images)

# Step 3: Generate JSON payload
json_payload = generate_json_payload(base64_images)
print("Generated JSON Payload:", json_payload)

# Step 4: Send JSON payload to LLM and retrieve responses
responses = call_llm_with_json_payload(json_payload, llm_endpoint)
print("LLM Responses:", responses)












import re

def organize_content_page_wise(responses):
    """
    Organizes AIMessage content into a dictionary grouped by page.
    """
    page_wise_content = {}

    for image_name, response in responses.items():
        # Extract page number from the image name (e.g., "page_3_1.png")
        match = re.search(r'page_(\d+)_\d+', image_name)
        if match:
            page_number = f"Page {match.group(1)}"
        else:
            continue  # Skip if page number is not found

        # Extract content from the AIMessage object
        content = response.content if hasattr(response, 'content') else "No content available"

        # Group content by page
        if page_number not in page_wise_content:
            page_wise_content[page_number] = []
        page_wise_content[page_number].append(content)

    return page_wise_content


# Example Usage
responses = {
    'page_1_1.png': AIMessage(content='STATE STREET'),
    'page_3_1.png': AIMessage(content='DATE: 190822\nTIME: 05:54\n\nSTATE STREET CORPORATION'),
    'page_3_2.png': AIMessage(content="I'm sorry, but I can't extract text from images."),
    'page_4_1.png': AIMessage(content='Navigation\nApplications\nAAS25 Report\nClient:\niVFMC\nChristian Super\nING'),
    'page_4_2.png': AIMessage(content='Job Monitor\n\nJob Status\n\nRefresh Manual Start\n\nJob Code'),
    'page_5_1.png': AIMessage(content='AAS25 Report\n\nNavigation\nApplications AAS25 Report'),
    'page_5_2.png': AIMessage(content='Job Monitor\n\nJob Status\n\nRefresh Manual Start\n\nJob Code: AAS_INB_DATA_VFMC'),
    'page_6_1.png': AIMessage(content='Job Status\n\nJob Code: AAS_INB_DATA_VFMC\nJob Name: Get Pos/Txn/Trial data and generate'),
    'page_6_2.png': AIMessage(content='Navigation\nApplications: AAS325 Report\nClient: VFMC'),
    'page_7_1.png': AIMessage(content='Navigation\nApplications: AAS25 Report\nClient: VFMC'),
    'page_7_2.jpeg': AIMessage(content='Navigation\nApplications: AAS25 Report\nClient: VFMC'),
    'page_8_1.jpeg': AIMessage(content='Navigation\nApplications: AAS25 Report\nClient: VFMC'),
    'page_8_2.png': AIMessage(content='Navigation\nApplications\nAAS25 Report\nClient:\nVFMC'),
    'page_9_1.png': AIMessage(content='Applications: AAS25 Report\nClient: VFMC\n\nNormalized Table'),
    'page_9_2.jpeg': AIMessage(content='Navigation\nApplications: AAS25 Report\nClient: VFMC'),
    'page_10_1.png': AIMessage(content='Navigation\nApplications: AAS25 Report\nClient: VFMC'),
    'page_10_2.png': AIMessage(content='Job Status\n\nApp Code | Report Id | Title | Report Text'),
    'page_11_1.png': AIMessage(content='VFMC June 2019\n\nReport\tColumn\tField\tPROD total\tDUMMY'),
    'page_11_2.png': AIMessage(content='Sort A to Z\nSort Z to A\nSort by Color\nClear Filter'),
    'page_113.png': AIMessage(content="I'm sorry, but I can't extract text from the image you provided."),
}

# Group responses by page
page_wise_content = organize_content_page_wise(responses)

# Print organized content
for page, content_list in page_wise_content.items():
    print(f"{page}:")
    for content in content_list:
        print(content)
    print("\n")











import re

# Function to clean noisy responses
def clean_response(response_content):
    """
    Cleans the noisy text content by removing unwanted characters, extra spaces,
    and irrelevant lines.
    """
    if not response_content:
        return "No meaningful text found."

    # Remove metadata lines (e.g., 'response_metadata', etc.)
    cleaned_text = re.sub(r'response_metadata.*', '', response_content, flags=re.DOTALL)

    # Remove extra whitespace, blank lines, and non-alphanumeric characters
    cleaned_text = re.sub(r'[^a-zA-Z0-9\s:/,.()-]+', '', cleaned_text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    return cleaned_text


# Function to process LLM responses
def process_responses(responses):
    """
    Iterates through the responses and cleans up the content for each image.
    """
    cleaned_responses = {}
    for image_name, response in responses.items():
        if response:
            # Access the content attribute directly
            content = response.content if hasattr(response, 'content') else ''
            cleaned_responses[image_name] = clean_response(content)
        else:
            cleaned_responses[image_name] = "No response from LLM."

    return cleaned_responses










import re

def clean_response(response_content):
    """
    Cleans the noisy text content by removing unwanted characters, extra spaces, 
    and irrelevant lines.
    """
    if not response_content:
        return "No meaningful text found."

    # Remove metadata lines (e.g., 'response_metadata', etc.)
    cleaned_text = re.sub(r'response_metadata.*', '', response_content, flags=re.DOTALL)

    # Remove extra whitespace, blank lines, and non-alphanumeric characters
    cleaned_text = re.sub(r'[^a-zA-Z0-9\s:/,.()-]+', '', cleaned_text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    return cleaned_text

# Clean all responses
def process_responses(responses):
    """
    Iterates through the responses and cleans up the content for each image.
    """
    cleaned_responses = {}
    for image_name, response in responses.items():
        if response:
            # Extract the content from the LLM response
            content = response.get('content', '')
            cleaned_responses[image_name] = clean_response(content)
        else:
            cleaned_responses[image_name] = "No response from LLM."

    return cleaned_responses


# Example usage with the noisy responses
noisy_responses = {
    "page_3_1.png": {
        "content": "STATE STREET response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 373}}"
    },
    "page_3_2.png": {
        "content": "I'm sorry, but I can't extract text from images."
    },
    "page_4_1.png": {
        "content": "Navigation\nApplications\nAAS25 Report\nClient: VFMC\nChristian Super\nING"
    },
    # Add more sample noisy responses here...
}

# Process and clean the responses
cleaned_responses = process_responses(noisy_responses)

# Print cleaned responses
for image_name, cleaned_text in cleaned_responses.items():
    print(f"Image: {image_name}")
    print(f"Cleaned Content: {cleaned_text}\n")










import fitz  # PyMuPDF
import os
import base64
import re
from azure.ai.openai import ChatGPT

# Extract images from PDF
def extract_images_with_pymupdf(pdf_path, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)
    for page_number in range(len(doc)):
        page = doc[page_number]
        images = page.get_images(full=True)
        if not images:
            print(f"No images found on page {page_number + 1}")
            continue

        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]
            image_filename = os.path.join(
                output_folder, f"page_{page_number + 1}_{img_index + 1}.{image_ext}"
            )
            with open(image_filename, "wb") as image_file:
                image_file.write(image_bytes)
            print(f"Saved image: {image_filename}")

# Sort files numerically
def sort_numerically(file_list):
    """Sort files numerically by extracting numbers from filenames."""
    def extract_numbers(file_name):
        return [int(num) for num in re.findall(r'\d+', file_name)]
    return sorted(file_list, key=extract_numbers)

# Convert images to Base64
def images_to_base64(folder_path):
    base64_images = {}
    # Get all files and sort them numerically
    file_list = sort_numerically(os.listdir(folder_path))
    for image_file in file_list:
        if image_file.endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, image_file)
            with open(image_path, "rb") as file:
                encoded_string = base64.b64encode(file.read()).decode("utf-8")
                base64_images[image_file] = encoded_string
    return base64_images

# Call Azure OpenAI LLM with images
def call_llm_with_images(base64_images):
    responses = {}
    messages_template = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Extract the text from the image"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": ""  # Placeholder, updated in loop
                    }
                }
            ]
        }
    ]

    # Configure AzureChatOpenAI
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="<azure_openai_api_version>",
        azure_endpoint="<azure_openai_endpoint>",
        deployment_name="ssgpt-40",
        seed=1
    )

    for image_name, base64_image in base64_images.items():
        try:
            messages = messages_template.copy()
            messages[0]["content"][1]["image_url"]["url"] = f"data:image/jpeg;base64,{base64_image}"
            response = llm.invoke(messages)
            responses[image_name] = response
        except Exception as e:
            print(f"Error calling LLM for {image_name}: {e}")
            responses[image_name] = None
    return responses

# Main Workflow
pdf_path = r"h:\Question and answer\Fund accounting (Resubmit_1025)36. VFMC AAS25 SOP 5.8.pdf"  # Replace with your PDF file
output_folder = r"h:\Question and answer Fund accounting\pdf_images"

# Step 1: Extract images from the PDF
extract_images_with_pymupdf(pdf_path, output_folder)

# Step 2: Convert extracted images to Base64
base64_images = images_to_base64(output_folder)
print("Base64 Encoded Images:", base64_images)

# Step 3: Call LLM with Base64-encoded images
responses = call_llm_with_images(base64_images)

# Step 4: Print LLM Responses
print("LLM Responses:")
for image_name, response in responses.items():
    print(f"Image: {image_name}")
    print(f"Response: {response}")









import fitz  # PyMuPDF
import os
import base64
import re
from azure.ai.openai import ChatGPT

# Extract images from PDF
def extract_images_with_pymupdf(pdf_path, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)
    for page_number in range(len(doc)):
        page = doc[page_number]
        images = page.get_images(full=True)
        if not images:
            print(f"No images found on page {page_number + 1}")
            continue

        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]
            image_filename = os.path.join(
                output_folder, f"page_{page_number + 1}_{img_index + 1}.{image_ext}"
            )
            with open(image_filename, "wb") as image_file:
                image_file.write(image_bytes)
            print(f"Saved image: {image_filename}")

# Sort files numerically
def sort_numerically(file_list):
    """Sort files numerically by extracting numbers from filenames."""
    def extract_numbers(file_name):
        return [int(num) for num in re.findall(r'\d+', file_name)]
    return sorted(file_list, key=extract_numbers)

# Convert images to Base64
def images_to_base64(folder_path):
    base64_images = {}
    # Get all files and sort them numerically
    file_list = sort_numerically(os.listdir(folder_path))
    for image_file in file_list:
        if image_file.endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, image_file)
            with open(image_path, "rb") as file:
                encoded_string = base64.b64encode(file.read()).decode("utf-8")
                base64_images[image_file] = encoded_string
    return base64_images

# Call Azure OpenAI LLM with images
def call_llm_with_images(base64_images):
    responses = {}
    messages_template = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": f"Extract the text from the image"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}"
                    }
                }
            ]
        }
    ]

    # Configure AzureChatOpenAI
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="<azure_openai_api_version>",
        azure_endpoint="<azure_openai_endpoint>",
        deployment_name="ssgpt-40",
        seed=1
    )

    for image_name, base64_image in base64_images.items():
        try:
            messages = messages_template.copy()
            messages[0]["content"][1]["image_url"]["url"] = f"data:image/jpeg;base64,{base64_image}"
            response = llm.invoke(messages)
            responses[image_name] = response
        except Exception as e:
            print(f"Error calling LLM for {image_name}: {e}")
            responses[image_name] = None
    return responses

# Main Workflow
pdf_path = r"h:\Question and answer\Fund accounting (Resubmit_1025)36. VFMC AAS25 SOP 5.8.pdf"  # Replace with your PDF file
output_folder = r"h:\Question and answer Fund accounting\pdf_images"

# Step 1: Extract images from the PDF
extract_images_with_pymupdf(pdf_path, output_folder)

# Step 2: Convert extracted images to Base64
base64_images = images_to_base64(output_folder)
print("Base64 Encoded Images:", base64_images)

# Step 3: Call LLM with Base64-encoded images
responses = call_llm_with_images(base64_images)

# Step 4: Print LLM Responses
print("LLM Responses:")
for image_name, response in responses.items():
    print(f"Image: {image_name}")
    print(f"Response: {response}")












import fitz  # PyMuPDF
import os
import base64
import re
from langchain.chat_models import AzureChatOpenAI  # Import the correct AzureChatOpenAI class
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

# Extract images from PDF
def extract_images_with_pymupdf(pdf_path, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)

    for page_number in range(len(doc)):
        page = doc.load_page(page_number)
        images = page.get_images(full=True)

        if not images:
            print(f"No images found on page {page_number + 1}")
            continue

        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]

            image_filename = os.path.join(
                output_folder, f"page_{page_number + 1}_{img_index + 1}.{image_ext}"
            )

            with open(image_filename, "wb") as image_file:
                image_file.write(image_bytes)

            print(f"Saved image: {image_filename}")

# Sort files numerically
def sort_numerically(file_list):
    """Sort files numerically by extracting numbers from filenames."""
    def extract_numbers(file_name):
        return [int(num) for num in re.findall(r'\d+', file_name)]

    return sorted(file_list, key=extract_numbers)

# Convert images to Base64
def images_to_base64(folder_path):
    base64_images = {}

    # Get all files and sort them numerically
    file_list = sort_numerically(os.listdir(folder_path))

    for image_file in file_list:
        if image_file.endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, image_file)

            with open(image_path, "rb") as file:
                encoded_string = base64.b64encode(file.read()).decode("utf-8")
            
            base64_images[image_file] = encoded_string

    return base64_images

# Send images and text to Azure OpenAI using AzureChatOpenAI (LangChain)
def generate_text_with_images(base64_images, text_input, azure_openai_endpoint, azure_api_key, azure_openai_api_version):
    # Construct the list of messages
    messages = [
        HumanMessage(content=text_input)  # Start with the text input as the initial user message
    ]

    # Add images as base64 data to the messages
    for image_name, base64_image in base64_images.items():
        image_message = HumanMessage(content=f"data:image/jpeg;base64,{base64_image}")
        messages.append(image_message)

    # Initialize the AzureChatOpenAI instance
    chat_openai = AzureChatOpenAI(
        openai_api_key=azure_api_key,
        openai_api_base=azure_openai_endpoint,
        openai_api_version=azure_openai_api_version,
        deployment_name="ssgpt-40",  # Use the correct deployment name for your model
        temperature=0  # Adjust the temperature as per your needs
    )

    # Call the AzureChatOpenAI with the constructed messages
    try:
        response = chat_openai.completion_with_retry(messages)  # Call the method for response

        # Return the response content
        return response['text']  # Ensure you are extracting the correct part of the response

    except Exception as e:
        print(f"Error calling LLM: {e}")
        return None

# Example usage
pdf_path = "h:\\Question and answer Fund accounting (Resubmit_1025)36. VFMC AAS25 SOP 5.0.pdf"  # Replace with your PDF file
output_folder = "h:\\Question and answer\\Fund accounting\\pdf_images"

# Extract images from the PDF
extract_images_with_pymupdf(pdf_path, output_folder)

# Convert images to Base64
base64_images = images_to_base64(output_folder)

# Provide the text input for the LLM
text_input = "Here is some text that goes along with the images."

# Azure-specific settings (ensure you have the correct values)
azure_openai_endpoint = "https://your-azure-endpoint"  # Replace with your Azure endpoint
azure_api_key = "your-azure-api-key"  # Replace with your Azure API key
azure_openai_api_version = "2023-05-15"  # Replace with the correct API version for your Azure model

# Generate response with both text and images using Azure OpenAI
response_text = generate_text_with_images(base64_images, text_input, azure_openai_endpoint, azure_api_key, azure_openai_api_version)

print(response_text)













import fitz  # PyMuPDF
import os
import base64
import re
from azure.ai.openai import ChatGPT
from azure.core.credentials import AzureKeyCredential

# Extract images from PDF
def extract_images_with_pymupdf(pdf_path, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)

    for page_number in range(len(doc)):
        page = doc.load_page(page_number)
        images = page.get_images(full=True)

        if not images:
            print(f"No images found on page {page_number + 1}")
            continue

        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]

            image_filename = os.path.join(
                output_folder, f"page_{page_number + 1}_{img_index + 1}.{image_ext}"
            )

            with open(image_filename, "wb") as image_file:
                image_file.write(image_bytes)

            print(f"Saved image: {image_filename}")

# Sort files numerically
def sort_numerically(file_list):
    """Sort files numerically by extracting numbers from filenames."""
    def extract_numbers(file_name):
        return [int(num) for num in re.findall(r'\d+', file_name)]

    return sorted(file_list, key=extract_numbers)

# Convert images to Base64
def images_to_base64(folder_path):
    base64_images = {}

    # Get all files and sort them numerically
    file_list = sort_numerically(os.listdir(folder_path))

    for image_file in file_list:
        if image_file.endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, image_file)

            with open(image_path, "rb") as file:
                encoded_string = base64.b64encode(file.read()).decode("utf-8")
            
            base64_images[image_file] = encoded_string

    return base64_images

# Send images and text to Azure OpenAI for processing
def generate_text_with_images(base64_images, text_input, azure_openai_endpoint, azure_openai_api_version, azure_api_key):
    messages = [
        {
            "role": "user",
            "content": text_input
        }
    ]

    # Add images as base64 data
    for image_name, base64_image in base64_images.items():
        image_message = {
            "role": "user",
            "content": f"data:image/jpeg;base64,{base64_image}"
        }
        messages.append(image_message)

    # Azure LLM API Call (using Azure-specific method)
    try:
        # Instantiate the Azure OpenAI client
        client = ChatGPT(endpoint=azure_openai_endpoint, credential=AzureKeyCredential(azure_api_key))

        # Call the Azure OpenAI model
        response = client.chat_completions.create(
            deployment_name="ssgpt-40",  # Change if using a different deployment
            messages=messages,  # Pass the entire structured messages list
            api_version=azure_openai_api_version
        )

        # Extract and return the response
        return response.choices[0].message.content  # Adjust according to the response format

    except Exception as e:
        print(f"Error calling LLM: {e}")
        return None

# Example usage
pdf_path = "h:\\Question and answer Fund accounting (Resubmit_1025)36. VFMC AAS25 SOP 5.0.pdf"  # Replace with your PDF file
output_folder = "h:\\Question and answer\\Fund accounting\\pdf_images"

# Extract images from the PDF
extract_images_with_pymupdf(pdf_path, output_folder)

# Convert images to Base64
base64_images = images_to_base64(output_folder)

# Provide the text input for the LLM
text_input = "Here is some text that goes along with the images."

# Azure-specific settings (ensure you have the correct values)
azure_openai_endpoint = "https://your-azure-endpoint"  # Replace with your Azure endpoint
azure_openai_api_version = "2023-05-15"  # Replace with the correct API version
azure_api_key = "your-azure-api-key"  # Replace with your Azure API key

# Generate response with both text and images using Azure OpenAI
response_text = generate_text_with_images(base64_images, text_input, azure_openai_endpoint, azure_openai_api_version, azure_api_key)

print(response_text)














import fitz  # PyMuPDF
import os
import base64
import re
import openai

# Extract images from PDF
def extract_images_with_pymupdf(pdf_path, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)

    for page_number in range(len(doc)):
        page = doc.load_page(page_number)
        images = page.get_images(full=True)

        if not images:
            print(f"No images found on page {page_number + 1}")
            continue

        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]

            image_filename = os.path.join(
                output_folder, f"page_{page_number + 1}_{img_index + 1}.{image_ext}"
            )

            with open(image_filename, "wb") as image_file:
                image_file.write(image_bytes)

            print(f"Saved image: {image_filename}")

# Sort files numerically
def sort_numerically(file_list):
    """Sort files numerically by extracting numbers from filenames."""
    def extract_numbers(file_name):
        return [int(num) for num in re.findall(r'\d+', file_name)]

    return sorted(file_list, key=extract_numbers)

# Convert images to Base64
def images_to_base64(folder_path):
    base64_images = {}

    # Get all files and sort them numerically
    file_list = sort_numerically(os.listdir(folder_path))

    for image_file in file_list:
        if image_file.endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, image_file)

            with open(image_path, "rb") as file:
                encoded_string = base64.b64encode(file.read()).decode("utf-8")
            
            base64_images[image_file] = encoded_string

    return base64_images

# Send images and text to OpenAI's GPT-3 (or other models) for processing
def generate_text_with_images(base64_images, text_input):
    messages = [
        {
            "role": "user",
            "content": text_input
        },
        {
            "role": "user",
            "content": {
                "type": "text",
                "text": "Extract the text from the image"
            }
        }
    ]

    for image_name, base64_image in base64_images.items():
        image_message = {
            "role": "user",
            "content": {
                "type": "image_url",
                "image_url": f"data:image/jpeg;base64,{base64_image}"
            }
        }
        messages.append(image_message)

    # Call OpenAI API with the messages payload
    response = openai.ChatCompletion.create(
        model="gpt-4",  # or another model as required
        messages=messages
    )

    return response['choices'][0]['message']['content']

# Example usage
pdf_path = "path/to/your/file.pdf"
output_folder = "output_images_folder"

# Extract images from the PDF
extract_images_with_pymupdf(pdf_path, output_folder)

# Convert images to Base64
base64_images = images_to_base64(output_folder)

# Provide the text input for the LLM
text_input = "Here is some text that goes along with the images."

# Generate response with both text and images
response_text = generate_text_with_images(base64_images, text_input)

print(response_text)












import openai
from google.cloud import vision

# Step 1: Use Google Vision API to Extract Text
def extract_text_from_image(image_path):
    client = vision.ImageAnnotatorClient()
    with open(image_path, "rb") as image_file:
        content = image_file.read()
    image = vision.Image(content=content)
    response = client.text_detection(image=image)
    if response.text_annotations:
        return response.text_annotations[0].description
    return "No text detected"

# Step 2: Use OpenAI LLM to Process the Extracted Text
def process_text_with_llm(text):
    openai.api_key = "your_openai_api_key"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": f"Please analyze the following text:\n{text}"},
        ],
    )
    return response['choices'][0]['message']['content']

# Main Execution
image_path = "path_to_your_image.jpg"
extracted_text = extract_text_from_image(image_path)
llm_response = process_text_with_llm(extracted_text)

print("Extracted Text:", extracted_text)
print("LLM Response:", llm_response)









def call_1lm_with_images(base64_images):
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": f"This is the Base64 content of an image file name {{image_name}}. Please read the text or contents this image might contain:\n\n{base64_images}"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_images}"
                    }
                }
            ]
        }
    ]
    
    # Configure the LLM
    llm = AzureChatOpenAI(
        temperature=0.7,
        api_version=azure_openai_api_version,
        azure_endpoint=azure_openai_endpoint,
        deployment_name="ssgpt-40"
    )

    try:
        response = llm.invoke(messages)
        return response
    except Exception as e:
        print(f"Error calling LLM: {e}")
        return None










"messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What’s in this image?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": f"data:image/jpeg;base64,{base64_image}"
            }
          }
        ]
      }
    ],






import base64

# Example of decoding binary content
encoded_text = "iVBORw0KGgoAAAANSUhEUgAAAAUA..."

# Decode without converting to UTF-8
binary_data = base64.b64decode(encoded_text)

# Save to a file if needed
with open("output_file", "wb") as file:
    file.write(binary_data)

print("Binary data successfully decoded and saved.")








import base64

# Base64 encoded string with incorrect padding
encoded_text = "SGVsbG8gV29ybGQ"

# Add padding
missing_padding = len(encoded_text) % 4
if missing_padding:
    encoded_text += '=' * (4 - missing_padding)

# Decode the Base64 string
decoded_text = base64.b64decode(encoded_text).decode('utf-8')
print(decoded_text)  # Output: Hello World








import base64

# Base64 encoded string with incorrect padding
encoded_text = "SGVsbG8gV29ybGQ"

# Decode the Base64 string with validation
try:
    decoded_text = base64.b64decode(encoded_text, validate=True).decode('utf-8')
    print(decoded_text)
except base64.binascii.Error as e:
    print(f"Decoding failed: {e}")





import os
import base64
import re
from langchain.chat_models import AzureChatOpenAI

# Replace these with your Azure OpenAI configurations
azure_openai_api_version = "2023-03-15-preview"
azure_openai_endpoint = "https://your-endpoint.openai.azure.com/"
deployment_name = "ssgpt-40"

# Sort files numerically
def sort_numerically(file_list):
    """Sort files numerically by extracting numbers from filenames."""
    def extract_numbers(file_name):
        return [int(num) for num in re.findall(r'\d+', file_name)]

    return sorted(file_list, key=extract_numbers)

# Convert images to Base64
def images_to_base64(folder_path):
    base64_images = {}
    # Get all files and sort them numerically
    file_list = sort_numerically(os.listdir(folder_path))
    for image_file in file_list:
        if image_file.endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, image_file)
            with open(image_path, "rb") as file:
                encoded_string = base64.b64encode(file.read()).decode("utf-8")
                base64_images[image_file] = encoded_string
    return base64_images

# Call the LLM with Base64 images
def call_llm_with_images(base64_images):
    messages = [{"role": "user", "content": "Please compare the following images:"}]
    for image_name, base64_string in base64_images.items():
        messages.append({
            "role": "user",
            "content": f"Image: {image_name}\nBase64:\n{base64_string[:100]}..."  # Truncated for readability
        })

    # Initialize the Azure OpenAI Chat model
    llm = AzureChatOpenAI(
        temperature=0,
        api_version=azure_openai_api_version,
        azure_endpoint=azure_openai_endpoint,
        deployment_name=deployment_name
    )

    # Invoke the model
    response = llm.invoke(messages)
    return response

# Main Workflow
output_folder = "extracted_images"  # Replace with your folder containing images

# Convert images to Base64
base64_images = images_to_base64(output_folder)

# Call LLM with Base64-encoded images
response = call_llm_with_images(base64_images)

# Print the response
print("LLM Response:")
print(response)













def images_to_base64(folder_path):
    base64_images = {}
    for image_file in sorted(os.listdir(folder_path)):
        if image_file.endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, image_file)
            with open(image_path, "rb") as file:
                encoded_string = base64.b64encode(file.read()).decode("utf-8")
                base64_images[image_file] = encoded_string
    return base64_images



def call_llm_with_images(base64_images):
    # Start the LLM messages structure
    messages = [{"role": "user", "content": "Please compare the following images:"}]
    
    # Append each Base64-encoded image as a structured message
    for image_name, base64_string in base64_images.items():
        messages.append({
            "role": "user",
            "content": f"Image: {image_name}\nBase64:\n{base64_string[:100]}..."  # Truncated for readability
        })

    # Initialize the Azure OpenAI Chat model
    llm = AzureChatOpenAI(
        temperature=0,
        api_version=azure_openai_api_version,
        azure_endpoint=azure_openai_endpoint,
        deployment_name=deployment_name
    )

    # Invoke the model with the messages
    try:
        response = llm.invoke(messages)
        return response
    except Exception as e:
        print(f"Error calling LLM: {e}")
        return None










import fitz  # PyMuPDF
import os
import base64
import openai

# Set your OpenAI API key
openai.api_key = "your-api-key"  # Replace with your OpenAI API key

# Extract images from PDF
def extract_images_with_pymupdf(pdf_path, output_folder):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)
    for page_number in range(len(doc)):
        page = doc[page_number]
        images = page.get_images(full=True)

        if not images:
            print(f"No images found on page {page_number + 1}")
            continue

        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]

            image_filename = os.path.join(
                output_folder, f"page_{page_number + 1}_{img_index + 1}.{image_ext}"
            )
            with open(image_filename, "wb") as image_file:
                image_file.write(image_bytes)
            print(f"Saved image: {image_filename}")

# Convert images in the output folder to Base64 strings
def images_to_base64(folder_path):
    base64_images = {}
    for image_file in sorted(os.listdir(folder_path)):
        if image_file.endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, image_file)
            with open(image_path, "rb") as file:
                encoded_string = base64.b64encode(file.read()).decode("utf-8")
                base64_images[image_file] = encoded_string
    return base64_images

# Call the LLM with the Base64 images
def call_llm_with_images(base64_images):
    messages = [{"role": "user", "content": "Please compare the following images:"}]
    for image_name, base64_string in base64_images.items():
        messages.append({
            "role": "user",
            "content": {
                "type": "image_url",
                "image_url": f"data:image/jpeg;base64,{base64_string}"
            }
        })

    # Call the LLM
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=messages
    )
    return response.choices[0].message["content"]

# Main Workflow
pdf_path = "example.pdf"  # Replace with your PDF path
output_folder = "extracted_images"

# Step 1: Extract images from the PDF
extract_images_with_pymupdf(pdf_path, output_folder)

# Step 2: Convert extracted images to Base64
base64_images = images_to_base64(output_folder)

# Step 3: Call LLM with Base64-encoded images
response = call_llm_with_images(base64_images)

# Step 4: Print LLM Response
print("LLM Response:")
print(response)











import fitz  # PyMuPDF
import os

def extract_images_with_pymupdf(pdf_path, output_folder):
    # Ensure the output folder exists
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)
    for page_number in range(len(doc)):
        page = doc[page_number]
        images = page.get_images(full=True)

        if not images:
            print(f"No images found on page {page_number + 1}")
            continue

        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]

            image_filename = os.path.join(
                output_folder, f"page_{page_number + 1}_{img_index + 1}.{image_ext}"
            )
            with open(image_filename, "wb") as image_file:
                image_file.write(image_bytes)
            print(f"Saved image: {image_filename}")

# Example Usage
pdf_path = "example.pdf"  # Replace with your PDF path
output_folder = "extracted_images"
extract_images_with_pymupdf(pdf_path, output_folder)









from pdfminer.pdfpage import PDFPage
from pdfminer.pdfparser import PDFParser
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdftypes import resolve1, stream_value
from PIL import Image
import os
import io

def extract_images_from_pdf(pdf_path, output_folder):
    # Ensure the output folder exists
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Open the PDF file
    with open(pdf_path, "rb") as file:
        parser = PDFParser(file)
        document = PDFDocument(parser)

        # Iterate through all pages in the PDF
        for page_number, page in enumerate(PDFPage.create_pages(document), start=1):
            resources = resolve1(page.resources)
            if resources and '/XObject' in resources:
                xobjects = resolve1(resources['/XObject'])
                for obj_name, obj in xobjects.items():
                    obj_resolved = resolve1(obj)
                    if obj_resolved.get('/Subtype') == '/Image':
                        try:
                            # Extract the image stream
                            image_stream = stream_value(obj_resolved)
                            image_data = image_stream.get_data()

                            # Get image properties
                            width = obj_resolved.get('/Width', 0)
                            height = obj_resolved.get('/Height', 0)
                            color_space = obj_resolved.get('/ColorSpace', '/DeviceRGB')

                            # Define image mode
                            mode = "RGB" if color_space == '/DeviceRGB' else "L"

                            # Convert image data to a PIL Image
                            image = Image.frombytes(mode, (width, height), image_data)

                            # Save the image
                            image_filename = os.path.join(
                                output_folder, f"page_{page_number}_{obj_name[1:]}.png"
                            )
                            image.save(image_filename)
                            print(f"Saved image: {image_filename}")
                        except Exception as e:
                            print(f"Error extracting image on page {page_number}: {e}")
            else:
                print(f"No XObject found on page {page_number}")

# Example Usage
pdf_path = "example.pdf"  # Replace with your PDF file path
output_folder = "extracted_images"
extract_images_from_pdf(pdf_path, output_folder)














import fitz

pdf_path = "example.pdf"  # Replace with your PDF path
doc = fitz.open(pdf_path)
for page_number in range(len(doc)):
    page = doc.load_page(page_number)
    pix = page.get_pixmap()
    print(f"Page {page_number + 1}: Width={pix.width}, Height={pix.height}")











from PIL import Image
import pytesseract
import fitz  # PyMuPDF
import os

# Configure Tesseract executable path (Windows only)
# pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def extract_text_from_pdf(pdf_path, output_folder):
    # Ensure the output folder exists
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)
    for page_number in range(len(doc)):
        page = doc[page_number]
        pix = page.get_pixmap()  # Render the page as an image
        image_path = os.path.join(output_folder, f"page_{page_number + 1}.png")
        pix.save(image_path)  # Save the rendered image
        print(f"Saved image: {image_path}")

        # Use Tesseract to extract text
        extracted_text = pytesseract.image_to_string(Image.open(image_path))
        text_file = os.path.join(output_folder, f"page_{page_number + 1}.txt")
        with open(text_file, "w", encoding="utf-8") as f:
            f.write(extracted_text)
        print(f"Extracted text saved to: {text_file}")

# Example Usage
pdf_path = "example.pdf"  # Replace with your PDF file path
output_folder = "output"
extract_text_from_pdf(pdf_path, output_folder)









import fitz  # PyMuPDF
import os

def extract_images_with_pymupdf(pdf_path, output_folder):
    # Ensure the output folder exists
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    doc = fitz.open(pdf_path)
    for page_number in range(len(doc)):
        page = doc.load_page(page_number)
        images = page.get_images(full=True)

        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]

            image_filename = os.path.join(
                output_folder, f"image_page_{page_number + 1}_{img_index + 1}.{image_ext}"
            )
            with open(image_filename, "wb") as image_file:
                image_file.write(image_bytes)
            print(f"Saved image: {image_filename}")

# Example Usage
pdf_path = "example.pdf"  # Replace with your PDF file path
output_folder = "extracted_images"
extract_images_with_pymupdf(pdf_path, output_folder)








from pdfminer.pdfpage import PDFPage
from pdfminer.pdfparser import PDFParser
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdftypes import resolve1, stream_value
from PIL import Image
import os
import io

def extract_images_from_pdf(pdf_path, output_folder):
    # Ensure the output folder exists
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Open the PDF file
    with open(pdf_path, "rb") as file:
        parser = PDFParser(file)
        document = PDFDocument(parser)

        # Iterate through the pages in the PDF
        for page_number, page in enumerate(PDFPage.create_pages(document), start=1):
            resources = resolve1(page.resources)
            if resources and '/XObject' in resources:
                xobjects = resolve1(resources['/XObject'])
                for obj_name, obj in xobjects.items():
                    obj_resolved = resolve1(obj)
                    if obj_resolved.get('/Subtype') == '/Image':
                        try:
                            # Extract image data
                            image_stream = stream_value(obj_resolved)
                            image_data = image_stream.get_data()
                            width = obj_resolved.get('/Width', 0)
                            height = obj_resolved.get('/Height', 0)

                            # Ensure valid width and height
                            if width > 0 and height > 0:
                                # Determine the color space
                                color_space = obj_resolved.get('/ColorSpace', '/DeviceRGB')
                                mode = "RGB" if color_space == '/DeviceRGB' else "L"

                                # Save the image
                                image = Image.frombytes(mode, (width, height), image_data)
                                image_filename = os.path.join(
                                    output_folder,
                                    f"image_page_{page_number}_{obj_name}.png"
                                )
                                image.save(image_filename)
                                print(f"Saved image: {image_filename}")
                        except Exception as e:
                            print(f"Error processing image on page {page_number}: {e}")

# Example Usage
pdf_path = "example.pdf"  # Replace with your PDF file path
output_folder = "extracted_images"
extract_images_from_pdf(pdf_path, output_folder)










from pdfminer.pdfpage import PDFPage
from pdfminer.pdfparser import PDFParser
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdftypes import resolve1, stream_value
from PIL import Image
import os
import io

def extract_images_from_pdf(pdf_path, output_folder):
    # Create the output folder if it doesn't exist
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Open the PDF file
    with open(pdf_path, "rb") as file:
        parser = PDFParser(file)
        document = PDFDocument(parser)

        # Iterate through the pages in the PDF
        for page_number, page in enumerate(PDFPage.create_pages(document), start=1):
            resources = resolve1(page.resources)
            if resources and '/XObject' in resources:
                xobjects = resolve1(resources['/XObject'])
                for obj_name, obj in xobjects.items():
                    obj_resolved = resolve1(obj)
                    # Check if the object is an image
                    if obj_resolved.get('/Subtype') == '/Image':
                        # Extract image data
                        image_stream = stream_value(obj_resolved)
                        image_data = image_stream.get_data()
                        width = obj_resolved.get('/Width', 0)
                        height = obj_resolved.get('/Height', 0)

                        if width > 0 and height > 0:
                            # Determine the color space
                            color_space = obj_resolved.get('/ColorSpace', '/DeviceRGB')
                            mode = "RGB" if color_space == '/DeviceRGB' else "L"

                            # Save the image using Pillow
                            image = Image.frombytes(mode, (width, height), image_data)
                            image_filename = f"{output_folder}/image_page_{page_number}_{obj_name}.png"
                            image.save(image_filename)
                            print(f"Saved image: {image_filename}")

# Example Usage
pdf_path = "example.pdf"  # Replace with your PDF file
output_folder = "extracted_images"
extract_images_from_pdf(pdf_path, output_folder)













from pdfminer.pdfpage import PDFPage
from pdfminer.pdfparser import PDFParser
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdftypes import resolve1, stream_value
from PIL import Image
import os
import io

def extract_images_from_pdf(pdf_path, output_folder):
    # Create the output folder if it doesn't exist
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Open the PDF file
    with open(pdf_path, "rb") as file:
        parser = PDFParser(file)
        document = PDFDocument(parser)

        # Iterate through the pages in the PDF
        for page_number, page in enumerate(PDFPage.create_pages(document), start=1):
            resources = resolve1(page.resources)
            if resources and 'XObject' in resources:
                xobjects = resolve1(resources['XObject'])
                for obj_name, obj in xobjects.items():
                    obj_resolved = resolve1(obj)
                    if obj_resolved['/Subtype'] == '/Image':
                        # Extract image data
                        image_stream = stream_value(obj_resolved)
                        image_data = image_stream.get_data()
                        width = obj_resolved['/Width']
                        height = obj_resolved['/Height']

                        # Determine the color space
                        color_space = obj_resolved.get('/ColorSpace', '/DeviceRGB')
                        mode = "RGB" if color_space == '/DeviceRGB' else "L"

                        # Save the image using Pillow
                        image = Image.frombytes(mode, (width, height), image_data)
                        image_filename = f"{output_folder}/image_page_{page_number}_{obj_name}.png"
                        image.save(image_filename)
                        print(f"Saved image: {image_filename}")

# Example Usage
pdf_path = "example.pdf"  # Replace with your PDF file
output_folder = "extracted_images"
extract_images_from_pdf(pdf_path, output_folder)














from pdfminer.pdfpage import PDFPage
from pdfminer.pdfparser import PDFParser
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdftypes import resolve1
from PIL import Image
import os
import io

def extract_images_from_pdf(pdf_path, output_folder):
    # Create the output folder if it doesn't exist
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Open the PDF file
    with open(pdf_path, "rb") as file:
        parser = PDFParser(file)
        document = PDFDocument(parser)

        # Check for image objects in the PDF
        for page_number, page in enumerate(PDFPage.create_pages(document), start=1):
            resources = resolve1(page.resources)
            if resources and 'XObject' in resources:
                xobjects = resolve1(resources['XObject'])
                for obj_name, obj in xobjects.items():
                    if obj.get('Subtype') == '/Image':
                        # Extract image data
                        image_data = obj.get_data()
                        width = obj.get('Width')
                        height = obj.get('Height')

                        # Handle different color spaces
                        color_space = obj.get('ColorSpace')
                        if color_space == '/DeviceRGB':
                            mode = "RGB"
                        elif color_space == '/DeviceGray':
                            mode = "L"
                        else:
                            mode = "P"

                        # Save the image using Pillow
                        image = Image.frombytes(mode, (width, height), image_data)
                        image_filename = f"{output_folder}/image_page_{page_number}_{obj_name[1:]}.png"
                        image.save(image_filename)
                        print(f"Saved image: {image_filename}")

# Example Usage
pdf_path = "example.pdf"  # Replace with your PDF file
output_folder = "extracted_images"
extract_images_from_pdf(pdf_path, output_folder)

















from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

def process_ocr_with_gpt4(content_dict, prompt_template, azure_openai_api_version, azure_openai_endpoint):
    """
    Process a dictionary of OCR text with GPT-4.

    Parameters:
        content_dict (dict): A dictionary where keys are page numbers and values are OCR text.
        prompt_template (str): The prompt template for GPT-4.
        azure_openai_api_version (str): Azure OpenAI API version.
        azure_openai_endpoint (str): Azure OpenAI endpoint URL.

    Returns:
        dict: A dictionary with the same keys, but values are the formatted text.
    """
    if not isinstance(content_dict, dict):
        raise ValueError("The input should be a dictionary where keys are page numbers and values are OCR text.")

    # Initialize the Azure OpenAI model
    llm = AzureChatOpenAI(
        temperature=0,
        api_version=azure_openai_api_version,
        azure_endpoint=azure_openai_endpoint
    )

    # Prepare the prompt template
    prompt = PromptTemplate(template=prompt_template, input_variables=["content"])

    # Initialize the LLM chain
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    # Dictionary to store formatted results
    formatted_content_dict = {}

    # Process each page's content
    for page_number, ocr_text in content_dict.items():
        try:
            if not isinstance(ocr_text, str):
                raise ValueError(f"Value for page {page_number} must be a string.")
            
            # Use the LLM chain to format the OCR text
            formatted_text = llm_chain.predict(content=ocr_text)
            formatted_content_dict[page_number] = formatted_text
        except Exception as e:
            print(f"Error formatting content for page {page_number} with GPT-4: {e}")
            formatted_content_dict[page_number] = f"Error: {e}"

    return formatted_content_dict










from langchain.chains import LLMChain

def process_ocr_with_gpt4(ocr_text_by_page, prompt_template, llm):
    formatted_ocr_by_page = {}

    # Process each page's OCR text with GPT-4
    for page_num, ocr_text in ocr_text_by_page.items():
        if not ocr_text.strip():  # Skip empty OCR text
            continue

        try:
            # Create the prompt template
            prompt = PromptTemplate(template=prompt_template, input_variables=["content"])

            # Initialize the LLMChain with the LLM and the prompt
            llm_chain = LLMChain(llm=llm, prompt=prompt)

            # Format the OCR text using GPT-4
            formatted_ocr_text = llm_chain.predict(content=ocr_text)

            # Save the formatted text by page
            formatted_ocr_by_page[page_num] = formatted_ocr_text

        except Exception as e:
            print(f"Error formatting OCR text for Page {page_num} with GPT-4: {e}")
            formatted_ocr_by_page[page_num] = f"Error formatting OCR text: {e}"

    return formatted_ocr_by_page









import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io

def extract_text_from_pdf(pdf_path):
    final_text = ""  # Combined text for all pages
    ocr_text_by_page = {}  # Store OCR-specific text by page number

    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page_text = ""

                # Step 1: Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # Step 2: Extract text and tables using pdfplumber
                try:
                    with pdfplumber.open(pdf_path) as pdf:
                        page_plumber = pdf.pages[page_num]
                        pdfplumber_text = page_plumber.extract_text()
                        page_text += pdfplumber_text or ""

                        # Extract table data
                        tables = page_plumber.extract_tables()
                        if tables:
                            for table in tables:
                                for row in table:
                                    row_text = " | ".join(
                                        str(cell) if not isinstance(cell, list) else " ".join(map(str, cell))
                                        for cell in row
                                    )
                                    page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # Step 3: Perform OCR extraction for images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_page_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_page_text  # Add OCR text to the current page
                            if page_num + 1 not in ocr_text_by_page:
                                ocr_text_by_page[page_num + 1] = ""
                            ocr_text_by_page[page_num + 1] += ocr_page_text  # Store OCR text by page
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the combined content of the current page
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text, ocr_text_by_page  # Return combined text and OCR-specific text by page



from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

def process_ocr_with_gpt4(ocr_text_by_page, prompt_template, llm):
    formatted_ocr_by_page = {}

    # Process each page's OCR text with GPT-4
    for page_num, ocr_text in ocr_text_by_page.items():
        if not ocr_text.strip():
            continue

        # Create the prompt template
        prompt = PromptTemplate(template=prompt_template, input_variables=["content"])

        # Initialize the LLMChain with the LLM and the prompt
        llm_chain = LLMChain(llm=llm, prompt=prompt)

        # Use GPT-4 to format the OCR text
        try:
            formatted_ocr_text = llm_chain.predict(content=ocr_text)
            formatted_ocr_by_page[page_num] = formatted_ocr_text
        except Exception as e:
            print(f"Error formatting OCR text for Page {page_num} with GPT-4: {e}")

    return formatted_ocr_by_page  # Return formatted OCR text by page




def combine_text_with_formatting(full_text, formatted_ocr_by_page):
    combined_text = ""  # Start with an empty string
    pages = full_text.split("\n--- Page ")  # Split full text by pages

    for page in pages:
        if not page.strip():
            continue

        # Extract the page number and content
        header, page_content = page.split("---\n", 1)
        page_num = int(header.strip())

        # Add the formatted OCR content for the current page
        if page_num in formatted_ocr_by_page:
            page_content += f"\n\n--- Formatted OCR Content ---\n{formatted_ocr_by_page[page_num]}"

        # Append the processed page back to the combined text
        combined_text += f"\n--- Page {page_num} ---\n{page_content}"

    return combined_text

# Define the PDF path
pdf_path = "path/to/your/pdf/document.pdf"

# Define the GPT-4 prompt template
prompt_template = """
You are a document formatter. Your task is to organize the following extracted OCR text into a structured and readable format.

Output the formatted content:
{content}
"""

# Initialize the LLM (GPT-4)
llm = OpenAI(
    temperature=0.7,
    api_version="your_api_version",
    azure_endpoint="your_azure_endpoint"
)

# Step 1: Extract text
full_text, ocr_text_by_page = extract_text_from_pdf(pdf_path)

# Step 2: Format OCR text with GPT-4
formatted_ocr_by_page = process_ocr_with_gpt4(ocr_text_by_page, prompt_template, llm)

# Step 3: Combine all content
final_output = combine_text_with_formatting(full_text, formatted_ocr_by_page)









from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.llms import OpenAI

# Initialize the OpenAI model (GPT-4) with appropriate parameters
llm = OpenAI(temperature=0.7, api_version="your_api_version", azure_endpoint="your_azure_endpoint")

# Define the prompt template
prompt_template = """
You are a document formatter. Your task is to organize the following extracted OCR text into a structured and readable format.

Output the formatted content:
{content}
"""

# Define the function to generate questions or organize content
def generate_questions(content, prompt_template):
    # Check if the content is a string
    if not isinstance(content, str):
        raise ValueError("The content should be a string.")

    # Create the prompt template using LangChain's PromptTemplate
    prompt = PromptTemplate(template=prompt_template, input_variables=["content"])

    # Initialize the LLMChain with the LLM and the prompt
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        # Ensure that the content is passed correctly
        response = llm_chain.predict(content=content)
        return response  # return the formatted content or response
    except Exception as e:
        print(f"Error formatting with GPT-4: {e}")
        return None

# Example usage
content = "This is the OCR-extracted content from your document."
formatted_content = generate_questions(content, prompt_template)
print(formatted_content)











import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import openai

# Azure OpenAI credentials
AZURE_OPENAI_API_KEY = "your_azure_openai_api_key"  # Replace with your Azure API key
AZURE_OPENAI_ENDPOINT = "your_azure_endpoint"       # Replace with your Azure endpoint
AZURE_OPENAI_DEPLOYMENT_ID = "gpt4-deployment-id"   # Replace with your GPT-4 deployment ID

# Set the path to Tesseract executable
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup


def extract_text_from_pdf(pdf_path):
    """
    Extract text, tables, and images from a PDF file.
    
    Args:
        pdf_path (str): Path to the PDF file.
    
    Returns:
        str: Extracted content from the PDF.
    """
    final_text = ""

    print("Starting extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                print(f"Processing Page {page_num + 1}...")
                page_text = ""

                # 1. Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # 2. Extract text using pdfplumber
                try:
                    with pdfplumber.open(pdf_path) as pdf:
                        page_plumber = pdf.pages[page_num]
                        pdfplumber_text = page_plumber.extract_text()
                        page_text += pdfplumber_text or ""

                        # Extract table data if available
                        table = page_plumber.extract_tables()
                        if table:
                            for row in table:
                                row_text = " | ".join(
                                    str(cell) if not isinstance(cell, list) else " ".join(map(str, cell))
                                    for cell in row
                                )
                                page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # 3. Perform OCR extraction from images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_text
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the page content to the final output
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                else:
                    print(f"No content found on Page {page_num + 1}")

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text


def format_with_gpt4(content):
    """
    Format extracted content using GPT-4.
    
    Args:
        content (str): The raw text content to format.
    
    Returns:
        str: The formatted content.
    """
    openai.api_key = AZURE_OPENAI_API_KEY
    openai.api_type = "azure"
    openai.api_base = AZURE_OPENAI_ENDPOINT
    openai.api_version = "2023-03-15-preview"  # Use the appropriate version for your setup

    # GPT-4 prompt to format content
    prompt = f"""
    You are a document formatter. Your task is to organize the following extracted OCR text into a structured and readable format with proper headings, sections, and tables if applicable:

    {content}

    Output the formatted content:
    """

    try:
        response = openai.ChatCompletion.create(
            engine=AZURE_OPENAI_DEPLOYMENT_ID,
            messages=[
                {"role": "system", "content": "You are an expert formatter."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=3000,
            temperature=0.3
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        print(f"Error formatting with GPT-4: {e}")
        return None















import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def extract_text_from_pdf(pdf_path):
    final_text = ""

    print("Starting extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                print(f"Processing Page {page_num + 1}...")
                page_text = ""
                
                # 1. Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # 2. Extract text using pdfplumber
                try:
                    with pdfplumber.open(pdf_path) as pdf:
                        page_plumber = pdf.pages[page_num]
                        pdfplumber_text = page_plumber.extract_text()
                        page_text += pdfplumber_text or ""

                        # Extract table data if available
                        table = page_plumber.extract_tables()
                        if table:
                            for row in table:
                                # Convert each cell in the row to string and join with " | "
                                row_text = " | ".join(
                                    str(cell) if not isinstance(cell, list) else " ".join(map(str, cell))
                                    for cell in row
                                )
                                page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # 3. Perform OCR extraction from images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_text
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the page content to the final output
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                else:
                    print(f"No content found on Page {page_num + 1}")

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal Extracted Text:\n", pdf_text)

# Optionally, save to a file
with open("extracted_text.txt", "w", encoding="utf-8") as f:
    f.write(pdf_text)










import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def extract_text_from_pdf(pdf_path):
    final_text = ""

    print("Starting extraction...")
    try:
        with fitz.open(pdf_path) as doc, pdfplumber.open(pdf_path) as pdf_plumber:
            for page_num in range(len(doc)):
                print(f"Processing Page {page_num + 1}...")
                page_text = ""
                
                # 1. Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # 2. Extract text using pdfplumber
                try:
                    page_plumber = pdf_plumber.pages[page_num]
                    pdfplumber_text = page_plumber.extract_text()
                    page_text += pdfplumber_text or ""

                    # Extract table data if available
                    table = page_plumber.extract_tables()
                    if table:
                        for row in table:
                            row_text = " | ".join(row)
                            page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # 3. Perform OCR extraction from images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_text
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the page content to the final output
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                else:
                    print(f"No content found on Page {page_num + 1}")

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal Extracted Text:\n", pdf_text)

# Optionally, save to a file
with open("extracted_text.txt", "w", encoding="utf-8") as f:
    f.write(pdf_text)










import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def extract_text_from_pdf(pdf_path):
    final_text = ""

    print("Starting extraction...")
    try:
        with fitz.open(pdf_path) as doc, pdfplumber.open(pdf_path) as pdf_plumber:
            for page_num in range(len(doc)):
                print(f"Processing Page {page_num + 1}...")
                page_text = ""
                
                # 1. Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # 2. Extract text using pdfplumber
                try:
                    page_plumber = pdf_plumber.pages[page_num]
                    pdfplumber_text = page_plumber.extract_text()
                    page_text += pdfplumber_text or ""

                    # Extract table data if available
                    table = page_plumber.extract_tables()
                    if table:
                        for row in table:
                            row_text = " | ".join(row)
                            page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # 3. Perform OCR extraction from images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_text
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the page content to the final output
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                else:
                    print(f"No content found on Page {page_num + 1}")

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal Extracted Text:\n", pdf_text)

# Optionally, save to a file
with open("extracted_text.txt", "w", encoding="utf-8") as f:
    f.write(pdf_text)








import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def extract_text_from_pdf(pdf_path):
    final_text = ""

    print("Starting extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                print(f"Processing Page {page_num + 1}...")
                page_text = ""
                
                # 1. Extract text using PyMuPDF
                try:
                    page = doc.load_page(page_num)
                    pymupdf_text = page.get_text("text")
                    page_text += pymupdf_text or ""
                except Exception as e:
                    print(f"PyMuPDF Error on Page {page_num + 1}: {e}")

                # 2. Extract text using pdfplumber
                try:
                    with pdfplumber.open(pdf_path) as pdf:
                        page_plumber = pdf.pages[page_num]
                        pdfplumber_text = page_plumber.extract_text()
                        page_text += pdfplumber_text or ""

                        # Extract table data if available
                        table = page_plumber.extract_tables()
                        if table:
                            for row in table:
                                row_text = " | ".join(row)
                                page_text += "\n" + row_text
                except Exception as e:
                    print(f"pdfplumber Error on Page {page_num + 1}: {e}")

                # 3. Perform OCR extraction from images
                try:
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        try:
                            image = Image.open(io.BytesIO(image_bytes))
                            ocr_text = pytesseract.image_to_string(image)
                            page_text += "\n" + ocr_text
                        except Exception as e:
                            print(f"OCR Error on Page {page_num + 1}, Image {img_index + 1}: {e}")
                except Exception as e:
                    print(f"OCR Extraction Error on Page {page_num + 1}: {e}")

                # Append the page content to the final output
                if page_text.strip():
                    final_text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                else:
                    print(f"No content found on Page {page_num + 1}")

    except Exception as e:
        print(f"Error processing the PDF: {e}")

    return final_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal Extracted Text:\n", pdf_text)

# Optionally, save to a file
with open("extracted_text.txt", "w", encoding="utf-8") as f:
    f.write(pdf_text)











import fitz  # PyMuPDF
import pdfplumber
from PIL import Image  # Ensure this is included
import pytesseract
import io
import sys

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    pymupdf_text = ""
    pdfplumber_text = ""
    ocr_text = ""

    # 1. Extract text using PyMuPDF
    print("Starting PyMuPDF extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                pymupdf_text += page_text or ""
                print(f"PyMuPDF Page {page_num + 1}:\n{page_text}")
    except Exception as e:
        print(f"PyMuPDF Error: {e}")

    # 2. Extract text using pdfplumber
    print("Starting pdfplumber extraction...")
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                pdfplumber_text += page_text or ""
                print(f"pdfplumber Page {page_num + 1}:\n{page_text}")
    except Exception as e:
        print(f"pdfplumber Error: {e}")

    # 3. Perform OCR extraction
    print("Starting OCR extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    try:
                        image = Image.open(io.BytesIO(image_bytes))
                        extracted_text = pytesseract.image_to_string(image)
                        ocr_text += extracted_text or ""
                        print(f"OCR Page {page_num + 1}, Image {img_index + 1}:\n{extracted_text}")
                    except Exception as e:
                        print(f"OCR Error: {e}")
    except Exception as e:
        print(f"OCR Extraction Error: {e}")

    return pymupdf_text + pdfplumber_text + ocr_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Update with your file path
result = extract_text_from_pdf(pdf_path)
print("Final Extracted Text:\n", result)









import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def extract_text_from_pdf(pdf_path):
    pymupdf_text = ""
    pdfplumber_text = ""
    ocr_text = ""

    # 1. Extract text using PyMuPDF
    print("Starting PyMuPDF extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                pymupdf_text += page_text or ""
                print(f"PyMuPDF Page {page_num + 1}:\n{page_text}")
    except Exception as e:
        print(f"PyMuPDF Error: {e}")

    # 2. Extract text using pdfplumber
    print("Starting pdfplumber extraction...")
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                pdfplumber_text += page_text or ""
                print(f"pdfplumber Page {page_num + 1}:\n{page_text}")
    except Exception as e:
        print(f"pdfplumber Error: {e}")

    # 3. Perform OCR extraction
    print("Starting OCR extraction...")
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    try:
                        image = Image.open(io.BytesIO(image_bytes))
                        extracted_text = pytesseract.image_to_string(image)
                        ocr_text += extracted_text or ""
                        print(f"OCR Page {page_num + 1}, Image {img_index + 1}:\n{extracted_text}")
                    except Exception as e:
                        print(f"OCR Error: {e}")
    except Exception as e:
        print(f"OCR Extraction Error: {e}")

    return pymupdf_text + pdfplumber_text + ocr_text

# Usage
pdf_path = "path_to_pdf.pdf"  # Update with your file path
result = extract_text_from_pdf(pdf_path)
print("Final Extracted Text:\n", result)













import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable (make sure it's properly set up)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    pymupdf_text = ""
    pdfplumber_text = ""
    ocr_text = ""

    # 1. Extract text from PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                pymupdf_text += page_text
                if page_text.strip():  # Print extracted text from PyMuPDF
                    print(f"Extracted text from page {page_num + 1} using PyMuPDF:\n{page_text}")
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
        sys.stdout.flush()

    # 2. Extract text from PDF using pdfplumber
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                pdfplumber_text += page_text if page_text else ""
                if page_text.strip():  # Print extracted text from pdfplumber
                    print(f"Extracted text from page {page_num + 1} using pdfplumber:\n{page_text}")
                    sys.stdout.flush()

                # Extract table data if available
                table = page.extract_tables()
                if table:
                    print(f"Extracted table from page {page_num + 1}:")
                    for row in table:
                        row_text = " | ".join(row)
                        print(row_text)
                        pdfplumber_text += "\n" + row_text  # Append table data to full text
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with pdfplumber: {e}")
        sys.stdout.flush()

    # 3. Perform OCR extraction from images
    print("Attempting OCR extraction from images (this will run even if regular text was found)...")
    sys.stdout.flush()
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    
                    # Perform OCR on the image and collect the extracted text
                    extracted_text = pytesseract.image_to_string(image)
                    if extracted_text.strip():  # Check if OCR text was found
                        ocr_text += extracted_text
                        print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                    else:
                        print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                    sys.stdout.flush()

    except Exception as e:
        print(f"Error extracting text using OCR: {e}")
        sys.stdout.flush()

    # Combine all extracted text
    full_text = pymupdf_text + pdfplumber_text + ocr_text
    return full_text

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal extracted text (including OCR and tables):\n", pdf_text)











import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable (make sure it's properly set up)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    full_text = ""
    
    # 1. Extract text from PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                full_text += page_text
                if page_text.strip():  # Print extracted text from PyMuPDF
                    print(f"Extracted text from page {page_num + 1} using PyMuPDF:\n{page_text}")
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
        sys.stdout.flush()

    # 2. Extract text from PDF using pdfplumber (to handle tables)
    if not full_text:  # Only try pdfplumber if no text was extracted yet
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    full_text += page_text
                    if page_text.strip():  # Print extracted text from pdfplumber
                        print(f"Extracted text from page {page_num + 1} using pdfplumber:\n{page_text}")
                        sys.stdout.flush()

                    # Extract table data if available
                    table = page.extract_tables()
                    if table:
                        print(f"Extracted table from page {page_num + 1}:")
                        for row in table:
                            row_text = " | ".join(row)
                            print(row_text)
                            full_text += "\n" + row_text  # Append table data to full text
                        sys.stdout.flush()
        except Exception as e:
            print(f"Error extracting text with pdfplumber: {e}")
            sys.stdout.flush()

    # 3. Perform OCR extraction from images (always attempt this, even if text is already found)
    print("Attempting OCR extraction from images (this will run even if regular text was found)...")
    sys.stdout.flush()
    ocr_text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    
                    # Perform OCR on the image and collect the extracted text
                    extracted_text = pytesseract.image_to_string(image)
                    if extracted_text.strip():  # Check if OCR text was found
                        ocr_text += extracted_text
                        print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                    else:
                        print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                    sys.stdout.flush()

        # Append OCR text to the full text
        if ocr_text.strip():
            full_text += ocr_text
        else:
            print("No OCR text extracted.")
            sys.stdout.flush()

    except Exception as e:
        print(f"Error extracting text using OCR: {e}")
        sys.stdout.flush()

    return full_text

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal extracted text (including OCR and tables):\n", pdf_text)










import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable (make sure it's properly set up)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    full_text = ""
    
    # 1. Extract text from PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                full_text += page_text
                if page_text.strip():  # Print extracted text from PyMuPDF
                    print(f"Extracted text from page {page_num + 1} using PyMuPDF:\n{page_text}")
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
        sys.stdout.flush()

    # 2. Extract text from PDF using pdfplumber (to handle tables)
    if not full_text:  # Only try pdfplumber if no text was extracted yet
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    full_text += page_text
                    if page_text.strip():  # Print extracted text from pdfplumber
                        print(f"Extracted text from page {page_num + 1} using pdfplumber:\n{page_text}")
                        sys.stdout.flush()

                    # Extract table data if available
                    table = page.extract_tables()
                    if table:
                        print(f"Extracted table from page {page_num + 1}:")
                        for row in table:
                            row_text = " | ".join(row)
                            print(row_text)
                            full_text += "\n" + row_text  # Append table data to full text
                        sys.stdout.flush()
        except Exception as e:
            print(f"Error extracting text with pdfplumber: {e}")
            sys.stdout.flush()

    # 3. Perform OCR extraction from images (always attempt this, even if text is already found)
    print("Attempting OCR extraction from images (this will run even if regular text was found)...")
    sys.stdout.flush()
    ocr_text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    
                    # Perform OCR on the image and collect the extracted text
                    extracted_text = pytesseract.image_to_string(image)
                    if extracted_text.strip():  # Check if OCR text was found
                        ocr_text += extracted_text
                        print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                    else:
                        print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                    sys.stdout.flush()

        # Append OCR text to the full text
        if ocr_text.strip():
            full_text += ocr_text
        else:
            print("No OCR text extracted.")
            sys.stdout.flush()

    except Exception as e:
        print(f"Error extracting text using OCR: {e}")
        sys.stdout.flush()

    return full_text

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal extracted text (including OCR and tables):\n", pdf_text)






import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable (make sure it's properly set up)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    full_text = ""
    
    # 1. Extract text from PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                full_text += page_text
                if page_text.strip():  # Print extracted text from PyMuPDF
                    print(f"Extracted text from page {page_num + 1} using PyMuPDF:\n{page_text}")
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
        sys.stdout.flush()

    # 2. Extract text from PDF using pdfplumber (to handle tables)
    if not full_text:  # Only try pdfplumber if no text was extracted yet
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    full_text += page_text
                    if page_text.strip():  # Print extracted text from pdfplumber
                        print(f"Extracted text from page {page_num + 1} using pdfplumber:\n{page_text}")
                        sys.stdout.flush()

                    # Extract table data if available
                    table = page.extract_tables()
                    if table:
                        print(f"Extracted table from page {page_num + 1}:")
                        for row in table:
                            row_text = " | ".join(row)
                            print(row_text)
                            full_text += "\n" + row_text  # Append table data to full text
                        sys.stdout.flush()
        except Exception as e:
            print(f"Error extracting text with pdfplumber: {e}")
            sys.stdout.flush()

    # 3. Perform OCR extraction from images (always attempt this, even if text is already found)
    print("Attempting OCR extraction from images (this will run even if regular text was found)...")
    sys.stdout.flush()
    ocr_text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    
                    # Perform OCR on the image and collect the extracted text
                    extracted_text = pytesseract.image_to_string(image)
                    if extracted_text.strip():  # Check if OCR text was found
                        ocr_text += extracted_text
                        print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                    else:
                        print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                    sys.stdout.flush()

        # Append OCR text to the full text
        if ocr_text.strip():
            full_text += ocr_text
        else:
            print("No OCR text extracted.")
            sys.stdout.flush()

    except Exception as e:
        print(f"Error extracting text using OCR: {e}")
        sys.stdout.flush()

    return full_text

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal extracted text (including OCR and tables):\n", pdf_text)









import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import io
import sys

# Set the path to the Tesseract executable (make sure it's properly set up)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Adjust for your setup

def extract_text_from_pdf(pdf_path):
    full_text = ""
    
    # 1. Extract text from PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                full_text += page_text
                if page_text.strip():  # Print extracted text from PyMuPDF
                    print(f"Extracted text from page {page_num + 1} using PyMuPDF:\n{page_text}")
                    sys.stdout.flush()
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
        sys.stdout.flush()

    # 2. Extract text from PDF using pdfplumber (to handle tables)
    if not full_text:  # Only try pdfplumber if no text was extracted yet
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    full_text += page_text
                    if page_text.strip():  # Print extracted text from pdfplumber
                        print(f"Extracted text from page {page_num + 1} using pdfplumber:\n{page_text}")
                        sys.stdout.flush()

                    # Extract table data if available
                    table = page.extract_tables()
                    if table:
                        print(f"Extracted table from page {page_num + 1}:")
                        for row in table:
                            row_text = " | ".join(row)
                            print(row_text)
                            full_text += "\n" + row_text  # Append table data to full text
                        sys.stdout.flush()
        except Exception as e:
            print(f"Error extracting text with pdfplumber: {e}")
            sys.stdout.flush()

    # 3. If no text found, extract OCR text from images (using pytesseract)
    if not full_text:
        print("No regular text found, attempting OCR extraction from images...")
        sys.stdout.flush()
        ocr_text = ""
        try:
            with fitz.open(pdf_path) as doc:
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        image = Image.open(io.BytesIO(image_bytes))
                        
                        # Perform OCR on the image and collect the extracted text
                        extracted_text = pytesseract.image_to_string(image)
                        if extracted_text.strip():  # Check if OCR text was found
                            ocr_text += extracted_text
                            print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                        else:
                            print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                        sys.stdout.flush()

            # Append OCR text to the main text variable
            if ocr_text.strip():
                full_text += ocr_text
            else:
                print("No OCR text extracted.")
                sys.stdout.flush()

        except Exception as e:
            print(f"Error extracting text using OCR: {e}")
            sys.stdout.flush()

    return full_text

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
pdf_text = extract_text_from_pdf(pdf_path)
print("\nFinal extracted text (including OCR and tables):\n", pdf_text)











https://github.com/UB-Mannheim/tesseract/wiki



import fitz  # PyMuPDF
from PIL import Image
import pytesseract
import io
import sys

# Function to extract OCR text from images in the PDF
def extract_ocr_from_pdf(pdf_path):
    ocr_text = ""
    print("Attempting OCR extraction from images...")

    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                # Extract images
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image = Image.open(io.BytesIO(image_bytes))
                    
                    # Perform OCR on the image and collect the extracted text
                    extracted_text = pytesseract.image_to_string(image)
                    if extracted_text.strip():  # Check if any OCR text was found
                        ocr_text += extracted_text
                        print(f"OCR text extracted from page {page_num + 1}, image {img_index + 1}:\n{extracted_text}")
                    else:
                        print(f"No OCR text found in image on page {page_num + 1}, image {img_index + 1}")
                    sys.stdout.flush()

        if ocr_text.strip():
            print("\nFinal OCR extracted text:\n")
            print(ocr_text)  # Print the full OCR text if any was extracted
        else:
            print("No OCR text extracted from any images.")
        
    except Exception as e:
        print(f"Error extracting text using OCR: {e}")
        sys.stdout.flush()

# Example usage
pdf_path = "path_to_pdf.pdf"  # Replace with the path to your PDF
extract_ocr_from_pdf(pdf_path)










import os
import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
import openpyxl

def extract_text_from_pdf(pdf_path):
    text = ""
    
    # Try to extract text from the PDF using PyMuPDF (fitz)
    try:
        with fitz.open(pdf_path) as doc:
            for page in doc:
                text += page.get_text("text")
    except Exception as e:
        print(f"Error extracting text with PyMuPDF: {e}")
    
    # If PyMuPDF doesn't work, try pdfplumber for better handling of tables and text
    if not text:
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page in pdf.pages:
                    text += page.extract_text()
        except Exception as e:
            print(f"Error extracting text with pdfplumber: {e}")
    
    # If text is still empty, use OCR (pytesseract) to extract text from images within the PDF
    if not text:
        try:
            with fitz.open(pdf_path) as doc:
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)
                    # Extract images
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        image = Image.open(io.BytesIO(image_bytes))
                        text += pytesseract.image_to_string(image)
        except Exception as e:
            print(f"Error extracting text using OCR: {e}")
    
    return text

def generate_questions_by_all_types(text, total_questions, chunk_size):
    # Dummy function for generating questions
    # Replace with your own logic
    return [{"question": f"Question {i}", "answer": "Sample answer"} for i in range(1, total_questions+1)]

def save_to_excel(questions_and_answers, output_file):
    # Save the questions and answers to an Excel file
    wb = openpyxl.Workbook()
    ws = wb.active
    ws.append(["Question", "Answer"])

    for qa in questions_and_answers:
        ws.append([qa["question"], qa["answer"]])
    
    wb.save(output_file)

def process_pdfs(root_directory, output_folder, total_questions=5, chunk_size=500):
    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")

# Example usage
root_directory = "path_to_pdf_folder"
output_folder = "path_to_output_folder"
process_pdfs(root_directory, output_folder)












import os
import time
import signal
import sys
import PyPDF2
import pandas as pd
import json
import tiktoken
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import logging

# Configure logging
logging.basicConfig(
    filename="batch_processing.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

def log_info(message):
    logging.info(message)
    print(message)

def log_error(message):
    logging.error(message)
    print(message)

# Function to calculate tokens using tiktoken
def calculate_tokens(text, model="gpt-4"):
    try:
        if model in ["gpt-3.5-turbo", "gpt-4"]:
            encoding = tiktoken.get_encoding("cl100k_base")
        elif model == "text-davinci-003":
            encoding = tiktoken.get_encoding("p50k_base")
        else:
            raise ValueError(f"Unsupported model: {model}")
        return len(encoding.encode(text))
    except Exception as e:
        log_error(f"Error calculating tokens: {e}")
        return 0

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    try:
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            for page in reader.pages:
                text += page.extract_text()
    except Exception as e:
        log_error(f"Error extracting text from {pdf_path}: {e}")
    return text

# Function to divide items into batches
def divide_into_batches(items, batch_size):
    for i in range(0, len(items), batch_size):
        yield items[i:i + batch_size]

# Prompts for question generation
FACTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.

    Input Text: {paragraph}
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.

    Input Text: {paragraph}
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Questions and answers must be distinct, unique, and not repeated.

    Input Text: {paragraph}
"""

# Function to generate questions for a batch of paragraphs
def generate_questions_for_batch(paragraphs, total_questions_per_paragraph, chunk_size=10):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        for i, paragraph in enumerate(paragraphs):
            log_info(f"Processing {question_type} questions for paragraph {i + 1}/{len(paragraphs)}...")
            try:
                llm = AzureChatOpenAI(
                    temperature=0,
                    azure_openai_api_base="https://<your-endpoint>.openai.azure.com/",
                    deployment_name="gpt4-0",
                    api_version="2023-06-01-preview"
                )

                prompt_template = PromptTemplate(
                    input_variables=["paragraph", "num_questions"],
                    template=prompt
                )

                llm_chain = LLMChain(llm=llm, prompt=prompt_template)
                response = llm_chain.predict(paragraph=paragraph, num_questions=str(total_questions_per_paragraph))
                questions = json.loads(response)

                for qa in questions:
                    question_text = qa["question"]
                    answer_text = qa["answer"]
                    all_questions.append({
                        "Question Type": question_type,
                        "Question": question_text,
                        "Answer": answer_text,
                        "Question Length (Chars)": len(question_text),
                        "Answer Length (Chars)": len(str(answer_text)),
                        "Question Tokens": calculate_tokens(question_text),
                        "Answer Tokens": calculate_tokens(answer_text)
                    })
            except Exception as e:
                log_error(f"Error processing {question_type} questions for paragraph {i + 1}: {e}")
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        log_info(f"Questions and answers saved to {output_file}.")
    else:
        log_info(f"No data to save for {output_file}.")

# Function to continuously monitor and process PDFs
def monitor_and_process(root_directory, batch_size=5, total_questions=10, chunk_size=10, output_folder="output", interval=300):
    log_info("Starting continuous monitoring...")
    processed_files = set()

    while True:
        try:
            pdf_files = []
            for dirpath, _, filenames in os.walk(root_directory):
                for filename in filenames:
                    if filename.endswith('.pdf'):
                        pdf_files.append(os.path.join(dirpath, filename))

            new_files = [f for f in pdf_files if f not in processed_files]

            if new_files:
                log_info(f"Found {len(new_files)} new files. Processing...")
                pdf_batches = list(divide_into_batches(new_files, batch_size))

                for batch_index, batch in enumerate(pdf_batches):
                    log_info(f"Processing batch {batch_index + 1}/{len(pdf_batches)}...")
                    paragraphs = [extract_text_from_pdf(pdf_path) for pdf_path in batch]
                    batch_questions = generate_questions_for_batch(paragraphs, total_questions, chunk_size)
                    batch_output_file = os.path.join(output_folder, f"batch_{batch_index + 1}_questions.xlsx")
                    save_to_excel(batch_questions, batch_output_file)

                processed_files.update(new_files)
            else:
                log_info("No new files found.")

        except Exception as e:
            log_error(f"Error during monitoring: {e}")

        time.sleep(interval)

# Graceful shutdown
def graceful_shutdown(signal_received, frame):
    log_info("Shutting down gracefully...")
    sys.exit(0)

signal.signal(signal.SIGINT, graceful_shutdown)
signal.signal(signal.SIGTERM, graceful_shutdown)

# Main execution
if __name__ == "__main__":
    root_directory = "path/to/pdf/folder"
    output_folder = "path/to/output/folder"
    batch_size = 5
    total_questions = 10
    chunk_size = 10
    interval = 300

    monitor_and_process(root_directory, batch_size, total_questions, chunk_size, output_folder, interval)
















import json

def json_to_text(json_obj):
    # Convert JSON object to a human-readable text format
    text = ""
    for key, value in json_obj.items():
        if isinstance(value, dict):  # Handle nested dictionaries
            text += f"{key}:\n"
            text += json_to_text(value)  # Recursive call for nested dictionaries
        elif isinstance(value, list):  # Handle lists
            text += f"{key}: {', '.join(map(str, value))}\n"
        else:
            text += f"{key}: {value}\n"
    return text

# Read JSON data from a file
file_path = "data.json"  # Replace with the path to your JSON file

try:
    with open(file_path, "r") as json_file:
        json_data = json.load(json_file)  # Load JSON data from the file

    # Convert JSON to plain text
    text_output = json_to_text(json_data)

    # Print or save the text output
    print(text_output)

    # Optionally save the text to another file
    with open("output.txt", "w") as text_file:
        text_file.write(text_output)

except FileNotFoundError:
    print(f"Error: File '{file_path}' not found.")
except json.JSONDecodeError:
    print(f"Error: File '{file_path}' is not a valid JSON file.")







import json

def json_to_text(json_obj):
    # Convert JSON object to a human-readable text format
    text = ""
    for key, value in json_obj.items():
        if isinstance(value, dict):  # Handle nested dictionaries
            text += f"{key}:\n"
            text += json_to_text(value)  # Recursive call for nested dictionaries
        elif isinstance(value, list):  # Handle lists
            text += f"{key}: {', '.join(map(str, value))}\n"
        else:
            text += f"{key}: {value}\n"
    return text

# Example JSON object
json_data = {
    "name": "John Doe",
    "age": 30,
    "address": {
        "street": "123 Main St",
        "city": "New York",
        "zipcode": "10001"
    },
    "hobbies": ["reading", "traveling", "coding"]
}

# Convert JSON to plain text
text_output = json_to_text(json_data)

# Print the result
print(text_output)














import os
import PyPDF2
import pandas as pd
import json
import tiktoken
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

# Function to calculate tokens using tiktoken
def calculate_tokens(text, model="gpt-3.5-turbo"):
    """
    Calculate the number of tokens in a given text using tiktoken.

    Args:
        text (str): Input text.
        model (str): Model name to determine the encoding (default: "gpt-3.5-turbo").

    Returns:
        int: Number of tokens in the text.
    """
    try:
        if model in ["gpt-3.5-turbo", "gpt-4"]:
            encoding = tiktoken.get_encoding("cl100k_base")
        elif model == "text-davinci-003":
            encoding = tiktoken.get_encoding("p50k_base")
        else:
            raise ValueError(f"Unsupported model: {model}")

        return len(encoding.encode(text))
    except Exception as e:
        print(f"Error calculating tokens: {e}")
        return 0

# Prompts
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.

    Args:
        pdf_path (str): Path to the PDF file.

    Returns:
        str: Extracted text.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.

    Args:
        response (str): Response string.

    Returns:
        list: Parsed JSON list.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to remove duplicate questions and answers
def remove_duplicates(questions):
    """
    Remove duplicate questions and answers.

    Args:
        questions (list): List of question-answer dictionaries.

    Returns:
        list: Unique question-answer pairs.
    """
    seen_questions = set()
    seen_answers = set()
    unique_questions = []

    for qa in questions:
        question_text = qa["question"]
        answer_text = str(qa["answer"])  # Convert list-type answers to strings for comparison

        if question_text not in seen_questions and answer_text not in seen_answers:
            seen_questions.add(question_text)
            seen_answers.add(answer_text)
            unique_questions.append(qa)

    return unique_questions

# Function to generate questions by all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, ensuring uniqueness.

    Args:
        paragraph (str): Input text.
        total_questions (int): Total number of questions to generate.
        chunk_size (int): Number of questions to generate per chunk.

    Returns:
        list: List of question-answer dictionaries with metadata.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)

        # Remove duplicates
        questions = remove_duplicates(questions)

        # Add metadata
        for qa in questions:
            question_text = qa["question"]
            answer_text = qa["answer"]
            all_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text,
                "Question Length (Chars)": len(question_text),
                "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                "Question Tokens": calculate_tokens(question_text),
                "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else " ".join(answer_text))
            })

        # Break if the desired number of questions is reached
        if len(all_questions) >= total_questions:
            break

    # Return the top N unique questions
    return all_questions[:total_questions]

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions and answers to an Excel file.

    Args:
        questions_and_answers (list): List of question-answer dictionaries.
        output_file (str): Path to the output Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.

    Args:
        root_directory (str): Root directory containing PDF files.
        total_questions (int): Number of questions to generate per file.
        chunk_size (int): Number of questions to generate per chunk.
        output_folder (str): Directory to save the output Excel files.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")













import os
import PyPDF2
import pandas as pd
import json
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

# Tokenization (Whitespace-Based)
def calculate_tokens(text):
    """
    Calculate the number of tokens in a given text using whitespace tokenization.
    """
    return len(text.split())

# Prompts
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct inferential question-answer pairs based on the input text.

    Ensure:
    - Questions require drawing conclusions or making interpretations beyond the direct information in the text.
    - Answers justify the inference with clear logical reasoning and include the reasoning process explicitly.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What inference can be made about {specific topic} based on the input text?",
            "answer": "The inference that can be drawn is {logical explanation with reasoning}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or abstract ideas mentioned in the text.
    - Answers provide a detailed explanation of the underlying concept or principle, including why it is important.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the concept of {specific principle or idea} in the input text?",
            "answer": "The concept refers to {detailed explanation and reasoning about its importance}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to remove duplicate questions and answers
def remove_duplicates(questions):
    """
    Remove duplicate questions and answers.
    """
    seen_questions = set()
    seen_answers = set()
    unique_questions = []

    for qa in questions:
        question_text = qa["question"]
        answer_text = str(qa["answer"])  # Convert list-type answers to strings for comparison

        if question_text not in seen_questions and answer_text not in seen_answers:
            seen_questions.add(question_text)
            seen_answers.add(answer_text)
            unique_questions.append(qa)

    return unique_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, ensuring uniqueness.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)

        # Remove duplicates
        questions = remove_duplicates(questions)

        # Add metadata
        for qa in questions:
            question_text = qa["question"]
            answer_text = qa["answer"]
            all_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text,
                "Question Length (Chars)": len(question_text),
                "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                "Question Tokens": calculate_tokens(question_text),
                "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else " ".join(answer_text))
            })

        # Break if the desired number of questions is reached
        if len(all_questions) >= total_questions:
            break

    # Return the top N unique questions
    return all_questions[:total_questions]

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions, answers, and their lengths to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")













import os
import PyPDF2
import pandas as pd
import json
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

# Tokenization (Whitespace-Based)
def calculate_tokens(text):
    """
    Calculate the number of tokens in a given text using whitespace tokenization.
    """
    return len(text.split())

# Prompts
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct inferential question-answer pairs based on the input text.

    Ensure:
    - Questions require interpretation or conclusions derived from the input text.
    - Answers justify the inference with clear logical reasoning.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What inference can be made about {specific topic}?",
            "answer": "The inference that can be drawn is {logical explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or core concepts mentioned in the text.
    - Answers provide a detailed explanation of the underlying concept or principle.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the concept of {specific principle or idea} in the input text?",
            "answer": "The concept refers to {detailed explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to remove duplicate questions and answers
def remove_duplicates(questions):
    """
    Remove duplicate questions and answers.
    """
    seen_questions = set()
    seen_answers = set()
    unique_questions = []

    for qa in questions:
        question_text = qa["question"]
        answer_text = str(qa["answer"])  # Convert list-type answers to strings for comparison

        if question_text not in seen_questions and answer_text not in seen_answers:
            seen_questions.add(question_text)
            seen_answers.add(answer_text)
            unique_questions.append(qa)

    return unique_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, ensuring uniqueness.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)

        # Remove duplicates
        questions = remove_duplicates(questions)

        # Add metadata
        for qa in questions:
            question_text = qa["question"]
            answer_text = qa["answer"]
            all_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text,
                "Question Length (Chars)": len(question_text),
                "Answer Length (Chars)": len(str(answer_text)) if isinstance(answer_text, str) else sum(len(str(a)) for a in answer_text),
                "Question Tokens": calculate_tokens(question_text),
                "Answer Tokens": calculate_tokens(answer_text if isinstance(answer_text, str) else " ".join(answer_text))
            })

        # Break if the desired number of questions is reached
        if len(all_questions) >= total_questions:
            break

    # Return the top N unique questions
    return all_questions[:total_questions]

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions, answers, and their lengths to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")


















import os
import PyPDF2
import pandas as pd
import json
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

# Tokenization (Whitespace-Based)
def calculate_tokens(text):
    """
    Calculate the number of tokens in a given text using whitespace tokenization.
    """
    return len(text.split())

# Prompts
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct inferential question-answer pairs based on the input text.

    Ensure:
    - Questions require interpretation or conclusions derived from the input text.
    - Answers justify the inference with clear logical reasoning.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What inference can be made about {specific topic}?",
            "answer": "The inference that can be drawn is {logical explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or core concepts mentioned in the text.
    - Answers provide a detailed explanation of the underlying concept or principle.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the concept of {specific principle or idea} in the input text?",
            "answer": "The concept refers to {detailed explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to remove duplicate questions and answers
def remove_duplicates(questions):
    """
    Remove duplicate questions and answers.
    """
    seen_questions = set()
    seen_answers = set()
    unique_questions = []

    for qa in questions:
        question_text = qa["question"]
        answer_text = str(qa["answer"])  # Convert list-type answers to strings for comparison

        if question_text not in seen_questions and answer_text not in seen_answers:
            seen_questions.add(question_text)
            seen_answers.add(answer_text)
            unique_questions.append(qa)

    return unique_questions

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    """
    Generate questions in chunks to avoid exceeding token limits.
    """
    all_questions = []
    for _ in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, chunk_size, prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, ensuring uniqueness.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)

        # Remove duplicates
        questions = remove_duplicates(questions)

        all_questions.extend(questions)

        # Break if the desired number of questions is reached
        if len(all_questions) >= total_questions:
            break

    # Return the top N unique questions
    return all_questions[:total_questions]

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions, answers, and their lengths to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")
















import os
import PyPDF2
import pandas as pd
import json
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI

# Tokenization (Whitespace-Based)
def calculate_tokens(text):
    """
    Calculate the number of tokens in a given text using whitespace tokenization.
    """
    return len(text.split())

# Prompts
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions.
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions and answers must be distinct, unique, and not repeated.
    - If fewer unique questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to remove duplicate questions and answers
def remove_duplicates(questions):
    """
    Remove duplicate questions and answers.
    """
    seen_questions = set()
    seen_answers = set()
    unique_questions = []

    for qa in questions:
        question_text = qa["question"]
        answer_text = qa["answer"]

        if question_text not in seen_questions and answer_text not in seen_answers:
            seen_questions.add(question_text)
            seen_answers.add(answer_text)
            unique_questions.append(qa)

    return unique_questions

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    """
    Generate questions in chunks to avoid exceeding token limits.
    """
    all_questions = []
    for _ in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, chunk_size, prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, ensuring uniqueness.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []

    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)

        # Remove duplicates
        questions = remove_duplicates(questions)

        all_questions.extend(questions)

        # Break if the desired number of questions is reached
        if len(all_questions) >= total_questions:
            break

    # Return the top N unique questions
    return all_questions[:total_questions]

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions, answers, and their lengths to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")














# Prompts for Factual, Procedural, Inferential, Conceptual, and Reasoning-based questions

FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate up to {num_questions} distinct factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, names, titles, or entities mentioned in the input text.
    - Answers are precise and concise.
    - Questions and answers must be unique and should not repeat.
    - If fewer distinct questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions with clear descriptions for each step.
    - Questions and answers must be unique and should not repeat.
    - If fewer distinct questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed."
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct inferential question-answer pairs based on the input text.

    Ensure:
    - Questions require interpretation or conclusions derived from the input text.
    - Answers justify the inference with clear logical reasoning.
    - Questions and answers must be unique and should not repeat.
    - If fewer distinct questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What inference can be made about {specific topic}?",
            "answer": "The inference that can be drawn is {logical explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or core concepts mentioned in the text.
    - Answers provide a detailed explanation of the underlying concept or principle.
    - Questions and answers must be unique and should not repeat.
    - If fewer distinct questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the concept of {specific principle or idea} in the input text?",
            "answer": "The concept refers to {detailed explanation}."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate up to {num_questions} distinct reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Answers justify conclusions with clear and logical reasoning.
    - Questions and answers must be unique and should not repeat.
    - If fewer distinct questions are possible, return only the available ones.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""



















import os
import tiktoken
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Tokenizer setup
tokenizer = tiktoken.get_encoding("cl100k_base")  # Use the appropriate encoding for your model

# Prompts remain the same as before

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to calculate token counts
def calculate_tokens(text):
    """
    Calculate the number of tokens in a given text.
    """
    return len(tokenizer.encode(text))

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    """
    Generate questions in chunks to avoid exceeding token limits.
    """
    all_questions = []
    for _ in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, chunk_size, prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    """
    Generate categorized questions and answers for all types, including token and character counts.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)
        for qa in questions:
            question_text = qa["question"]
            answer_text = qa["answer"]

            # Calculate token and character counts
            question_tokens = calculate_tokens(question_text)
            answer_tokens = calculate_tokens(answer_text if isinstance(answer_text, str) else " ".join(answer_text))
            answer_length = len(answer_text) if isinstance(answer_text, str) else sum(len(str(ans)) for ans in answer_text)

            # Append question type, question, answer, and their counts
            all_questions.append({
                "Question Type": question_type,
                "Question": question_text,
                "Answer": answer_text,
                "Question Length (Chars)": len(question_text),
                "Answer Length (Chars)": answer_length,
                "Question Tokens": question_tokens,
                "Answer Tokens": answer_tokens
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    """
    Save the questions, answers, and their lengths to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")



















import os
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Updated Factual Prompt
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate {num_questions} factual question-answer pairs based on the input text.

    Ensure:
    - Questions are focused on names, titles, or specific entities mentioned in the input text.
    - Answers are precise and concise.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Updated Procedural Prompt
PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions with clear descriptions for each step.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Brief description of the first step.",
                "Step 2: Brief description of the second step.",
                "Step 3: Add additional steps as needed.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Updated Reasoning-Based Prompt
REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Answers justify conclusions with clear and logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to validate and extract valid JSON from responses
def extract_valid_json(response):
    """
    Extract valid JSON from a response string.
    """
    try:
        json_start = response.find('[')
        json_end = response.rfind(']')
        if json_start != -1 and json_end != -1:
            return json.loads(response[json_start:json_end + 1])
    except json.JSONDecodeError:
        print("Error: Invalid JSON detected.")
    return []

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return extract_valid_json(response)
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions in chunks
def generate_questions_in_chunks(paragraph, question_type, total_questions, prompt_template, chunk_size=10):
    """
    Generate questions in chunks to avoid exceeding token limits.
    """
    all_questions = []
    for _ in range(0, total_questions, chunk_size):
        try:
            chunk_questions = generate_questions(paragraph, question_type, chunk_size, prompt_template)
            all_questions.extend(chunk_questions)
        except Exception as e:
            print(f"Error generating chunk for {question_type}: {e}")
            continue
    return all_questions

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, total_questions, chunk_size=10):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions_in_chunks(paragraph, question_type, total_questions, prompt, chunk_size)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, total_questions=10, chunk_size=10, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, total_questions, chunk_size)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")















import os
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Updated Factual Prompt
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate {num_questions} factual question-answer pairs based on the input text.

    Ensure:
    - Questions are focused on names, titles, or specific entities mentioned in the input text.
    - Answers are precise and concise.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What is the name of the entity mentioned in the input text?",
            "answer": "Name or entity mentioned in the input text."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Updated Procedural Prompt
PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step instructions with clear descriptions for each step.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "What are the steps to achieve or complete this process?",
            "answer": [
                "Step 1: Describe the first action or decision clearly.",
                "Step 2: Provide the next step with clarity and details.",
                "Step 3: Add any additional steps needed for completion.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Updated Reasoning-Based Prompt
REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} reasoning-based question-answer pairs based on the input text.

    Ensure:
    - All questions begin with "Why".
    - Questions require logical argumentation, critical thinking, or evaluation.
    - Answers justify conclusions with clear and logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Why does this happen or exist based on the input text?",
            "answer": "Detailed reasoning or justification for the question."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON for {question_type} questions.")
        return []
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, num_questions):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions(paragraph, question_type, num_questions, prompt)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to sav

















import os
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Prompts remain the same as earlier

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON for {question_type} questions.")
        return []
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, num_questions):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Explanatory": EXPLANATORY_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions(paragraph, question_type, num_questions, prompt)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory and its subdirectories
def process_pdfs_in_subdirectories(root_directory, num_questions=3, output_folder="output"):
    """
    Processes all PDF files in the given directory and its subdirectories.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for dirpath, _, filenames in os.walk(root_directory):
        for filename in filenames:
            if filename.endswith('.pdf'):
                pdf_path = os.path.join(dirpath, filename)
                try:
                    print(f"Processing: {pdf_path}")
                    # Extract text from the PDF
                    pdf_text = extract_text_from_pdf(pdf_path)

                    # Generate questions
                    questions_and_answers = generate_questions_by_all_types(pdf_text, num_questions)

                    # Save results to Excel
                    relative_path = os.path.relpath(pdf_path, root_directory)
                    output_file = os.path.join(output_folder, relative_path.replace(".pdf", "_questions.xlsx"))

                    # Ensure the subdirectory structure in the output folder
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    save_to_excel(questions_and_answers, output_file)
                except Exception as e:
                    print(f"Error processing {pdf_path}: {e}")


















import os
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Prompts remain the same as provided in earlier examples

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON for {question_type} questions.")
        return []
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Function to generate questions for all types
def generate_questions_by_all_types(paragraph, num_questions):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Explanatory": EXPLANATORY_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions(paragraph, question_type, num_questions, prompt)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print(f"No data to save for {output_file}.")

# Function to process all PDF files in a directory
def process_pdfs_in_directory(directory_path, num_questions=3, output_folder="output"):
    """
    Processes all PDF files in the given directory and generates categorized questions.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    pdf_files = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]
    for pdf_file in pdf_files:
        pdf_path = os.path.join(directory_path, pdf_file)
        try:
            print(f"Processing: {pdf_path}")
            # Extract text from the PDF
            pdf_text = extract_text_from_pdf(pdf_path)

            # Generate questions
            questions_and_answers = generate_questions_by_all_types(pdf_text, num_questions)

            # Save results to Excel
            output_file = os.path.join(output_folder, pdf_file.replace(".pdf", "_questions.xlsx"))
            save_to_excel(questions_and_answers, output_file)
        except Exception as e:
            print(f"Error processing {pdf_file}: {e}")



















from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Define prompts for each question type
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate {num_questions} factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, definitions, or data.
    - Answers are precise and concise.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your factual question here",
            "answer": "Your factual answer here"
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step explanations.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your procedural question here",
            "answer": [
                "Step 1: Description of the first step.",
                "Step 2: Description of the second step.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

EXPLANATORY_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} explanatory question-answer pairs based on the input text.

    Ensure:
    - Questions aim to clarify concepts or ideas in the text.
    - Answers are comprehensive, including step-by-step reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your explanatory question here",
            "answer": [
                "Step 1: Introduction to the concept or idea.",
                "Step 2: Explanation of key elements.",
                "Step 3: How these elements interact or apply in practice.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} inferential question-answer pairs based on the input text.

    Ensure:
    - Questions encourage interpretation or conclusions based on the data.
    - Answers justify inferences with logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your inferential question here",
            "answer": "Your inference-based answer with justification here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or ideas.
    - Answers provide detailed explanations of the underlying concepts.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your conceptual question here",
            "answer": "Your conceptual answer here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} reasoning-based question-answer pairs based on the input text.

    Ensure:
    - Questions require logical argumentation or critical thinking.
    - Answers justify conclusions with clear and logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your reasoning-based question here",
            "answer": "Your reasoning-based answer with justification here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Extract text from a PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON for {question_type} questions.")
        return []
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Generate questions for all types
def generate_questions_by_all_types(paragraph, num_questions):
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Explanatory": EXPLANATORY_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions(paragraph, question_type, num_questions, prompt)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Save results to Excel
def save_to_excel(questions_and_answers, output_file="questions_by_type.xlsx"):
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Process multiple PDFs in a loop
def process_multiple_pdfs(pdf_paths, num_questions=3, output_folder="output"):
    """
    Process multiple PDFs and generate categorized questions for each.
    """
    for pdf_path in pdf_paths:
        try:
            print(f"Processing PDF: {pdf_path}")
            # Extract text
            pdf_text = extract_text_from_pdf(pdf_path)

            # Generate questions
            questions_and_answers = generate_questions_by_all_types(pdf_text, num_questions)

            # Save to Excel
            output_file = f"{output_folder}/{pdf_path.split('/')[-1].replace('.pdf', '_questions.xlsx')}"
            save_to_excel(questions_and_answers, output_file)

        except Exception as e:
            print(f"Error processing {pdf_path}: {e}")



















from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

# Define separate prompts for each question type
FACTUAL_PROMPT = """
    You are a financial expert with deep expertise in corporate finance, investment strategies, and accounting.
    Your task is to generate {num_questions} factual question-answer pairs based on the input text.

    Ensure:
    - Questions focus on specific details, definitions, or data.
    - Answers are precise and concise.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your factual question here",
            "answer": "Your factual answer here"
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

PROCEDURAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} procedural question-answer pairs based on the input text.

    Ensure:
    - Questions focus on processes, methods, or sequences of steps.
    - Answers provide detailed, step-by-step explanations.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your procedural question here",
            "answer": [
                "Step 1: Description of the first step.",
                "Step 2: Description of the second step.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

EXPLANATORY_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} explanatory question-answer pairs based on the input text.

    Ensure:
    - Questions aim to clarify concepts or ideas in the text.
    - Answers are comprehensive, including step-by-step reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your explanatory question here",
            "answer": [
                "Step 1: Introduction to the concept or idea.",
                "Step 2: Explanation of key elements.",
                "Step 3: How these elements interact or apply in practice.",
                ...
            ]
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

INFERENTIAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} inferential question-answer pairs based on the input text.

    Ensure:
    - Questions encourage interpretation or conclusions based on the data.
    - Answers justify inferences with logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your inferential question here",
            "answer": "Your inference-based answer with justification here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

CONCEPTUAL_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} conceptual question-answer pairs based on the input text.

    Ensure:
    - Questions explore theories, principles, or ideas.
    - Answers provide detailed explanations of the underlying concepts.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your conceptual question here",
            "answer": "Your conceptual answer here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

REASONING_PROMPT = """
    You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
    Generate {num_questions} reasoning-based question-answer pairs based on the input text.

    Ensure:
    - Questions require logical argumentation or critical thinking.
    - Answers justify conclusions with clear and logical reasoning.

    Input Text: {paragraph}

    Output format:
    [
        {{
            "question": "Your reasoning-based question here",
            "answer": "Your reasoning-based answer with justification here."
        }},
        ...
    ]
    Ensure the response is a valid JSON array.
"""

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to generate questions for a specific type
def generate_questions(paragraph, question_type, num_questions, prompt_template):
    """
    Generates questions for a specific question type.
    """
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    prompt = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template=prompt_template
    )
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    try:
        response = llm_chain.predict(
            paragraph=paragraph,
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON for {question_type} questions.")
        print("Raw Response:", response)
        return []
    except Exception as e:
        print(f"Error generating {question_type} questions: {e}")
        return []

# Main function to generate questions for all types
def generate_questions_by_all_types(paragraph, num_questions):
    """
    Generate categorized questions for all types.
    """
    question_types_and_prompts = {
        "Factual": FACTUAL_PROMPT,
        "Procedural": PROCEDURAL_PROMPT,
        "Explanatory": EXPLANATORY_PROMPT,
        "Inferential": INFERENTIAL_PROMPT,
        "Conceptual": CONCEPTUAL_PROMPT,
        "Reasoning-based": REASONING_PROMPT,
    }

    all_questions = []
    for question_type, prompt in question_types_and_prompts.items():
        questions = generate_questions(paragraph, question_type, num_questions, prompt)
        for qa in questions:
            all_questions.append({
                "Question Type": question_type,
                "Question": qa["question"],
                "Answer": qa["answer"]
            })
    return all_questions

# Function to save results to Excel
def save_to_excel(questions_and_answers, output_file="questions_by_type.xlsx"):
    """
    Saves categorized questions and answers to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Main Execution
if __name__ == "__main__":
    pdf_path = "your_pdf_file.pdf"  # Replace with the path to your PDF file

    try:
        # Extract text from the PDF
        pdf_text = extract_text_from_pdf(pdf_path)

        # Generate categorized questions
        questions_and_answers = generate_questions_by_all_types(pdf_text, num_questions=3)

        # Save the output to an Excel file
        if questions_and_answers:
            save_to_excel(questions_and_answers, "questions_by_type.xlsx")
        else:
            print("No questions generated.")
    except Exception as e:
        print(f"Error: {e}")










from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import pandas as pd
import json

def generate_questions_by_type(paragraph, num_questions=3):
    """
    Generates categorized questions and answers (Factual, Procedural, etc.)
    """
    question_types = [
        "Factual",
        "Procedural",
        "Explanatory",
        "Inferential",
        "Conceptual",
        "Reasoning-based"
    ]

    # Define prompts for each question type
    prompt_templates = {
        "Factual": """
            You are a financial expert with a deep understanding of corporate finance, investment strategies, and accounting.
            Generate {num_questions} factual question-answer pairs based on the input text.
            
            Factual questions should:
            - Focus on specific details, definitions, or data.
            - Provide precise answers without unnecessary elaboration.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your factual question here",
                    "answer": "Your factual answer here"
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """,
        "Procedural": """
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
            Generate {num_questions} procedural question-answer pairs based on the input text.

            Procedural questions should:
            - Focus on processes, methods, or sequences of steps.
            - Provide step-by-step answers that are logical and easy to follow.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your procedural question here",
                    "answer": [
                        "Step 1: Description of the first step.",
                        "Step 2: Description of the second step.",
                        "Step 3: Description of the third step."
                    ]
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """,
        "Explanatory": """
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
            Generate {num_questions} explanatory question-answer pairs based on the input text.

            Explanatory questions should:
            - Clarify complex concepts or ideas in the text.
            - Provide step-by-step explanations to improve understanding.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your explanatory question here",
                    "answer": [
                        "Step 1: Introduction to the concept or idea.",
                        "Step 2: Explanation of the key elements.",
                        "Step 3: How these elements interact or apply in practice.",
                        "Step 4: Conclusion or summary of the explanation."
                    ]
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """,
        "Inferential": """
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
            Generate {num_questions} inferential question-answer pairs based on the input text.

            Inferential questions should:
            - Require interpreting information or drawing logical conclusions.
            - Encourage predictions or implications based on the provided data.
            - Include answers that justify the inference with clear reasoning.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your inferential question here",
                    "answer": "Your inference-based answer with justification here."
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """,
        "Conceptual": """
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
            Generate {num_questions} conceptual question-answer pairs based on the input text.

            Conceptual questions should:
            - Explore theories, principles, or abstract ideas.
            - Provide answers that are detailed and cover the underlying concepts comprehensively.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your conceptual question here",
                    "answer": "Your conceptual answer here."
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """,
        "Reasoning-based": """
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting.
            Generate {num_questions} reasoning-based question-answer pairs based on the input text.

            Reasoning-based questions should:
            - Require logical argumentation, critical thinking, or evaluation.
            - Include answers that justify conclusions with a logical explanation.

            Input Text: {paragraph}

            Output:
            [
                {{
                    "question": "Your reasoning-based question here",
                    "answer": "Your reasoning-based answer with justification here."
                }},
                ...
            ]
            Ensure the response is a valid JSON object.
        """
    }

    # Initialize the Azure LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    all_questions = []
    for question_type, template in prompt_templates.items():
        try:
            prompt = PromptTemplate(input_variables=["paragraph", "num_questions"], template=template)
            llm_chain = LLMChain(llm=llm, prompt=prompt)

            response = llm_chain.predict(paragraph=paragraph, num_questions=str(num_questions))
            questions_and_answers = json.loads(response)

            for qa in questions_and_answers:
                all_questions.append({
                    "Question Type": question_type,
                    "Question": qa["question"],
                    "Answer": qa["answer"]
                })
        except json.JSONDecodeError:
            print(f"Error: Invalid JSON for {question_type} questions.")
            print("Raw Response:", response)
        except Exception as e:
            print(f"Error generating {question_type} questions: {e}")

    return all_questions

# Save results to Excel
def save_to_excel(questions_and_answers, output_file="questions_by_type.xlsx"):
    """
    Saves categorized questions and answers to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Main execution
if __name__ == "__main__":
    paragraph = """
    The Management Fee is a percentage of the committed capital used to cover operational costs. 
    The term of the fund is 10 years, with an option for two 1-year extensions. 
    Investors are responsible for operational and organizational expenses, including the Investor Servicing Fee.
    """
    try:
        questions_and_answers = generate_questions_by_type(paragraph, num_questions=3)
        if questions_and_answers:
            save_to_excel(questions_and_answers, "questions_by_type.xlsx")
        else:
            print("No questions generated.")
    except Exception as e:
        print(f"Error: {e}")















from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Set Hugging Face cache directory
hf_home_path = r"/Volumes/dev2_catalog_01/default/txt-wtf-pe/hf/all-MiniLM-L6-v2"

# Load the model
model = SentenceTransformer(hf_home_path)
print("Model loaded successfully!")

# Example sentences
sentences = [
    "The weather is sunny and warm.",
    "It's a bright and sunny day.",
    "I love going to the beach when the weather is nice.",
    "Rainy days make me want to stay indoors."
]

# Generate embeddings for sentences
print("\nGenerating embeddings for sentences...")
embeddings = model.encode(sentences)
print("Embeddings generated!")

# Display embeddings
print("\nSentence embeddings:")
for i, embedding in enumerate(embeddings):
    print(f"Sentence {i + 1}: {sentences[i]}")
    print(f"Embedding (first 5 dimensions): {embedding[:5]}\n")

# Compute similarity between two sentences
print("\nCalculating similarity between the first two sentences...")
similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
print(f"Similarity between '{sentences[0]}' and '{sentences[1]}': {similarity:.2f}")

# Perform semantic search
print("\nPerforming semantic search...")
query = "What is the weather like?"
query_embedding = model.encode(query)
similarities = cosine_similarity([query_embedding], embeddings)[0]

# Find the most similar sentence
most_similar_idx = np.argmax(similarities)
print(f"Query: {query}")
print(f"Most similar sentence: {sentences[most_similar_idx]} (Similarity: {similarities[most_similar_idx]:.2f})")








combined_prompt_template = PromptTemplate(
    input_variables=["paragraph", "num_questions"],
    template="""
        You are an expert in finance and accounting. Your task is to generate {num_questions} question-answer pairs covering all the following types:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Requirements:
        - Ensure an even distribution of questions among all types.
        - Questions should be clear, concise, and relevant to the text.
        - Answers should be accurate, comprehensive, and easy to understand.

        Input Text: {paragraph}

        Generate the response in the following JSON format:
        [
            {{
                "question": "Your question here",
                "answer": "Your answer here",
                "type": "Type of the question here (e.g., Factual, Procedural, etc.)"
            }},
            ...
        ]
        Ensure the response is a valid JSON and contains no extra text outside the JSON format.
    """
)







from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import pandas as pd
import json

def generate_questions_by_type(paragraph, num_questions=3):
    """
    Generates categorized questions and answers (Procedural, Explanatory, etc.)
    """
    question_types = [
        "Factual",
        "Procedural",
        "Explanatory",
        "Inferential",
        "Conceptual",
        "Reasoning-based"
    ]

    # Template for generating categorized questions
    prompt_template = PromptTemplate(
        input_variables=["paragraph", "question_type", "num_questions"],
        template="""
            You are a financial expert with expertise in corporate finance, investment strategies, and accounting. 
            Your task is to generate {num_questions} {question_type} question-answer pairs based on the input text.

            Ensure:
            - The questions are clear, concise, and aligned with the {question_type} type.
            - Answers are comprehensive yet easy to understand.
            - The response is a valid JSON array in the format:
              [
                  {{
                      "question": "Your question here",
                      "answer": "Your answer here"
                  }},
                  ...
              ]

            Input Text: {paragraph}
        """
    )

    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )

    llm_chain = LLMChain(llm=llm, prompt=prompt_template)

    # Generate questions for each type
    all_questions = []
    for question_type in question_types:
        try:
            response = llm_chain.predict(
                paragraph=paragraph,
                question_type=question_type,
                num_questions=str(num_questions)
            )
            questions_and_answers = json.loads(response)

            for qa in questions_and_answers:
                all_questions.append({
                    "Question Type": question_type,
                    "Question": qa["question"],
                    "Answer": qa["answer"]
                })
        except json.JSONDecodeError:
            print(f"Error: Invalid JSON for {question_type} questions.")
            continue
        except Exception as e:
            print(f"Error: {e}")
            continue

    return all_questions

def save_to_excel(questions_and_answers, output_file="questions_by_type.xlsx"):
    """
    Saves categorized questions and answers to an Excel file.
    """
    if questions_and_answers:
        df = pd.DataFrame(questions_and_answers)
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Example Usage
paragraph = """
The Management Fee is a percentage of the committed capital used to cover operational costs. 
The term of the fund is 10 years, with an option for two 1-year extensions. 
Investors are responsible for operational and organizational expenses, including the Investor Servicing Fee.
"""

try:
    questions_and_answers = generate_questions_by_type(paragraph, num_questions=3)
    if questions_and_answers:
        save_to_excel(questions_and_answers, "questions_by_type.xlsx")
    else:
        print("No questions generated.")
except Exception as e:
    print(f"Error: {e}")













from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    """
    # Define the prompt template
    prompt_template = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template="""
            You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
            Your task is to generate precise and accurate question-answer pairs based on the input text. 
            Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
            
            Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
            - Factual
            - Procedural
            - Explanatory
            - Inferential
            - Conceptual
            - Reasoning-based

            Input Text: {paragraph}

            Generate {num_questions} questions along with their answers in the following JSON format:
            [
                {
                    "question": "Your question here",
                    "answer": "Your answer here"
                },
                ...
            ]
            Ensure the JSON format is valid and parsable.
        """
    )
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Create LLM Chain
    llm_chain = LLMChain(llm=llm, prompt=prompt_template)
    
    # Generate the response
    try:
        response = llm_chain.predict(paragraph=pdf_text, num_questions=num_questions)
        questions_and_answers = json.loads(response)
        return questions_and_answers
    except json.JSONDecodeError:
        print("Error: The model response is not valid JSON.")
        print("Raw Response:", response)
        return []
    except Exception as e:
        print(f"Error: {e}")
        return []

def save_to_excel(questions_and_answers, output_file="questions_and_answers.xlsx"):
    """
    Saves the questions and answers to an Excel file.
    """
    if questions_and_answers:
        # Convert to DataFrame
        df = pd.DataFrame(questions_and_answers)
        # Save to Excel
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Main Function to Process Entire PDF
pdf_path = "your_pdf_file.pdf"  # Replace with the actual PDF file path

try:
    # Step 1: Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Step 2: Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    if questions_and_answers:
        print("Questions and Answers Generated Successfully.")

        # Step 3: Save to Excel
        save_to_excel(questions_and_answers, "questions_and_answers.xlsx")
    else:
        print("No questions generated.")
except Exception as e:
    print(f"Error: {e}")









from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    """
    # Define the prompt template
    prompt_template = PromptTemplate(
        input_variables=["paragraph", "num_questions"],
        template="""
            You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
            Your task is to generate precise and accurate question-answer pairs based on the input text. 
            Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
            
            Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
            - Factual
            - Procedural
            - Explanatory
            - Inferential
            - Conceptual
            - Reasoning-based

            Input Text: {paragraph}

            Generate {num_questions} questions along with their answers in the following JSON format:
            [
                {
                    "question": "Your question here",
                    "answer": "Your answer here"
                },
                ...
            ]
            Ensure the JSON format is valid and parsable.
        """
    )
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",  # Replace with your API version
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Create LLM Chain
    llm_chain = LLMChain(llm=llm, prompt=prompt_template)
    
    # Generate the response
    try:
        response = llm_chain.predict(paragraph=pdf_text, num_questions=num_questions)
        questions_and_answers = json.loads(response)
        return questions_and_answers
    except json.JSONDecodeError:
        print("Error: The model response is not valid JSON.")
        print("Raw Response:", response)
        return []
    except Exception as e:
        print(f"Error: {e}")
        return []

def save_to_excel(questions_and_answers, output_file="questions_and_answers.xlsx"):
    """
    Saves the questions and answers to an Excel file.
    """
    if questions_and_answers:
        # Convert to DataFrame
        df = pd.DataFrame(questions_and_answers)
        # Save to Excel
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Save to Excel
    save_to_excel(questions_and_answers, "questions_and_answers.xlsx")
except Exception as e:
    print(f"Error: {e}")







from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import pandas as pd
import json

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    """
    # Define the prompt template
    prompt_template = """
        You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
        Your task is to generate precise and accurate question-answer pairs based on the input text. 
        Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
        
        Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Input Text: {paragraph}

        Generate {num_questions} questions along with their answers in the following JSON format:
        [
            {
                "question": "Your question here",
                "answer": "Your answer here"
            },
            ...
        ]
        Ensure the JSON format is valid and parsable.
    """
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",  # Replace with your API version
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Set up the prompt
    formatted_prompt = prompt_template.format(paragraph=pdf_text, num_questions=num_questions)
    
    # Generate the response
    response = llm(formatted_prompt)
    
    try:
        # Attempt to parse the response as JSON
        questions_and_answers = json.loads(response)
        return questions_and_answers
    except json.JSONDecodeError:
        print("Error: The model response is not valid JSON. Cleaning the response...")
        # Attempt to clean and parse
        cleaned_response = response.replace("\n", "").strip()
        try:
            questions_and_answers = json.loads(cleaned_response)
            return questions_and_answers
        except Exception as e:
            print("Failed to clean and parse JSON response.")
            print("Raw Response:", response)
            return []

def save_to_excel(questions_and_answers, output_file="questions_and_answers.xlsx"):
    """
    Saves the questions and answers to an Excel file.
    """
    if questions_and_answers:
        # Convert to DataFrame
        df = pd.DataFrame(questions_and_answers)
        # Save to Excel
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Save to Excel
    save_to_excel(questions_and_answers, "questions_and_answers.xlsx")
except Exception as e:
    print(f"Error: {e}")













from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import json
import pandas as pd

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    """
    # Define the prompt template
    prompt_template = """
        You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
        Your task is to generate precise and accurate question-answer pairs based on the input text. 
        Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
        
        Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Input Text: {paragraph}

        Generate {num_questions} questions along with their answers in the following JSON format:
        [
            {
                "question": "Your question here",
                "answer": "Your answer here"
            },
            ...
        ]
        Ensure the JSON format is valid and parsable.
    """
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",  # Replace with your API version
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Set up the prompt
    formatted_prompt = prompt_template.format(paragraph=pdf_text, num_questions=num_questions)
    
    # Generate the response
    response = llm(formatted_prompt)
    
    try:
        # Attempt to parse the response as JSON
        questions_and_answers = json.loads(response)
        return questions_and_answers
    except json.JSONDecodeError:
        print("Error: The model response is not valid JSON.")
        print("Model Response:", response)
        return []

def save_to_excel(questions_and_answers, output_file="questions_and_answers.xlsx"):
    """
    Saves the questions and answers to an Excel file.
    """
    if questions_and_answers:
        # Convert to DataFrame
        df = pd.DataFrame(questions_and_answers)
        # Save to Excel
        df.to_excel(output_file, index=False)
        print(f"Questions and answers saved to {output_file}.")
    else:
        print("No data to save.")

# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Save to Excel
    save_to_excel(questions_and_answers, "questions_and_answers.xlsx")
except Exception as e:
    print(f"Error: {e}")










from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import json

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    """
    # Define the prompt template
    prompt_template = """
        You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
        Your task is to generate precise and accurate question-answer pairs based on the input text. 
        Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
        
        Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Input Text: {paragraph}

        Generate {num_questions} questions along with their answers in the following JSON format:
        [
            {
                "question": "Your question here",
                "answer": "Your answer here"
            },
            ...
        ]
        Ensure the JSON format is valid and parsable.
    """
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",  # Replace with your API version
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Set up the prompt
    formatted_prompt = prompt_template.format(paragraph=pdf_text, num_questions=num_questions)
    
    # Generate the response
    response = llm(formatted_prompt)
    
    try:
        # Attempt to parse the response as JSON
        questions_and_answers = json.loads(response)
        return questions_and_answers
    except json.JSONDecodeError:
        print("Error: The model response is not valid JSON.")
        print("Model Response:", response)
        return []

# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Display the results
    print(json.dumps(questions_and_answers, indent=2))
except Exception as e:
    print(f"Error: {e}")







from langchain.prompts.prompt import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import AzureChatOpenAI
import PyPDF2
import json

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    
    Args:
        pdf_path (str): Path to the PDF file.
        
    Returns:
        str: Combined text from all pages of the PDF.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    
    Args:
        pdf_text (str): Input text to generate questions from.
        num_questions (int): Number of question-answer pairs to generate.
    
    Returns:
        list: List of question-answer pairs in JSON format.
    """
    # Define the prompt template
    prompt_template = """
        You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
        Your task is to generate precise and accurate question-answer pairs based on the input text. 
        Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
        
        Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Input Text: {paragraph}

        Generate {num_questions} questions along with their answers. Present the output in JSON format as:
        [
            {
                "question": "Question text here",
                "answer": "Answer text here"
            },
            ...
        ]
    """
    
    # Create the LLM
    llm = AzureChatOpenAI(
        temperature=0,
        api_version="2023-06-01-preview",  # Replace with your actual API version
        azure_openai_api_base="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40"
    )
    
    # Set up the prompt
    formatted_prompt = prompt_template.format(paragraph=pdf_text, num_questions=num_questions)
    
    # Generate the response
    response = llm(formatted_prompt)
    
    return json.loads(response)  # Parse the JSON output

# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Step 1: Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Step 2: Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Step 3: Display the results
    print(json.dumps(questions_and_answers, indent=2))

except Exception as e:
    print(f"Error: {e}")



# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Step 1: Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Step 2: Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Step 3: Display the results
    print(json.dumps(questions_and_answers, indent=2))

except Exception as e:
    print(f"Error: {e}")







from langchain.prompts import PromptTempla



te
from langchain.chains import LLMChain
from langchain.llms import AzureChatOpenAI
import PyPDF2

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.
    
    Args:
        pdf_path (str): Path to the PDF file.
        
    Returns:
        str: Combined text from all pages of the PDF.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def generate_questions_from_text(pdf_text, num_questions=25):
    """
    Generates question-answer pairs from the given text.
    
    Args:
        pdf_text (str): Input text to generate questions from.
        num_questions (int): Number of question-answer pairs to generate.
    
    Returns:
        list: List of question-answer pairs in JSON format.
    """
    # Prompt template for generating questions and answers
    prompt_template = PromptTemplate.from_template("""
        You are a Financial expert with a deep understanding of corporate finance, investment strategies, and accounting. 
        Your task is to generate precise and accurate question-answer pairs based on the input text. 
        Each question should be clear, concise, and relevant to the financial domain, while the answers should be comprehensive yet easy to understand. 
        
        Ensure the questions target different levels of difficulty ranging from beginner to expert. Also, make sure there are different types of questions such as:
        - Factual
        - Procedural
        - Explanatory
        - Inferential
        - Conceptual
        - Reasoning-based

        Input Text: {paragraph}

        Generate {num_questions} questions along with their answers. Present the output in JSON format as:
        [
            {
                "question": "Question text here",
                "answer": "Answer text here"
            },
            ...
        ]
    """)

    # LLM setup
    llm = AzureChatOpenAI(
        temperature=0, 
        api_version="2023-06-01-preview",  # Replace with your actual API version
        azure_endpoint="YOUR_AZURE_ENDPOINT",  # Replace with your Azure endpoint
        deployment_name="ssgpt-40", 
        seed=42
    )

    # Create LLM Chain
    llm_chain = LLMChain(llm=llm, prompt=prompt_template)

    # Predict output
    output = llm_chain.predict(paragraph=pdf_text, num_questions=num_questions)

    return output


# Path to the PDF file
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file path

try:
    # Step 1: Extract text from the PDF
    pdf_text = extract_text_from_pdf(pdf_path)
    print("PDF Text Extracted Successfully.")

    # Step 2: Generate questions and answers
    questions_and_answers = generate_questions_from_text(pdf_text, num_questions=25)
    print("Questions and Answers Generated Successfully.")

    # Step 3: Display the results
    import json
    print(json.dumps(questions_and_answers, indent=2))

except Exception as e:
    print(f"Error: {e}")













import os
from pathlib import Path
from sentence_transformers import SentenceTransformer

# Set Hugging Face cache directory
hf_home_path = r"C:\Users\subba\OneDrive\Desktop\test"
os.environ["HF_HOME"] = hf_home_path

# Ensure the directory exists
if not os.path.exists(hf_home_path):
    os.makedirs(hf_home_path, exist_ok=True)
print(f"Hugging Face cache directory set to: {hf_home_path}")

# Load model
try:
    print("Downloading model...")
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", cache_folder=hf_home_path)
    print("Model downloaded successfully.")
except Exception as e:
    print(f"Error downloading model: {e}")

# Verify downloaded files
print(f"Checking downloaded files in: {hf_home_path}")
try:
    files = list(Path(hf_home_path).rglob("*"))
    if files:
        print("Downloaded files:")
        for file in files:
            print(file)
    else:
        print("No files found. Model might not have been saved properly.")
except Exception as e:
    print(f"Error checking files: {e}")   "give me example how we load the model use it
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Set Hugging Face cache directory
hf_home_path = r"C:\Users\subba\OneDrive\Desktop\test"

# Load the model
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", cache_folder=hf_home_path)
print("Model loaded successfully!")

# Example sentences
sentences = [
    "The weather is sunny and warm.",
    "It’s a bright and sunny day.",
    "I love going to the beach when the weather is nice.",
    "Rainy days make me want to stay indoors."
]

# Generate embeddings for sentences
print("\nGenerating embeddings for sentences...")
embeddings = model.encode(sentences)
print("Embeddings generated!")

# Display embeddings
print("\nSentence embeddings:")
for i, embedding in enumerate(embeddings):
    print(f"Sentence {i+1}: {sentences[i]}")
    print(f"Embedding (first 5 dimensions): {embedding[:5]}\n")

# Compute similarity between two sentences
print("\nCalculating similarity between the first two sentences...")
similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
print(f"Similarity between '{sentences[0]}' and '{sentences[1]}': {similarity:.2f}")

# Perform semantic search
print("\nPerforming semantic search...")
query = "What is the weather like?"
query_embedding = model.encode(query)
similarities = cosine_similarity([query_embedding], embeddings)[0]

# Find the most similar sentence
most_similar_idx = np.argmax(similarities)
print(f"Query: {query}")
print(f"Most similar sentence: {sentences[most_similar_idx]} (Similarity: {similarities[most_similar_idx]:.2f})")








# Step 1: Stop Proxy
import os
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer

# Remove proxy settings if they exist
if "https_proxy" in os.environ:
    del os.environ["https_proxy"]
if "http_proxy" in os.environ:
    del os.environ["http_proxy"]

print("Proxy settings have been removed.")

# Step 2: Set Hugging Face Cache Directory
# Set the Hugging Face home to a writable directory in Databricks
hf_home_path = "/dbfs/tmp/huggingface_cache/"
os.environ["HF_HOME"] = hf_home_path

# Ensure the directory exists
if not os.path.exists(hf_home_path):
    os.makedirs(hf_home_path, exist_ok=True)
print(f"Hugging Face cache directory set to: {hf_home_path}")

# Step 3: Download Model and Tokenizer
model_name = "gpt2"  # Small model for testing

try:
    print("Downloading model...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",  # Automatically map model to available devices
        trust_remote_code=True  # Allow downloading custom models if needed
    )
    print("Model downloaded successfully.")
except Exception as e:
    print(f"Error downloading model: {e}")

try:
    print("Downloading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    print("Tokenizer downloaded successfully.")
except Exception as e:
    print(f"Error downloading tokenizer: {e}")

# Step 4: Verify Downloaded Files
print(f"Checking downloaded files in: {hf_home_path}")
try:
    files = list(Path(hf_home_path).rglob("*"))
    if files:
        print("Downloaded files:")
        for file in files:
            print(file)
    else:
        print("No files found. Model might not have been saved properly.")
except Exception as e:
    print(f"Error checking files: {e}")







import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

# Environment setup
os.environ['CURL_CA_BUNDLE'] = ''
os.environ["HF_HOME"] = "/Volumes/dev2_catalog_01/default/txt-wtf-pe/hf/test-model/"  # Change to a test directory

# Small model for testing
model_name = "gpt2"  # You can also try "bert-base-uncased" or any lightweight model

# Load the model
print("Downloading model...")
try:
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map='auto',  # Automatically map model to available devices
        trust_remote_code=True  # Allow downloading custom models if needed
    )
    print("Model downloaded successfully.")
except Exception as e:
    print(f"Error downloading model: {e}")

# Load the tokenizer
print("Downloading tokenizer...")
try:
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    print("Tokenizer downloaded successfully.")
except Exception as e:
    print(f"Error downloading tokenizer: {e}")

# Check if the model and tokenizer are saved
hf_home_path = os.environ["HF_HOME"]
print(f"Checking model files in: {hf_home_path}")
try:
    from pathlib import Path

    files = list(Path(hf_home_path).rglob("*"))
    if files:
        print("Downloaded files:")
        for file in files:
            print(file)
    else:
        print("No files found. Model might not have been saved properly.")
except Exception as e:
    print(f"Error checking files: {e}")








import openai
import pandas as pd

# Set your OpenAI API key
openai.api_key = "your_openai_api_key"

def generate_questions(paragraph, num_questions=5):
    """
    Generate questions based on a paragraph using GPT-4.
    """
    prompt = f"""
    Based on the following paragraph, generate {num_questions} meaningful and relevant questions:
    
    Paragraph:
    {paragraph}
    
    Questions:
    """
    try:
        # Call GPT-4 model
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a helpful assistant for generating questions."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.7
        )
        # Extract and return the questions
        questions = response['choices'][0]['message']['content']
        return questions.strip()
    except Exception as e:
        return f"Error: {e}"

def process_excel(file_path, output_path, num_questions=5):
    """
    Process the Excel file, generate questions for each paragraph, and save the results.
    """
    # Read the Excel file
    df = pd.read_excel(file_path)

    # Check for the required columns
    if 'Paragraph' not in df.columns:
        raise ValueError("The Excel file must contain a 'Paragraph' column.")

    # Generate questions for each paragraph
    questions_list = []
    for index, row in df.iterrows():
        paragraph = row['Paragraph']
        if pd.isna(paragraph):  # Skip empty paragraphs
            questions_list.append("")
            continue
        print(f"Generating questions for S. No {row['S. No']}...")
        questions = generate_questions(paragraph, num_questions=num_questions)
        questions_list.append(questions)

    # Add questions to a new column
    df['Questions'] = questions_list

    # Save the updated DataFrame to a new Excel file
    df.to_excel(output_path, index=False)
    print(f"Questions have been added and saved to {output_path}")

if __name__ == "__main__":
    # Input Excel file path
    input_file = "input_file.xlsx"  # Replace with your input file path
    output_file = "output_with_questions.xlsx"  # Replace with your desired output file path

    # Number of questions to generate per paragraph
    num_questions = 5

    # Process the Excel file
    process_excel(input_file, output_file, num_questions=num_questions)




import fitz  # PyMuPDF
import csv
import re

def is_invalid_heading(text):
    """Check if a heading is invalid (empty, only special characters, or only numbers)."""
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings(line):
    """Extract headings or subheadings from a given line."""
    match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)", line)  # Match section numbers and titles
    heading = None

    if match:
        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
        title = match.group(3)  # Title text (e.g., "Formation and Continuation")

        if not is_invalid_heading(title):  # Check if the heading is valid
            if len(section.split(".")) == 1:  # Top-level heading
                heading = {
                    "type": "Heading",
                    "section": section,
                    "text": section.strip() + " " + title.strip(),  # Combine section and title
                }
            elif len(section.split(".")) > 1:  # Subheading
                heading = {
                    "type": "Subheading",
                    "section": section,
                    "text": section.strip() + " " + title.strip(),  # Combine section and title
                }

    return heading

def extract_headings_from_pdf(pdf_path):
    """Extract headings from a PDF file."""
    headings = []
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        temp = ""

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    # Identify headings based on font size or style
                    if span["size"] >= 10:  # Adjust threshold if needed
                        line_text = span["text"].strip()

                        if line_text != "":
                            if re.match(r'^\d+\.*\s*$', line_text):
                                # Likely a standalone section number
                                temp = line_text
                            else:
                                # Combine with previous line if it's a continuation
                                line_text = temp + " " + line_text if temp else line_text
                                temp = ""  # Reset temp after combining

                                # Extract headings
                                heading = extract_headings(line_text.strip())
                                if heading:
                                    headings.append({
                                        "page_number": page_number + 1,
                                        "type": heading["type"],
                                        "text": heading["text"].strip(),
                                    })

    return headings

def save_headings_to_csv(pdf_path):
    """Extract headings and save them to a CSV file."""
    headings = extract_headings_from_pdf(pdf_path)
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")

    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Text"])  # CSV header
        for heading in headings:
            writer.writerow([heading["page_number"], heading["type"], heading["text"]])

    print(f"Results saved to the file: {csv_file}")

# Example usage:
# save_headings_to_csv("example.pdf")













import fitz  # PyMuPDF
import csv
import re

def is_invalid_heading(text):
    # Check if a heading is invalid (empty, only special characters, or only numbers).
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings(line):
    # Match patterns like "1.", "1.1", etc., followed by a heading
    match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)", line)
    heading = None

    if match:
        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
        title = match.group(3)    # Title text
        if not is_invalid_heading(title):
            if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                heading = {
                    "type": "Heading",
                    "section": section,
                    "text": title.strip()
                }
            elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                heading = {
                    "type": "Subheading",
                    "section": section,
                    "text": title.strip()
                }
    return heading

def extract_headings_from_pdf(pdf_path):
    headings = []
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            temp_line = ""
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    if span["size"] >= 10:  # Adjust this threshold for font size
                        line_text = span["text"].strip()
                        if re.match(r'^\d+\.*$', line_text):  # Handle cases where section number is on a separate line
                            temp_line = line_text
                        else:
                            if temp_line:
                                line_text = temp_line + " " + line_text
                                temp_line = ""
                            heading = extract_headings(line_text)
                            if heading:
                                headings.append({
                                    "page_number": page_number + 1,
                                    "type": heading["type"],
                                    "text": heading["text"]
                                })
    return headings

# Save results to a CSV file
def save_to_csv(pdf_path, headings):
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Heading"])
        for heading in headings:
            writer.writerow([heading['page_number'], heading['type'], heading['text']])
    print(f"Results saved to the file: {csv_file}")

# Main Function
pdf_path = "example.pdf"  # Replace with your PDF file path
headings = extract_headings_from_pdf(pdf_path)
save_to_csv(pdf_path, headings)








import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_path
import re
import csv

# Configure Tesseract path (if not in PATH)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Update with your Tesseract path

# Configure Poppler path (for Windows users)
POPPLER_PATH = r"C:\poppler\bin"  # Update with your Poppler installation path

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF using PyMuPDF for text-based PDFs.
    If PyMuPDF fails, fallback to OCR using pytesseract.
    """
    document = fitz.open(pdf_path)
    text_content = []

    for page_number in range(len(document)):
        page = document[page_number]
        text = page.get_text()

        if not text.strip():  # If the page has no text, fallback to OCR
            images = convert_from_path(pdf_path, first_page=page_number + 1, last_page=page_number + 1, poppler_path=POPPLER_PATH)
            for image in images:
                ocr_text = pytesseract.image_to_string(image)
                text_content.append(ocr_text)
        else:
            text_content.append(text)

    return "\n".join(text_content)

def extract_headings_and_content(text):
    """
    Extract headings, subheadings, and their associated content.
    """
    results = []
    lines = text.splitlines()

    current_heading = None
    current_content = []

    for line in lines:
        line = line.strip()

        # Regex to match headings and subheadings
        # Matches: "1. Organization", "1.1 Formation and Continuation", "1.1.1 Details"
        match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)\.", line)
        if match:
            section = match.group(1)  # Section number (e.g., "1.", "1.1.")
            title = match.group(3)  # Title text

            if not is_invalid_heading(title):
                # If there is a current heading, save it with its content
                if current_heading:
                    results.append({
                        "type": current_heading["type"],
                        "section": current_heading["section"],
                        "text": current_heading["text"],
                        "content": " ".join(current_content).strip()
                    })

                # Start a new heading or subheading
                heading_type = "Heading" if len(section.split(".")) == 1 else "Subheading"
                current_heading = {
                    "type": heading_type,
                    "section": section,
                    "text": title.strip() + "."
                }
                current_content = []  # Reset content for the new heading/subheading
        else:
            # Add non-heading lines as content for the current heading/subheading
            if current_heading:
                current_content.append(line)

    # Save the last heading/subheading with its content
    if current_heading:
        results.append({
            "type": current_heading["type"],
            "section": current_heading["section"],
            "text": current_heading["text"],
            "content": " ".join(current_content).strip()
        })

    return results

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings, subheadings, and their content to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Type", "Section", "Text", "Content"])
        for heading in headings:
            writer.writerow([heading["type"], heading["section"], heading["text"], heading["content"]])

def main(pdf_path):
    """
    Main function to extract and save headings, subheadings, and their content from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings_and_content.csv").replace(".PDF", "_headings_and_content.csv")

    # Step 1: Extract text from the PDF
    print("Extracting text from PDF...")
    text = extract_text_from_pdf(pdf_path)

    # Step 2: Extract headings, subheadings, and content
    print("Extracting headings, subheadings, and content...")
    headings = extract_headings_and_content(text)

    # Step 3: Save to CSV
    print(f"Saving to CSV file: {csv_file}")
    save_headings_to_csv(headings, csv_file)

    # Output results
    count = len(headings)
    print(f"Extracted {count} headings and subheadings with content:")
    for heading in headings:
        print(f"{heading['type']} ({heading['section']}): {heading['text']}")
        print(f"Content: {heading['content']}\n")

if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)








import fitz  # PyMuPDF
import csv
import re

def is_invalid_heading(text):
    # Check if a heading is invalid (empty, only special characters, or only numbers).
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings(line):
    match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)", line)
    heading = None

    if match:
        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
        title = match.group(3)    # Title text
        if not is_invalid_heading(title):
            if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                heading = {
                    "type": "Heading",
                    "section": section,
                    "text": line.strip()
                }
            elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                heading = {
                    "type": "Subheading",
                    "section": section,
                    "text": line.strip()
                }
    return heading

def extract_headings_from_pdf(pdf_path):
    headings = []
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    if span["size"] >= 10:  # Adjust this threshold as needed
                        line_text = span["text"].strip()
                        if line_text != "":
                            heading = extract_headings(line_text)
                            if heading:
                                headings.append({
                                    "page_number": page_number + 1,
                                    "type": heading["type"],
                                    "text": heading["text"]
                                })
    return headings

# Save results to a CSV file
def save_to_csv(pdf_path, headings):
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Heading"])
        for heading in headings:
            writer.writerow([heading['page_number'], heading['type'], heading['text']])
    print(f"Results saved to the file: {csv_file}")

# Main Function
pdf_path = "example.pdf"  # Replace with your PDF file path
headings = extract_headings_from_pdf(pdf_path)
save_to_csv(pdf_path, headings)










import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_path
import re
import csv

# Configure Tesseract path (if not in PATH)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Update with your Tesseract path

# Configure Poppler path (for Windows users)
POPPLER_PATH = r"C:\poppler\bin"  # Update with your Poppler installation path

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF using PyMuPDF for text-based PDFs.
    If PyMuPDF fails, fallback to OCR using pytesseract.
    """
    document = fitz.open(pdf_path)
    text_content = []

    for page_number in range(len(document)):
        page = document[page_number]
        text = page.get_text()

        if not text.strip():  # If the page has no text, fallback to OCR
            images = convert_from_path(pdf_path, first_page=page_number + 1, last_page=page_number + 1, poppler_path=POPPLER_PATH)
            for image in images:
                ocr_text = pytesseract.image_to_string(image)
                text_content.append(ocr_text)
        else:
            text_content.append(text)

    return "\n".join(text_content)

def extract_headings_and_subheadings(text):
    """
    Extract both headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Formation and Continuation').
    """
    headings = []
    lines = text.splitlines()

    for line in lines:
        line = line.strip()
        # Regex to match headings and subheadings
        # Matches: "1. Organization", "1.1 Formation and Continuation", "1.1.1 Details"
        match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)\.", line)
        if match:
            section = match.group(1)  # Section number (e.g., "1.", "1.1.")
            title = match.group(3)  # Title text

            if not is_invalid_heading(title):
                if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                    headings.append({
                        "type": "Heading",
                        "section": section,
                        "text": title.strip() + "."
                    })
                elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                    headings.append({
                        "type": "Subheading",
                        "section": section,
                        "text": title.strip() + "."
                    })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([heading["type"], heading["section"], heading["text"]])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")

    # Step 1: Extract text from the PDF
    print("Extracting text from PDF...")
    text = extract_text_from_pdf(pdf_path)

    # Step 2: Extract headings and subheadings
    print("Extracting headings and subheadings...")
    headings = extract_headings_and_subheadings(text)

    # Step 3: Save to CSV
    print(f"Saving to CSV file: {csv_file}")
    save_headings_to_csv(headings, csv_file)

    # Output results
    count = len(headings)
    print(f"Extracted {count} headings and subheadings:")
    for heading in headings:
        print(f"{heading['type']}: {heading['section']} {heading['text']}")

if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)















POPPLER_PATH = r"C:\poppler\bin"  # Replace with the path to your Poppler installation

def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF using PyMuPDF for text-based PDFs.
    If PyMuPDF fails, fallback to OCR using pytesseract.
    """
    document = fitz.open(pdf_path)
    text_content = []

    for page_number in range(len(document)):
        page = document[page_number]
        text = page.get_text()

        if not text.strip():  # If the page has no text, fallback to OCR
            images = convert_from_path(pdf_path, first_page=page_number + 1, last_page=page_number + 1, poppler_path=POPPLER_PATH)
            for image in images:
                ocr_text = pytesseract.image_to_string(image)
                text_content.append(ocr_text)
        else:
            text_content.append(text)

    return "\n".join(text_content)
Step 3: Install Required Python Libraries
Ensure the following libraries are installed:

bash
Copy code
pip install PyMuPDF pdf2image pytesseract pillow
Step 4: Test Poppler Installation
Run the following command to verify that Poppler is installed correctly:

bash
Copy code
pdftoppm -h
If you see the help message, Poppler is correctly installed.

Step 5: Run the Script
After configuring the Poppler path and installing dependencies, run the script:

bash
Copy code
python your_script.py
Expected Output
If everything is set up correctly, the script will extract headings and subheadings from your PDF and save them to a CSV file.

Let me know if you encounter further issues!












ChatGPT can make mistakes. Check important info.








import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_path
import re
import csv
import os

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF using PyMuPDF for text-based PDFs.
    If PyMuPDF fails, fallback to OCR using pytesseract.
    """
    document = fitz.open(pdf_path)
    text_content = []

    for page_number in range(len(document)):
        page = document[page_number]
        text = page.get_text()

        if not text.strip():  # If the page has no text, fallback to OCR
            images = convert_from_path(pdf_path, first_page=page_number + 1, last_page=page_number + 1)
            for image in images:
                ocr_text = pytesseract.image_to_string(image)
                text_content.append(ocr_text)
        else:
            text_content.append(text)

    return "\n".join(text_content)

def extract_headings_and_subheadings(text):
    """
    Extract both headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Formation and Continuation').
    """
    headings = []
    lines = text.splitlines()

    for line in lines:
        line = line.strip()
        # Regex to match headings and subheadings
        # Matches: "1. Organization", "1.1 Formation and Continuation", "1.1.1 Details"
        match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)\.", line)
        if match:
            section = match.group(1)  # Section number (e.g., "1.", "1.1.")
            title = match.group(3)  # Title text

            if not is_invalid_heading(title):
                if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                    headings.append({
                        "type": "Heading",
                        "section": section,
                        "text": title.strip() + "."
                    })
                elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                    headings.append({
                        "type": "Subheading",
                        "section": section,
                        "text": title.strip() + "."
                    })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([heading["type"], heading["section"], heading["text"]])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")

    # Step 1: Extract text from the PDF
    print("Extracting text from PDF...")
    text = extract_text_from_pdf(pdf_path)

    # Step 2: Extract headings and subheadings
    print("Extracting headings and subheadings...")
    headings = extract_headings_and_subheadings(text)

    # Step 3: Save to CSV
    print(f"Saving to CSV file: {csv_file}")
    save_headings_to_csv(headings, csv_file)

    # Output results
    count = len(headings)
    print(f"Extracted {count} headings and subheadings:")
    for heading in headings:
        print(f"{heading['type']}: {heading['section']} {heading['text']}")

if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)










import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract both headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Formation and Continuation') from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()

                    # Regex to match headings and subheadings
                    # Matches: "1. Organization", "1.1 Formation and Continuation", "1.1.1 Details"
                    match = re.match(r"^(\d+(\.\d+)*\.?)\s+([^\.\n]+)\.", text)
                    if match:
                        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
                        title = match.group(3)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(title):
                            if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip() + ".",
                                    "page_number": page_number + 1
                                })
                            elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip() + ".",
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)










import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract both top-level headings (e.g., '1. Organization') and clean subheadings (e.g., '1.1 Mission Statement') from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()

                    # Regex to match headings and subheadings
                    # Matches: "1. Organization", "1.1 Mission Statement", "1.1.1 Details"
                    match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*?)(\.|\s)*$", text)
                    if match:
                        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
                        title = match.group(3)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(title):
                            # Differentiate between top-level headings and subheadings
                            if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                            elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)












import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract both top-level headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Something') from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()

                    # Regex to match headings and subheadings
                    # Matches: "1. Organization", "2. List", "1.1 Something", "1.1.1 Details"
                    match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*)$", text)
                    if match:
                        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
                        title = match.group(3)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(title):
                            # Differentiate between top-level headings and subheadings
                            if len(section.split(".")) == 1:  # Top-level heading (e.g., "1.")
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                            elif len(section.split(".")) > 1:  # Subheading (e.g., "1.1")
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)







import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Something') from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Regex to match headings and subheadings
                    # Matches: "1. Organization", "1.1 Something", "1.1.1 Details"
                    match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*)$", text)
                    if match:
                        section = match.group(1)  # Section number (e.g., "1.", "1.1.")
                        title = match.group(3)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(text):
                            if len(section.split(".")) == 1:  # Heading (e.g., "1.")
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                            else:  # Subheading (e.g., "1.1", "1.1.1")
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)











import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract both headings and underlined subheadings from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Regex to match headings and subheadings
                    # Matches: "1. Organization", "2. Capital", "3.2.3 Distribution"
                    match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*)$", text)
                    if match:
                        section = match.group(1)  # Section number (e.g., "1.", "3.2.")
                        title = match.group(3)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(text):
                            if is_upper or "Bold" in font_name or font_size >= 12:  # Heading
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                            elif is_underlined or "Italic" in font_name:  # Subheading
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)









import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract headings (e.g., '1. Organization') and underlined subheadings from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Regex to match headings (e.g., '1. Organization') and subheadings (e.g., '1.1 Distribution')
                    match = re.match(r"^(\d+\.)\s+(.*)$", text)
                    if match:
                        section = match.group(1)  # Section number
                        title = match.group(2)  # Title text

                        # Exclude invalid headings
                        if not is_invalid_heading(text):
                            if is_upper or "Bold" in font_name or font_size >= 12:  # Heading
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                            elif is_underlined:  # Subheading
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)












import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_underlined_subheadings(pdf_path):
    """
    Extract headings and underlined subheadings from a PDF.
    Only keeps text before the first sentence-ending punctuation.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Extract only headings and underlined subheadings
                    if not is_invalid_heading(text):
                        if is_upper or "Bold" in font_name:  # Heading
                            # Keep only the section number and title
                            match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*)$", text)
                            if match:
                                section = match.group(1)  # Section number
                                title = match.group(3)  # Title text
                                headings.append({
                                    "type": "Heading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })
                        elif is_underlined:  # Subheading with underline
                            # Keep only the section number and title
                            match = re.match(r"^(\d+(\.\d+)*\.?)\s+(.*)$", text)
                            if match:
                                section = match.group(1)  # Section number
                                title = match.group(3)  # Title text
                                headings.append({
                                    "type": "Subheading",
                                    "section": section,
                                    "text": title.strip(),
                                    "page_number": page_number + 1
                                })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Section", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["section"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and underlined subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_underlined_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['section']} {heading['text']}")
        count += 1

    print(f"Number of headings and underlined subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)












import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_underlined_subheadings(pdf_path):
    """
    Extract headings and underlined subheadings from a PDF.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Extract only headings and underlined subheadings
                    if not is_invalid_heading(text):
                        if is_upper or "Bold" in font_name:  # Heading
                            headings.append({
                                "type": "Heading",
                                "text": text,
                                "page_number": page_number + 1
                            })
                        elif is_underlined:  # Subheading with underline
                            headings.append({
                                "type": "Subheading",
                                "text": text,
                                "page_number": page_number + 1
                            })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Text"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["text"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and underlined subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_underlined_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['text']}")
        count += 1

    print(f"Number of headings and underlined subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)












import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract headings and subheadings from a PDF.
    Subheadings are identified by underlining, and normal content is ignored.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Ignore normal content and extract only headings and subheadings
                    if not is_invalid_heading(text):
                        if is_upper or "Bold" in font_name:  # Heading
                            headings.append({
                                "type": "Heading",
                                "text": text,
                                "font_name": font_name,
                                "font_size": font_size,
                                "page_number": page_number + 1
                            })
                        elif is_underlined:  # Subheading
                            headings.append({
                                "type": "Subheading",
                                "text": text,
                                "font_name": font_name,
                                "font_size": font_size,
                                "page_number": page_number + 1
                            })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Heading", "Font_Size", "Font_Name"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["text"],
                heading["font_size"],
                heading["font_name"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['text']} (Font: {heading['font_name']}, Size: {heading['font_size']})")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)









import fitz  # PyMuPDF
import re
import csv

def is_invalid_heading(text):
    """
    Check if a heading is invalid (empty, only special characters, or only numbers).
    """
    return re.fullmatch(r"[\W_]*|\d+", text.strip()) is not None

def extract_headings_and_subheadings(pdf_path):
    """
    Extract headings and subheadings from a PDF.
    Subheadings are identified by underlining.
    """
    headings = []

    # Open the PDF file
    document = fitz.open(pdf_path)

    for page_number in range(len(document)):
        page = document[page_number]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_name = span["font"]
                    font_size = span["size"]
                    text = span["text"].strip()
                    is_upper = text.isupper()
                    is_underlined = bool(span.get("flags", 0) & 4)  # Check for underline flag

                    # Identify headings and subheadings
                    if not is_invalid_heading(text):
                        if is_upper or "Bold" in font_name:  # Heading
                            headings.append({
                                "type": "Heading",
                                "text": text,
                                "font_name": font_name,
                                "font_size": font_size,
                                "page_number": page_number + 1
                            })
                        elif is_underlined:  # Subheading
                            headings.append({
                                "type": "Subheading",
                                "text": text,
                                "font_name": font_name,
                                "font_size": font_size,
                                "page_number": page_number + 1
                            })

    return headings

def save_headings_to_csv(headings, csv_file):
    """
    Save the extracted headings and subheadings to a CSV file.
    """
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Page", "Type", "Heading", "Font_Size", "Font_Name"])
        for heading in headings:
            writer.writerow([
                heading["page_number"],
                heading["type"],
                heading["text"],
                heading["font_size"],
                heading["font_name"]
            ])

def main(pdf_path):
    """
    Main function to extract and save headings and subheadings from a PDF.
    """
    csv_file = pdf_path.replace(".pdf", "_headings.csv").replace(".PDF", "_headings.csv")
    headings = extract_headings_and_subheadings(pdf_path)
    
    count = 0
    for heading in headings:
        print(f"Page {heading['page_number']} ({heading['type']}): {heading['text']} (Font: {heading['font_name']}, Size: {heading['font_size']})")
        count += 1

    print(f"Number of headings and subheadings: {count}")
    save_headings_to_csv(headings, csv_file)
    print(f"Headings and subheadings saved to {csv_file}")

# Example usage
if __name__ == "__main__":
    pdf_path = "example.pdf"  # Replace with the path to your PDF file
    main(pdf_path)






import fitz  # PyMuPDF
import re

def extract_headings_and_subheadings(pdf_path):
    headings = []
    section_pattern = re.compile(r'^\d+(\.\d+)*')  # Matches patterns like 1, 1.1, 1.1.1, etc.
    
    # Open the PDF
    doc = fitz.open(pdf_path)
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]
        
        for block in blocks:
            if "lines" in block:
                for line in block["lines"]:
                    # Extract font size and text
                    text = " ".join(span["text"] for span in line["spans"]).strip()
                    font_size = line["spans"][0]["size"]  # Assuming all spans in the line have the same size
                    
                    if section_pattern.match(text):  # Match section numbers
                        headings.append((font_size, text))
    
    # Post-process: Categorize by font size
    # Assuming larger font sizes are main headings
    font_sizes = [size for size, _ in headings]
    threshold = sum(font_sizes) / len(font_sizes)  # Use average font size as a threshold
    
    main_headings = [text for size, text in headings if size > threshold]
    subheadings = [text for size, text in headings if size <= threshold]
    
    return main_headings, subheadings

# Usage
pdf_path = "your_document.pdf"
main_headings, subheadings = extract_headings_and_subheadings(pdf_path)

print("Main Headings:")
print("\n".join(main_headings))

print("\nSubheadings:")
print("\n".join(subheadings))






import fitz  # PyMuPDF
import re

def extract_headings_from_pdf(pdf_path):
    headings = []
    section_pattern = re.compile(r'^\d+(\.\d+)*')  # Matches patterns like 1, 1.1, 1.1.1, etc.
    
    # Open the PDF
    doc = fitz.open(pdf_path)
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]
        
        for block in blocks:
            if "lines" in block:
                for line in block["lines"]:
                    text = " ".join(span["text"] for span in line["spans"]).strip()
                    if section_pattern.match(text):  # Match section numbers
                        headings.append(text)
    
    return headings

# Usage
pdf_path = "your_document.pdf"
headings = extract_headings_from_pdf(pdf_path)
print("\n".join(headings))









import fitz  # PyMuPDF
import re





def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF,
    filtering out noisy content like standalone numbers and retaining valid headings.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, and cleaned text.
    """
    # Open the PDF
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            if "lines" not in block:
                continue

            for line in block["lines"]:
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()

                    # Skip noisy content: Only keep lines with valid text and patterns
                    if len(text) <= 2 or text.isdigit():
                        continue

                    # Clean the text: Remove dots or unwanted characters
                    cleaned_text = re.sub(r"[\.]{2,}", "", text).strip()
                    cleaned_text = re.sub(r"\s{2,}", " ", cleaned_text)  # Remove extra spaces

                    # Regex to identify valid section numbers (e.g., 1, 1.1, 1.1.1)
                    match = re.match(r"^(\d+(\.\d+)*)(\s+.+)", cleaned_text)
                    if match:
                        section_number = match.group(1).strip()  # Extract the section number
                        heading_text = match.group(3).strip()    # Extract the heading text

                        # Append valid headings only
                        if heading_text:
                            extracted_data.append({
                                "page": page_num + 1,
                                "section_number": section_number,
                                "text": heading_text
                            })

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
if extracted_data:
    print_extracted_data(extracted_data)
else:
    print("No valid headings or subheadings found in the document.")











import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF,
    filtering out noisy content like standalone numbers.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, and cleaned text.
    """
    # Open the PDF
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            if "lines" not in block:
                continue

            for line in block["lines"]:
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()

                    # Skip noisy content: Ignore lines that are only numbers
                    if text.isdigit():
                        continue

                    # Clean the text to remove unwanted dots or patterns
                    cleaned_text = re.sub(r"[\.]{2,}", "", text).strip()
                    cleaned_text = re.sub(r"\s{2,}", " ", cleaned_text)  # Remove extra spaces

                    # Regex to match valid section numbers and headings
                    match = re.match(r"^(\d+(\.\d+)*)(\s+.*)", cleaned_text)
                    if match:
                        section_number = match.group(1).strip()  # Extract the section number
                        heading_text = match.group(3).strip()    # Extract the heading text

                        # Filter out empty or short headings and numeric-only headings
                        if heading_text and not heading_text.isdigit() and len(heading_text) > 1:
                            extracted_data.append({
                                "page": page_num + 1,
                                "section_number": section_number,
                                "text": heading_text
                            })

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
if extracted_data:
    print_extracted_data(extracted_data)
else:
    print("No valid headings or subheadings found in the document.")









import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, text, and position.
    """
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]

        page_data = []

        for block in blocks:
            if "lines" not in block:
                continue

            for line in block["lines"]:
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()
                    bbox = span["bbox"]

                    # Regex to match valid section numbers and headings
                    match = re.match(r"^(\d+(\.\d+)*)(\s+.*)", text)
                    if match:
                        section_number = match.group(1).strip()
                        heading_text = match.group(3).strip()

                        # Filtering noisy content: Only include lines with proper section numbering
                        if heading_text and len(heading_text) > 1:
                            page_data.append({
                                "page": page_num + 1,
                                "section_number": section_number,
                                "text": heading_text,
                                "position": bbox
                            })

        # Add data for this page only if valid headings exist
        if page_data:
            extracted_data.extend(page_data)

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
if extracted_data:
    print_extracted_data(extracted_data)
else:
    print("No valid headings or subheadings found in the document.")





import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF,
    excluding unwanted characters like '............'.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, and cleaned text.
    """
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        blocks = page.get_text("dict")["blocks"]

        page_data = []

        for block in blocks:
            if "lines" not in block:
                continue

            for line in block["lines"]:
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()

                    # Clean the text to remove unwanted dots or similar patterns
                    cleaned_text = re.sub(r"[\.]{2,}", "", text).strip()  # Remove repeated dots
                    cleaned_text = re.sub(r"\s{2,}", " ", cleaned_text)   # Remove extra spaces

                    # Regex to match valid section numbers and headings
                    match = re.match(r"^(\d+(\.\d+)*)(\s+.*)", cleaned_text)
                    if match:
                        section_number = match.group(1).strip()
                        heading_text = match.group(3).strip()

                        # Filter out invalid or empty headings
                        if heading_text and len(heading_text) > 1:
                            page_data.append({
                                "page": page_num + 1,
                                "section_number": section_number,
                                "text": heading_text
                            })

        # Add data for this page only if valid headings exist
        if page_data:
            extracted_data.extend(page_data)

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
if extracted_data:
    print_extracted_data(extracted_data)
else:
    print("No valid headings or subheadings found in the document.")






import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, text, and position.
    """
    # Open the PDF
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        # Extract blocks of text from the page
        blocks = page.get_text("dict")["blocks"]

        page_data = []  # Temporary storage for headings on this page

        for block in blocks:
            # Check if the block contains lines
            if "lines" not in block:
                continue

            for line in block["lines"]:
                # Check if the line contains spans
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()
                    bbox = span["bbox"]  # Bounding box for text position

                    # Regex to identify section numbers (e.g., 1, 1.1, 1.1.1)
                    match = re.match(r"^(\d+(\.\d+)*)(.*)", text)
                    if match:
                        section_number = match.group(1).strip()  # Extract the section number
                        heading_text = match.group(3).strip()    # Extract the heading text
                        page_data.append({
                            "page": page_num + 1,
                            "section_number": section_number,
                            "text": heading_text,
                            "position": bbox
                        })

        # Add data from this page only if it contains headings/subheadings
        if page_data:
            extracted_data.extend(page_data)

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
if extracted_data:
    print_extracted_data(extracted_data)
else:
    print("No headings or subheadings found in the document.")







import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, text, and position.
    """
    # Open the PDF
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        # Extract blocks of text from the page
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            # Check if the block contains lines
            if "lines" not in block:
                continue

            for line in block["lines"]:
                # Check if the line contains spans
                if "spans" not in line:
                    continue

                for span in line["spans"]:
                    text = span["text"].strip()
                    bbox = span["bbox"]  # Bounding box for text position

                    # Regex to identify section numbers (e.g., 1, 1.1, 1.1.1)
                    match = re.match(r"^(\d+(\.\d+)*)(.*)", text)
                    if match:
                        section_number = match.group(1).strip()  # Extract the section number
                        heading_text = match.group(3).strip()    # Extract the heading text
                        extracted_data.append({
                            "page": page_num + 1,
                            "section_number": section_number,
                            "text": heading_text,
                            "position": bbox
                        })

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
print_extracted_data(extracted_data)





import fitz  # PyMuPDF
import re

def extract_headings_with_sections(pdf_path):
    """
    Extract headings, subheadings, and section numbers from a PDF using PyMuPDF.

    Parameters:
        pdf_path (str): Path to the PDF file.

    Returns:
        list: A list of dictionaries containing page number, section number, text, and position.
    """
    # Open the PDF
    doc = fitz.open(pdf_path)
    extracted_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        # Extract blocks of text from the page
        blocks = page.get_text("dict")["blocks"]

        for block in blocks:
            for line in block["lines"]:
                for span in line["spans"]:
                    text = span["text"].strip()
                    bbox = span["bbox"]  # Bounding box for text position

                    # Regex to identify section numbers (e.g., 1, 1.1, 1.1.1)
                    match = re.match(r"^(\d+(\.\d+)*)(.*)", text)
                    if match:
                        section_number = match.group(1).strip()  # Extract the section number
                        heading_text = match.group(3).strip()    # Extract the heading text
                        extracted_data.append({
                            "page": page_num + 1,
                            "section_number": section_number,
                            "text": heading_text,
                            "position": bbox
                        })

    return extracted_data


def print_extracted_data(data):
    """
    Print extracted headings, subheadings, and section numbers in a formatted table.

    Parameters:
        data (list): List of extracted data dictionaries.
    """
    print(f"{'Page':<6} {'Section':<10} {'Text'}")
    print("=" * 50)
    for item in data:
        print(f"{item['page']:<6} {item['section_number']:<10} {item['text']}")


# Path to your PDF file
pdf_path = "example.pdf"

# Extract headings, subheadings, and section numbers
extracted_data = extract_headings_with_sections(pdf_path)

# Print the extracted data
print_extracted_data(extracted_data)


=SUMPRODUCT(--(ISNUMBER(SEARCH(TRIM(MID(SUBSTITUTE(A1, " ", REPT(" ", 100)), ROW(INDIRECT("1:" & LEN(A1)-LEN(SUBSTITUTE(A1, " ", ""))+1))*100-99, 100)), B1)))) / (LEN(A1)-LEN(SUBSTITUTE(A1, " ", ""))+1) * 100




# Initialize counters and sums
correct_matches = 0  # Total correctly matched paragraphs
matched_completeness_sum = 0  # Sum of completeness scores for matched paragraphs
matched_count = 0  # Count of matched paragraphs
total_gt_paragraphs = len(ground_truth_paragraphs)  # Total ground truth paragraphs
completeness_sum = 0  # Sum of completeness scores for all predictions

# Process predictions and ground truth
for gt_para, predicted_para in zip(ground_truth_paragraphs, predicted_paragraphs):
    # Find best match score and index for each ground truth paragraph
    best_match_score, best_match_idx = get_best_match(gt_para, predicted_para)

    if best_match_score > 0:  # Match found
        correct_matches += 1
        matched_completeness_sum += best_match_score  # Only for matched paragraphs
        matched_count += 1

    completeness_sum += best_match_score  # Sum completeness for all paragraphs

    # Append row-wise results
    result_rows.append({
        "groundTruth": gt_para,
        "predicted": predicted_para if predicted_para else "",
        "extracted": best_match_score > 0,  # True if a match exists
        "matching_ai_index": best_match_idx if best_match_score > 0 else -1,
        "completeness": best_match_score,
        "correctness": 1 if best_match_score > 0 else 0
    })

# Process extra ground truth paragraphs not matched with any prediction
extra_gts = [gt for gt in ground_truth_paragraphs if gt not in [row["groundTruth"] for row in result_rows]]

for pr_gt in extra_gts:
    result_rows.append({
        "groundTruth": pr_gt,
        "predicted": None,
        "extracted": False,
        "matching_ai_index": None,
        "completeness": 0,
        "correctness": 0
    })

# Calculate Overall Metrics
overall_correctness = correct_matches / total_gt_paragraphs if total_gt_paragraphs > 0 else 0
overall_completeness = (
    matched_completeness_sum / matched_count if matched_count > 0 else 0
)

# Calculate Recall and Precision
count_based_recall = correct_matches / total_gt_paragraphs if total_gt_paragraphs > 0 else 0
count_based_precision = correct_matches / len(predicted_paragraphs) if len(predicted_paragraphs) > 0 else 0

# Calculate Business Score
business_score = (0.6 * overall_correctness) + (0.4 * overall_completeness)

# Summary of results
summary = {
    "Overall Correctness": overall_correctness,
    "Overall Completeness": overall_completeness,
    "Recall": count_based_recall,
    "Precision": count_based_precision,
    "Business Score": business_score
}

# Return final result rows and summary
return result_rows, summary






Algorithm for the Document Processing Workflow
Here's a step-by-step algorithm that outlines the complete process for the document processing workflow using Azure and AI services:

Initialize Connections and Load Configurations

Establish connections to Azure Blob Storage using managed identity configurations (pm_azure_blob_connection and di_azure_blob_connection).
Load configurations for AI models, regex patterns, and document processing settings.
Input Document Acquisition

Retrieve the input document from Azure Blob Storage using the specified connection details.
Validate the document's format and accessibility.
Artifact Generation and Initialization

Initialize artifacts to capture essential document metadata:
Page Dimensions: Record the size of each page in the document.
Document Layout: Identify structural elements such as text blocks, tables, and images.
Logical Chunks: Segment the document into manageable sections for targeted analysis.
Document Parsing using Azure Document Intelligence

Parse the document using the Azure Cognitive Services model (prebuilt-layout).
Extract and store the document layout, page dimensions, and logical chunks for further processing.
Regex Filtering of Logical Chunks

Apply regex patterns to the logical chunks to filter out sections containing specific financial keywords.
Extract chunks that match the keywords such as “Management Fee,” “Compensation,” and other relevant terms.
AI Filtering of Extracts Using Azure OpenAI

Use Azure OpenAI models to further refine the filtered logical chunks.
Input: Filtered logical chunks.
Output: AI-filtered extracts containing relevant paragraphs that answer predefined financial questions.
Key Terms Identification

Identify key financial and legal terms within the AI-filtered extracts using the AI model.
Extract a list of key terms that are significant for understanding the financial context of the document.
Regex Filtering of Key Term Definitions

Apply regex patterns to further filter the identified key terms to pinpoint exact definitions within the document.
Extract text snippets that provide definitions for the key terms.
AI Filtered Key Term Definitions

Use Azure OpenAI to refine and enhance the definitions of the key terms.
Input: Regex-filtered key term definitions.
Output: Contextualized, detailed definitions of each key term.
AI MFees Interpretations

Generate interpretations of the management fee-related extracts using AI models.
Simplify complex financial language into concise, understandable summaries.
Provide interpretations that highlight critical insights about management fees and their calculations.
AI Key Term Definitions Interpretations

Use AI models to provide clear and comprehensive interpretations of the key term definitions.
Cross-reference key term definitions with relevant document content to ensure accuracy.
Generate summarized interpretations that facilitate easy understanding of complex financial and legal terms.
Output Generation

Compile the results, including page dimensions, document layout, logical chunks, filtered extracts, key terms, definitions, and interpretations.
Format the output data for presentation, reporting, or further analysis by end-users.
Error Handling and Validation

Implement error handling for data retrieval, model processing, and interpretation stages.
Validate outputs at each stage to ensure consistency and accuracy before final output generation.
End of Process

Save the processed data and results back to Azure Blob Storage or an alternative storage solution as required.
Log the process flow and outputs for auditing and troubleshooting purposes.
Key Points
The algorithm leverages a combination of regex filtering and AI-driven extraction techniques to ensure precise identification and interpretation of key financial terms within complex documents.
Each step is validated and cross-referenced to ensure data integrity and accuracy, providing reliable outputs that enhance decision-making.
The integration of Azure OpenAI models adds contextual understanding to the extracted information, making it easier to navigate complex legal and financial language.
This algorithm provides a structured approach to processing and analyzing financial documents, enabling automated insights that are critical for legal, financial, and compliance needs. Let me know if you need further details or specific code implementations for each step!










ChatGPT can make mistakes. Check









1. Input Document Acquisition
Description: This stage involves fetching the input documents from Azure Blob Storage using a managed identity for secure access. The system connects to specific Azure Blob containers using predefined connection settings to locate and retrieve the document files needed for processing.
Purpose: To provide the initial data required for analysis, ensuring that the documents are securely accessed and correctly identified.
2. Artifact Generation and Initialization
Description: Once the documents are retrieved, the system initializes various artifacts that are essential for further processing. This includes setting up data structures like page dimensions, document layout, logical chunks, key term lists, and more. These artifacts help in organizing and structuring the content of the document for subsequent analysis.
Components:
Page Dimensions: Captures the size and layout of each page.
Document Layout: Defines the structure of the document.
Logical Chunks: Divides the document into meaningful sections for processing.
3. Document Parsing
Description: In this stage, Azure Document Intelligence is used to analyze the document's layout and content. It identifies various elements such as text, tables, images, and other structural components. This process is crucial for extracting meaningful chunks of data that can be further analyzed.
Purpose: To break down the document into its fundamental parts, making it easier to target specific information during subsequent analysis.
4. Regex Filtering of Logical Chunks
Description: The logical chunks of the document are filtered using predefined regex patterns to identify specific keywords related to financial and management terms, such as "Management Fee," "Compensation," and others. This stage narrows down the content to the most relevant sections based on the target terms.
Purpose: To refine the document content by focusing on relevant sections that contain specific key terms, allowing for more efficient and targeted analysis.
5. AI Filtering of Extracts
Description: This stage uses Azure OpenAI models to further refine the extracts identified by the regex filtering. The AI models are prompted to answer specific financial questions or highlight relevant content related to management fees and other terms.
Purpose: To enhance the filtering process by leveraging AI's ability to understand context and provide insights beyond simple keyword matching.
6. Key Terms Identification
Description: AI models analyze the filtered content to identify key financial and legal terms. This process involves scanning the text to detect significant terms that are essential for understanding the document's content.
Purpose: To create a comprehensive list of key terms that are critical for interpreting the financial information presented in the document.
7. Regex Filtering of Key Term Definitions
Description: Identified key terms are subjected to another round of filtering using regex patterns to pinpoint the precise definitions within the document. This helps in associating each key term with its most relevant and accurate description.
Purpose: To ensure that each key term is clearly defined within the context of the document, enhancing comprehension for further analysis.
8. AI Filtered Key Term Definitions
Description: Azure OpenAI models are used to refine the definitions of the key terms identified in the previous stage. The models are prompted to provide detailed, contextual definitions based on the document content.
Purpose: To deliver clear, context-specific definitions that help in understanding the financial and legal nuances of the key terms.
9. AI MFees Interpretations
Description: The AI models interpret and summarize the management fee-related content extracted from the document. This involves breaking down complex financial language into simpler, more digestible summaries.
Purpose: To provide actionable insights and interpretations of management fee structures, calculations, and related financial information.
10. AI Key Term Definitions Interpretations
Description: In the final stage, AI models provide interpretations of the key term definitions, summarizing the financial and legal implications in a clear and concise manner. This includes cross-referencing the definitions with the document's content to ensure accuracy.
Purpose: To synthesize the key term definitions into understandable interpretations that can be easily used by non-experts, supporting decision-making and compliance.
These descriptions outline the specific functionality and purpose of each stage in the document processing workflow, highlighting how Azure and AI services are integrated to achieve comprehensive document analysis. Let me know if you need further details on any of these stages!






You said:
Overall description about the project 
ChatGPT said:
ChatGPT
Overall Description of the Project
The project is a comprehensive document processing and analysis system designed to handle complex financial and legal documents using Azure and AI technologies. The primary goal of the system is to extract, interpret, and summarize critical information related to management fees and other key financial terms found within these documents. This automated process aims to provide accurate, relevant insights, reducing manual effort and enhancing the efficiency of document analysis for financial institutions, legal firms, and other stakeholders.

Key Objectives
Automate Document Parsing and Analysis: Utilize Azure Document Intelligence and OpenAI models to automate the extraction and analysis of complex documents, thereby reducing the time and effort required for manual review.
Identify and Interpret Key Financial Terms: Use AI and regex-based filtering to identify key terms related to financial management, such as management fees, compensation, and other expenses, ensuring that relevant information is accurately highlighted.
Provide Clear, Actionable Insights: Interpret the extracted content to deliver concise, understandable summaries of complex financial terms, enabling better decision-making and compliance review.
Technical Components
Azure Blob Storage: Serves as the secure storage location for all input documents, enabling easy access and retrieval through managed identity connections.
Azure Cognitive Services (Document Intelligence): Analyzes the document structure, extracting logical chunks, page layouts, and other relevant information necessary for detailed processing.
Azure OpenAI Models: Plays a central role in refining the extracted content, identifying key terms, and generating interpretations and definitions that are contextually accurate.
Regex Filtering: Combines traditional pattern matching techniques with AI models to ensure precise extraction of financial and legal terms.
Workflow Overview
Document Acquisition: The process begins with retrieving the document from Azure Blob Storage, followed by artifact initialization to prepare the document for analysis.
Document Parsing and Chunking: Azure Document Intelligence parses the document into logical sections, enabling targeted analysis of specific content areas.
Filtering and AI-Driven Extraction: Key terms and relevant paragraphs are identified using regex filtering and AI models, focusing on specific financial terms related to management fees and other expenses.
Interpretation and Summarization: AI models provide detailed interpretations of the identified content, breaking down complex legal and financial information into manageable insights.
Output Generation: The system produces clear, organized summaries and interpretations that can be used directly by stakeholders for financial analysis, legal compliance, or reporting purposes.
Benefits
Enhanced Accuracy and Efficiency: Automates the extraction and interpretation of critical information, significantly reducing the risk of human error.
Time Savings: Speeds up the document review process, allowing analysts and legal professionals to focus on higher-value tasks rather than manual content extraction.
Improved Decision-Making: Provides actionable insights by breaking down complex financial terms into understandable summaries, facilitating better decision-making and strategic planning.
Scalability: The system is designed to handle large volumes of documents, making it suitable for organizations with extensive document processing needs.
Use Cases
Financial Institutions: For processing investment fund documents, understanding fee structures, and ensuring compliance with regulatory requirements.
Legal Firms: Assisting in the review of complex contracts, identifying critical terms, and providing clear interpretations for legal advice.
Corporate Finance Teams: Analyzing management fees, compensation terms, and other expenses to optimize financial management and strategy.
Conclusion
This project leverages state-of-the-art AI and cloud technologies to transform how complex financial documents are processed and interpreted. By automating key aspects of document analysis, the system not only enhances accuracy and efficiency but also empowers users with valuable insights that drive better business outcomes.

Let me know if you need further customization or additional information!









ChatGPT can make mistakes. Check important








ChatGPT 4o







You said:
{

"version": 1,

"vars": {

"pm_azure_blob_connection": {

"auth_type": "MANAGED",

"mi_client_id": "5da05a4c-edb2-4f9f-8ad8-d11f0214f557",

"account_url": "https://npeedevgtsd01txtmrm.blob.core.windows.net",

"doc_container": "pvt-markets"

},

"di_azure_blob_connection": {

"auth_type": "MANAGED",

"mi_client_id": "5da05a4c-edb2-4f9f-8ad8-d11f0214f557",

"account_url": "https://npeedevgtsd01txtmrm.blob.core.windows.net",

"doc_container": "npeedevgtsd@1txtmrm-strgcont-txtmrm-01"

}, "input_document_path": {

"source": "AzureBlob",

I

params": { "

}

}

"connection": "{vars.pm_azure_blob_connection)",

"storage_location": "(payload.documents[0].storagePath.absolute_path}"

},

"artifacts": {

"pageDimensions": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

},
"documentLayout": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

},

"logicalChunks": {

"source": "AzureBlob",

"data": [],

"params": {

"connection": "{vars.di_azure_blob_connection)",

"absolute_location": "<path_to_file>",

"encoding": "utf_8 e.g.// utf_8, binary, file_like_object"

},

"extant": false

}, "extractsUsingPrescribed Language": {

"source": "Transient",

I

"data": [],

"params": {},

"extant": false

},

"aiFilteredExtracts": {

"source": "Transient",

"data": [],

"params": {}, "extant": false
},

"keyTerms": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

}, "regexFilteredDefinitions": {

"source": "Transient",

"data": [],

"params": {},

"extant": false

},

"keyTermDefinitions": {

"source": "Transient",

"data": [],

"params": {},

"extant": true

},

"keyTermMapping":{

"source": "Transient",

"data": [],

"params": {},

"extant": true

},

"mFees Interpretations": {

"source": "AzureBlob",

data": [], "

"params": {

"connection": "{vars.di_azure_blob_connection)",

"absolute_location": "<path_to_file>",
"encoding": "utf_8 e.g.// utf_8, binary, file_like_object"

},

"extant": false

},

"keyTermInterpretations": {

"source": "AzureBlob",

"data": [],

"params": {

"connection": "{vars.di_azure_blob_connection)",

"absolute_location": "<path_to_file>",

"encoding": "utf_8 e.g.// utf_8, binary, file_like_object"

},

"extant": false

}

}, "output": {

"pageDimensions": "(artifacts.pageDimensions.data}", "documentLayout": "(artifacts.documentLayout.data}",

"logicalChunks": "{artifacts.logicalChunks.data}",

"extractsUsing PrescribedLanguage": "{artifacts.extractsUsing PrescribedLanguage.data}",

"keyTerms": "{artifacts.keyTerms.data)",

"keyTermDefinitions": "{artifacts.keyTermDefinitions.data)",

"keyTermMapping": "{artifacts.keyTermMapping.data)",

"aiFilteredExtracts": "{artifacts.aiFilteredExtracts.data}", "mFees Interpretations": "{artifacts.mFees Interpretations.data)",

"keyTermInterpretations": "{artifacts.keyTermInterpretations.data}"

},
"stages": [

{

"capability": "DocumentParsing",

"method": "AzureDocument Intelligence",

"label": "DocumentParsing",

"processing_config": {

"processing_engine": "Azure-openai",

"model": "prebuilt-layout",

"endpoint": "https://npee-use2-dev-fr-01.cognitiveservices.azure.com/",

"key": "bacfb986c0874ace9211551852829213"

},

"input": {

"document_path": "{vars.input_document_path}"

}, "output": {

"(artifacts.logicalChunks.data}": "logicalChunks",

"{artifacts.documentLayout.data)": "document Layout",

"(artifacts.pageDimensions.data)": "pageDimensions"

"capability": "GenAIExtraction",

"method": "RegexFiltering",

"label": "Logical Chunks Regex Filtering",

"input": {

"logicalChunks": "(artifacts.logicalChunks.data}",

"keywords": [

"Management Fee",

"Management +Fee",

I

}

"Servicing Fee", "Servicing +Fee",

"Investment Advisory Fee",
"Investment +Advisory +Fee",

"Compensation",

"Remuneration",

"Expenses",

"AIFM Fee",

"AIFM +Fee"

},

]

"{artifacts.extractsUsing Prescribed Language.data}": "extractsUsing Prescribed Language"

}

"output": {

"capability": "GenAIExtraction",

"method": "Prompting",

"label": "AI Filtered MFees Related Extracts",

"processing_config": {

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

}, "temperature": 0,

"seed": 17

"prompt": {

"template":" Objective \nYou will be given a list of paragraphs. You need to identify the paragraphs that helpenser one or more of exes which helped answer one or more of the questions. Ind. Create a field 'questions: List of the question Indexes that was wered by the paragra (

"input variables"

"questions":{

"indexing required": true,

"data":[

"Can you explain the calculation, payment timing, and due dates of the Management Fees?"

"How are the Management Fees affected by the Fund's Investment in any Primary Investee fund or Secondary Investee Fund, "How is the Management Fee calculated and reduced in relation to various Investments and fees

"paragraphs": {

"indexing required": true,

"date": "(artifacts.extractsUsing PrescribedLanguage.data)"

"post process required": true

"output" (

"(artifacts.eiFilteredExtracts.data)": "aifilteredExtracts"

"capability": "GenAZExtraction",

"method": "Prompting",

"label": "Keyteres Identification",

"processing config":{

model connection: (

"model provider_type": "AzureüpenAI",

"openai api type": "azurM
"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 17

},

"prompt": {

"template": "\n# Objective\nYou will be given a list of paragraphs. Your task is to identify the keyterms present in eac

ing company, financial services company and partnership.\n7. Respond back with the list of keyterms against each Paragraph in title case

"input_variables": {

"paragraphs": {

"indexing_required": true,

"data": "{artifacts.aiFilteredExtracts.data}"

}

},

"post_process_required": true

},

"output": {

"{artifacts.keyTerms.data}": "keyTerms"

"capability": "GenAIExtraction",

"method": "RegexFiltering",

"label": "Regex Filtered Keyterm Definitions",

input": { "

"logicalChunks": "fartifacts.logicalChunks.data}",

"keyTerms": "{artifacts.keyTerms.data}"

},
"output": {

"(artifacts.regexFilteredDefinitions.data)": "regexFilteredDefinitions"

}

"capability": "GenAIExtraction",

"method": "Prompting",

"label": "AI Filtered Keyterm Definitions",

processing_config": { "

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 2

}, "prompt": {

"template": "\n# Objective\nYou will be provided with a numbered list of Paragraphs and Keyterms. For each of the provided Keyter

hat provides the definition of the Keyterm, return -1 within a list as the index response to the Keyterm.\n\n\n# Output Instructions:\n\n## Outpu

"input_variables": { "paragraphs ": {

"indexing_required": true,

"data": "(artifacts.regexFilteredDefinitions.data)"

}, "keyterms": {

"indexing_required": true,

"data": "{artifacts.keyTerms.data)"
}

}, "post_process_required": true

}, "output": {

"(artifacts.keyTermDefinitions.data)": "keyTermDefinitions",

"(artifacts.keyTermMapping.data)": "keyTermMapping"

}

"capability": "GenAIExtraction",

"method" : "Prompting",

"label": "AI MFees Interpretations",

"processing_config": {

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview",

"openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 2

}, "prompt": {

"template": "\n# Objective\nYou are tasked to interpret and summarize legal and financial content related to investment funds and manag in title case or capital letters are not changed in the interpretation.\n6. Break down the interpretation into small sentences.\n7. If you are not able terpretations for the paragraph using the reference of 'Keyterm Definition Extracts' if required to understand the content of the paragraph for interpr

input_variables": { " "

mFees InterpretationsExtracts": {

"indexing_required": true,
"data": "(artifacts.aiFilteredExtracts.data}" },

"KeyTermDefinitionExtracts": {

"indexing_required": true,

"data": "{artifacts.keyTermDefinitions.data}"

}

},

"post_process_required": false

}, "output": {

"{artifacts.mFees Interpretations.data)": "mFees Interpretations"

}

"capability": "GenAIExtraction",

"method": "Prompting",

"label": "AI Keyterm Definitions Interpretations",

"processing_config": {

"model_connection": {

"model_provider_type": "AzureOpenAI",

"openai_api_type": "azure",

"azure_openai_endpoint": "https://ssgpt-predev.openai.azure.com/",

"azure_openai_api_version": "2023-07-01-preview", "openai_api_key": "43d6ab1448834574807eff7a195f76f3",

"deployment_name": "ssgpt-40"

},

"temperature": 0,

"seed": 2

}, "prompt": {

"template": "\n# Objective\nYou are tasked to interpret and summarize legal and financial content related to investment f wotation of the keyterm definition.\n\n# Understanding the layout of 'Keyterm Definition Extracts' provided in the i

The contents of 'Keyterm Definition Extracts' are bullet listed paragraphs, with each paragraph containing the mapping of s.",

"input_variables": {

"KeyTermDefinitionExtracts": {

"indexing_required": true,

"data": "{artifacts.keyTermDefinitions.data}"

},

"keyTermMapping": {

"indexing_required": true,

"data": "{artifacts.keyTermMapping.data}"

}

},

"post_process_required": true

},

"output": {

"{artifacts.key TermInterpretations.data}": "key Term Interpretations"

}

}

]

}



""Create a high level flowchat of work flow and create a detailed technical stage wise documentation and create service wise functinolity write up
ChatGPT said:
ChatGPT
To create a comprehensive understanding of the provided JSON configuration, we'll break it down into a detailed flowchart of the workflow and a stage-wise documentation that includes service-wise functionality descriptions. Here's the approach:

High-Level Flowchart of Workflow
Input Document Acquisition

Source: Azure Blob Storage.
Connections: pm_azure_blob_connection and di_azure_blob_connection.
Data: Input document path fetched from Azure Blob.
Artifact Generation and Initialization

Artifacts Created:
Page Dimensions
Document Layout
Logical Chunks
Extracts Using Prescribed Language
AI Filtered Extracts
Key Terms
Key Term Definitions
Key Term Mapping
MFees Interpretations
Key Term Interpretations
Document Parsing Stage

Capability: DocumentParsing
Method: AzureDocument Intelligence (prebuilt-layout model).
Output: Logical chunks, document layout, and page dimensions.
Regex Filtering Stage

Capability: GenAIExtraction
Method: RegexFiltering
Input: Logical chunks filtered with specified keywords (Management Fees, Compensation, etc.).
Output: Extracts using Prescribed Language.
AI Filtering of Extracts

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: AI Filtered MFees Related Extracts.
Key Terms Identification

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: Key Terms.
Regex Filtering of Key Term Definitions

Capability: GenAIExtraction
Method: RegexFiltering
Output: Regex Filtered Key Term Definitions.
AI Filtered Key Term Definitions

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: Key Term Definitions and Mapping.
AI MFees Interpretations

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: MFees Interpretations.
AI Key Term Definitions Interpretations

Capability: GenAIExtraction
Method: Prompting with AzureOpenAI.
Output: Key Term Interpretations.
Stage-wise Documentation
Input Document Acquisition

Description: Fetches documents from Azure Blob Storage using managed identity connections (pm_azure_blob_connection and di_azure_blob_connection).
Purpose: To provide input data for processing through the Azure OpenAI models.
Document Parsing

Description: Parses the document to identify layout and logical chunks using Azure Cognitive Services.
Functionality:
Extracts structural information like page dimensions and layout.
Identifies logical sections of the document for further analysis.
Regex Filtering of Logical Chunks

Description: Uses regex patterns to filter logical chunks for specific keywords related to fees and financial terms.
Functionality:
Focuses on terms like "Management Fee," "Compensation," and other financial keywords.
Extracts text segments relevant to fee calculations and descriptions.
AI Filtering of Extracts

Description: Applies Azure OpenAI models to refine filtered chunks by identifying relevant fee-related content.
Functionality:
Uses prompting techniques to focus on answering specific financial questions.
Outputs filtered extracts that provide relevant answers.
Key Terms Identification

Description: Identifies key financial and legal terms within the filtered extracts.
Functionality:
Uses AI models to detect and list key terms.
Provides context for understanding financial terminology within the document.
Regex Filtering of Key Term Definitions

Description: Further filters the key terms to refine their definitions using regex patterns.
Functionality:
Matches and extracts text that defines the identified key terms.
AI Filtered Key Term Definitions

Description: AI models provide detailed definitions and mapping for identified key terms.
Functionality:
Outputs refined definitions that are contextually accurate for financial interpretations.
MFees Interpretations

Description: Interprets the filtered extracts related to management fees and other financial terms.
Functionality:
Breaks down complex financial text into manageable summaries.
Provides interpretative insights for legal and financial understanding.
Key Term Definitions Interpretations

Description: Provides a clear, summarized interpretation of key term definitions with contextual mapping.
Functionality:
Offers a simplified, accurate understanding of complex legal and financial definitions.
Supports clearer comprehension for non-technical stakeholders.
Service-Wise Functionality Write-up
Azure Blob Storage

Manages document storage and retrieval securely using managed identity for access.
Azure Cognitive Services - Document Intelligence

Parses and analyzes document structure, enabling logical division and understanding of document layouts.
Azure OpenAI Services

Implements AI-driven extraction, filtering, and interpretation of financial and legal text.
Models configured with specific prompting strategies to ensure accurate, relevant outputs.
Regex Filtering and AI Extraction Methods

Ensures precise identification of terms and extracts based on defined patterns and AI model capabilities.
Enhances the accuracy of document analysis by combining deterministic regex and probabilistic AI techniques.
This documentation provides a detailed overview of each step, outlining how Azure and OpenAI services are used to process, extract, and interpret complex financial documents effectively. Let me know if you need any specific adjustments or further details!






You said:
I need flow chat diagram 
ChatGPT said:
ChatGPT

Create a flowchart diagram illustrating a document processing workflow using Azure and AI services. The flowchart should include the following stages: 1) Input Document Acquisition from Azure Blob Storage, 2) Artifact Generation and Initialization with components like Page Dimensions, Document Layout, Logical Chunks, etc., 3) Document Parsing using Azure Document Intelligence, 4) Regex Filtering of Logical Chunks with specific keywords, 5) AI Filtering of Extracts using Azure OpenAI, 6) Key Terms Identification using AI, 7) Regex Filtering of Key Term Definitions, 8) AI Filtered Key Term Definitions, 9) AI MFees Interpretations, and 10) AI Key Term Definitions Interpretations. The flowchart should have clear directional arrows showing the flow of data between each stage, with each stage represented by a distinct box labeled accordingly.
Here is the flowchart diagram illustrating the document processing workflow using Azure and AI services. It includes all the stages from document acquisition to AI interpretations as described. You can review and use it for your reference!












ChatGPT

You said:
Ai reletad taks like working on documents ""i need to send the information to my officer please give me content 
ChatGPT said:
ChatGPT
To assist you in preparing content to send to your officer, I will need more details about the specific task or type of information you're working on. Could you clarify:

What kind of document are you preparing (e.g., report, memo, email)?
What specific information needs to be included (e.g., project updates, data analysis, etc.)?
Any particular format or tone you prefer (formal, brief, detailed)?
Feel free to provide more details, and I’ll help craft the content for you.






You said:
Al related tasks:

Step 1: loading pdf document

Step 2: using pdf miner or ADI we are generating the text(Prescribed language)

Step 3: Extracts management fees related extracts from prescribed language(step 2 output we are giving as input)

Step 4: Identify the keyterms from management fees related extracts(step 3 output we are giving as input)

Step 5: extracts keyterm defination from keyterms identification ((step 4 output we are giving as input)

I

Search

Inibss
ChatGPT said:
ChatGPT
It sounds like you're describing an AI workflow for processing documents, specifically for extracting and analyzing management fees-related terms from PDF documents. Here’s a refined summary based on your steps:

AI Task Workflow: Document Processing for Management Fees
Step 1: Load PDF Document
Use tools like PDFMiner or Apache PDFBox to read and load the PDF document.
Step 2: Convert PDF to Text
Extract the text from the PDF using PDFMiner or ADI (Advanced Data Interface), ensuring it preserves the prescribed language or format from the document.
Step 3: Extract Management Fees-related Sections
Process the extracted text to filter out sections that mention management fees.
This can be done using Natural Language Processing (NLP) techniques, such as keyword matching or regex patterns, to isolate paragraphs or sentences that mention fees, costs, or similar terms.
Step 4: Identify Key Terms Related to Management Fees
From the extracted sections, run an analysis to detect key terms related to management fees, such as “annual fee,” “administrative fee,” etc.
This can be achieved through tokenization, Named Entity Recognition (NER), or custom dictionaries of financial terms.
Step 5: Extract Definitions of Key Terms
Once the key terms are identified, extract definitions or related explanations from the surrounding text.
This can involve proximity-based extraction, where definitions are typically found near the terms, or referencing a predefined glossary or external data source for definitions.
Tools & Libraries:
PDFMiner (for PDF text extraction)
NLTK or spaCy (for NLP tasks like keyword extraction and term recognition)
Custom Dictionaries or Financial Lexicons (for term matching)
Let me know if you need further details or if you would like me to elaborate on any specific part of this process!






You said:
Can u give me objective of the program. And whatvis your role ofvthe program " they are asking my role is building the models and also evalutingthe the model i
ChatGPT said:
ChatGPT
Here’s a potential objective for the program and a description of your role based on the tasks you’ve outlined:

Objective of the Program:
The objective of this program is to automate the extraction, identification, and analysis of management fee-related information from PDF documents using AI and Natural Language Processing (NLP) techniques. The system aims to streamline the process of reading complex documents, isolating specific fee-related sections, identifying key financial terms, and extracting their definitions. This will help in making fee structures and related information more accessible and manageable for further analysis or reporting.

Your Role in the Program:
As a Model Developer and Evaluator, your role involves:

Building NLP Models:

Designing and implementing AI models that process PDF documents, extract relevant text, and identify key terms related to management fees.
Utilizing tools like PDFMiner for document processing and NLP libraries (e.g., NLTK, spaCy) for extracting and analyzing text.
Developing customized models for key term extraction and definition identification, ensuring they are aligned with the prescribed language and specific requirements of the task.
Evaluating Models:

Testing and validating the accuracy and efficiency of the models to ensure the extracted information is relevant, correct, and in line with expected outputs.
Evaluating the performance of the models using metrics like precision, recall, and F1-score to measure the quality of extraction and identification processes.
Continuously improving models based on evaluation results, feedback, and new data to enhance overall performance.
Optimizing the Workflow:

Ensuring the entire process, from loading documents to extracting definitions, is seamless and optimized for scalability.
Collaborating with other team members to integrate the AI models into the broader system and refine them based on user needs or new use cases.
This should give you a solid overview of both the program’s objective and your critical role within it. Let me know if you'd like further adjustments or clarifications!






Is this conversation helpful so far?












AI Task Workflow: Document Processing for Management Fees
Step 1: Load PDF Document
Use tools like PDFMiner or Apache PDFBox to read and load the PDF document.
Step 2: Convert PDF to Text
Extract the text from the PDF using PDFMiner or ADI (Advanced Data Interface), ensuring it preserves the prescribed language or format from the document.
Step 3: Extract Management Fees-related Sections
Process the extracted text to filter out sections that mention management fees.
This can be done using Natural Language Processing (NLP) techniques, such as keyword matching or regex patterns, to isolate paragraphs or sentences that mention fees, costs, or similar terms.
Step 4: Identify Key Terms Related to Management Fees
From the extracted sections, run an analysis to detect key terms related to management fees, such as “annual fee,” “administrative fee,” etc.
This can be achieved through tokenization, Named Entity Recognition (NER), or custom dictionaries of financial terms.
Step 5: Extract Definitions of Key Terms
Once the key terms are identified, extract definitions or related explanations from the surrounding text.
This can involve proximity-based extraction, where definitions are typically found near the terms, or referencing a predefined glossary or external data source for definitions.
Tools & Libraries:
PDFMiner (for PDF text extraction)
NLTK or spaCy (for NLP tasks like keyword extraction and term recognition)
Custom Dictionaries or Financial Lexicons (for term matching)
Let me know if you need further details or if you would like me to elaborate on any specific part of this process!











import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples or numpy arrays
        actual = list(actual) if isinstance(actual, (tuple, np.ndarray)) else actual
        predicted = list(predicted) if isinstance(predicted, (tuple, np.ndarray)) else predicted
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(make_hashable(sub_item) for sub_item in item)
        elif isinstance(item, dict):
            return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))
        else:
            return item

    # Create sets of hashable items from actual and predicted
    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Ensure 'scores' is a dictionary and contains the necessary keys
    expected_keys = {"accuracy", "precision", "recall", "F1 score"}
    if not isinstance(scores, dict):
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', 'recall', 'F1 score'.")
    missing_keys = expected_keys - scores.keys()
    if missing_keys:
        raise ValueError(f"The following keys are missing in the 'scores' dictionary: {missing_keys}")

    # Populate the DataFrame with the scores in the first row
    df.loc[0, "Accuracy"] = scores.get("accuracy", np.nan)
    df.loc[0, "Precision"] = scores.get("precision", np.nan)
    df.loc[0, "Recall"] = scores.get("recall", np.nan)
    df.loc[0, "F1 Score"] = scores.get("F1 score", np.nan)

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df










import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        actual = list(actual) if isinstance(actual, (tuple, np.ndarray)) else actual
        predicted = list(predicted) if isinstance(predicted, (tuple, np.ndarray)) else predicted
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(make_hashable(sub_item) for sub_item in item)
        elif isinstance(item, dict):
            return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))
        else:
            return item

    # Create sets of hashable items from actual and predicted
    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df.loc[0, "Accuracy"] = scores.get("accuracy", np.nan)
        df.loc[0, "Precision"] = scores.get("precision", np.nan)
        df.loc[0, "Recall"] = scores.get("recall", np.nan)
        df.loc[0, "F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(make_hashable(sub_item) for sub_item in item)
        elif isinstance(item, dict):
            return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))
        else:
            return item

    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Convert unhashable types (like lists or dicts) to tuples
    def make_hashable(item):
        if isinstance(item, list):
            return tuple(item)
        elif isinstance(item, dict):
            return tuple(sorted(item.items()))
        else:
            return item

    actual_set = set(make_hashable(item) for item in actual)
    predicted_set = set(make_hashable(item) for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # If actual or predicted contain dictionaries, convert them to tuples
    actual_set = set(tuple(sorted(item.items())) if isinstance(item, dict) else item for item in actual)
    predicted_set = set(tuple(sorted(item.items())) if isinstance(item, dict) else item for item in predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df






import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    def equalize_lengths(actual, predicted):
        # Convert to list if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    # Ensure actual and predicted are lists of equal length
    actual, predicted = equalize_lengths(actual, predicted)

    # Create sets for comparison
    actual_set = set(actual)
    predicted_set = set(predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    # Filter the dataframe to only include rows where 'actual' is not NaN
    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    # Debug statement to check the type of scores
    print(f"scores type: {type(scores)}, scores content: {scores}")

    # Ensure 'scores' is a dictionary and populate DataFrame columns
    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    # Rearrange the DataFrame columns
    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    # Clear non-header rows for some columns
    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    # Output filename
    output_filename = "evaluation_metrics.csv"

    # Save as Excel or CSV
    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):

    def equalize_lengths(actual, predicted):
        len_actual = len(actual)
        len_predicted = len(predicted)

        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    actual_set = set(actual)
    predicted_set = set(predicted)

    all_items = list(actual_set.union(predicted_set))

    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    # Debug statement to check the type of scores
    print(f"scores type: {type(scores)}, scores content: {scores}")

    if isinstance(scores, dict):
        df["Accuracy"] = scores.get("accuracy", np.nan)
        df["Precision"] = scores.get("precision", np.nan)
        df["Recall"] = scores.get("recall", np.nan)
        df["F1 Score"] = scores.get("F1 score", np.nan)
    else:
        raise ValueError("Expected 'scores' to be a dictionary with keys like 'accuracy', 'precision', etc.")

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df





import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    
    def equalize_lengths(actual, predicted):
        # Convert to lists if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    # Convert lists of dictionaries to lists of strings (or tuples) to make them hashable
    actual_set = set([str(item) for item in actual])
    predicted_set = set([str(item) for item in predicted])

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    df["Accuracy"] = scores["accuracy"]
    df["Precision"] = scores["precision"]
    df["Recall"] = scores["recall"]
    df["F1 Score"] = scores["F1 score"]

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import pandas as pd
import numpy as np

def export_evaluation_metrics_as_csv(actual, predicted, scores, file_type):
    
    def equalize_lengths(actual, predicted):
        # Convert to lists if they are tuples
        if isinstance(actual, tuple):
            actual = list(actual)
        if isinstance(predicted, tuple):
            predicted = list(predicted)
        
        # Determine the lengths of the lists
        len_actual = len(actual)
        len_predicted = len(predicted)

        # Append None to the shorter list until both lists are the same length
        if len_actual > len_predicted:
            predicted.extend([None] * (len_actual - len_predicted))
        elif len_predicted > len_actual:
            actual.extend([None] * (len_predicted - len_actual))

        return actual, predicted

    actual, predicted = equalize_lengths(actual, predicted)

    actual_set = set(actual)
    predicted_set = set(predicted)

    # Combine sets to get all unique items
    all_items = list(actual_set.union(predicted_set))

    # Create a DataFrame
    df = pd.DataFrame(index=all_items, columns=["actual", "predicted"])

    # Populate the DataFrame
    df["actual"] = [item if item in actual_set else np.nan for item in all_items]
    df["predicted"] = [item if item in predicted_set else np.nan for item in all_items]

    # Reset index to get the items as columns instead of index
    df.reset_index(drop=True, inplace=True)

    df_eval = df.dropna(subset=["actual"])
    predicted = df_eval["predicted"].tolist()
    actual = df_eval["actual"].tolist()

    all_terms = list(set(predicted + actual))

    df["Accuracy"] = scores["accuracy"]
    df["Precision"] = scores["precision"]
    df["Recall"] = scores["recall"]
    df["F1 Score"] = scores["F1 score"]

    df = df[["actual", "predicted", "Accuracy", "Precision", "Recall", "F1 Score"]]

    df.loc[1:, "input file name"] = np.nan
    df.loc[1:, "Accuracy"] = np.nan
    df.loc[1:, "Precision"] = np.nan
    df.loc[1:, "Recall"] = np.nan
    df.loc[1:, "F1 Score"] = np.nan

    output_filename = "evaluation_metrics.csv"  # Modify this as needed

    if file_type == "excel":
        output_filename = output_filename.replace(".csv", ".xlsx")
        df.to_excel(output_filename, index=False)
    else:
        df.to_csv(output_filename, index=False)

    print(f"{output_filename} Metrics file saved")

    return df








import logging
import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient

from genai.processing_daemon.methods.adi_processing import adiProcessing
from common_svc.logger.log_util import configure_loggers

# Configure loggers
configure_loggers()
logger = logging.getLogger(__name__)

class DocumentParsing:
    def __init__(self):
        pass

    def pdfMiner(self):
        raise Exception("The method pdfMiner is not available")

    def connect_to_azure_blob(self, input_document_path):
        """
        Connect to Azure Blob Storage using Managed Identity.
        """
        try:
            connection = input_document_path['params']['connection_string']
            mi_client_id = connection["mi_client_id"]

            credential = ManagedIdentityCredential(client_id=mi_client_id)

            # Create a BlobServiceClient using the account URL and the Managed Identity Credential
            blob_service_client = BlobServiceClient(
                account_url=connection["account_url"],
                credential=credential
            )

            # Create a ContainerClient for the specified container
            container_client = blob_service_client.get_container_client(connection["container_name"])

            logger.info("Connected to Azure Blob Container")
            return container_client

        except Exception as e:
            logger.error(f"An error occurred: {e}")
            return None

    def download_pdf_from_blob(self, input_document_path, tmpdir):
        """
        Download a PDF file from Azure Blob Storage and save it to a specified directory.
        Return the file path including the file name.
        """
        container_client = self.connect_to_azure_blob(input_document_path)
        if not container_client:
            return None

        params = input_document_path["params"]
        absolute_path = params["absolute_path"]
        blob_name = os.path.basename(absolute_path)

        # Define the full path for the downloaded file
        pdf_file_path = os.path.join(tmpdir, blob_name)

        # Get the BlobClient for the specified blob
        blob_client = container_client.get_blob_client(blob_name)

        # Download the blob content and write it to the file
        try:
            with open(pdf_file_path, 'wb') as pdf_file:
                download_stream = blob_client.download_blob()
                pdf_file.write(download_stream.readall())
        except Exception as e:
            logger.error(f"Failed to download blob {blob_name} from Azure Blob Storage: {e}")
            return None

        return pdf_file_path

    def ADI(self, processing_config, input_document_path):
        """
        Run ADI processing on the downloaded PDF.
        """
        model_name = processing_config["model"]
        endpoint = processing_config["endpoint"]
        key = processing_config["key"]

        adi_object = adiProcessing()

        output_dir = os.path.join("tmp", "output")
        os.makedirs(output_dir, exist_ok=True)

        input_document_path = self.download_pdf_from_blob(input_document_path, output_dir)
        if not input_document_path:
            return None

        try:
            merged_content = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key
            )
            return merged_content
        except Exception as e:
            logger.exception("Exception Occurred while extracting ADI results: %s", e)
            return None

# Testing
processingConfig = {
    "processing_engine": "ADI",
    "model": "prebuilt-layout",
    "endpoint": "https://npeefrdns.cognitiveservices.azure.com/",
    "key": "dc7c0c7c3a9a4371b7dc983ac7b618b7"
}

input_document_path = {
    "storage_type": "blob",
    "container_name": "sample",
    "params": {
        "connection_string": {
            "account_url": "https://eventhubstorage919.blob.core.windows.net",
            "container_name": "sample",
            "mi_client_id": "5da5a4c-edb2-4f9f-8ad8-d11f0214f557"
        },
        "absolute_path": "PVT Markets/data/LPA/1719852c-91ac-498a-b4ad-508cdab7cbad/1885_private_opportunities_fund_1p_-_2nd_er_lpa_march_16_20211.pdf"
    }
}

docparser = DocumentParsing()
merged_content = docparser.ADI(processingConfig, input_document_path)
print(merged_content)








import logging
import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient

from genai.processing_daemon.methods.adi_processing import adiProcessing
from common_svc.logger.log_util import configure_loggers

# Configure loggers
configure_loggers()
logger = logging.getLogger(__name__)

class DocumentParsing:
    def __init__(self):
        pass

    def pdfMiner(self):
        raise Exception("The method pdfMiner is not available.")

    def connect_to_azure_blob(self, input_document_path):
        # Connect to Azure Blob Storage using Managed Identity.
        connection_string = input_document_path["params"]["connection_string"]

        try:
            credential = ManagedIdentityCredential(client_id=connection_string["mi_client_id"])

            # Create a BlobServiceClient using the account URL and the Managed Identity Credential
            blob_service_client = BlobServiceClient(
                account_url=connection_string["account_url"],
                credential=credential
            )

            # Create a ContainerClient for the specified container
            container_client = blob_service_client.get_container_client(connection_string["container_name"])

            logger.info("Connected to Azure Blob Container")
            return container_client

        except Exception as e:
            logger.error(f"An error occurred: {e}")
            return None

    def download_pdf_from_blob(self, input_document_path, tmpdir):
        """
        Download a PDF file from Azure Blob Storage and save it to a specified directory.
        Return the file path including the file name.
        """
        container_client = self.connect_to_azure_blob(input_document_path)
        if not container_client:
            return None

        params = input_document_path["params"]
        storage_location = params["storage_location"]
        blob_name = os.path.basename(storage_location)

        # Define the full path for the downloaded file
        pdf_file_path = os.path.join(tmpdir, blob_name)

        # Get the BlobClient for the specified blob
        blob_client = container_client.get_blob_client(blob_name)

        # Download the blob content and write it to the file
        with open(pdf_file_path, 'wb') as pdf_file:
            download_stream = blob_client.download_blob()
            pdf_file.write(download_stream.readall())

        return pdf_file_path

    def ADI(self, processing_config, input_document_path):
        model_name = processing_config["model"]
        endpoint = processing_config["endpoint"]
        key = processing_config["key"]

        adi_object = adiProcessing()

        output_dir = os.path.join("tmp", "output")
        os.makedirs(output_dir, exist_ok=True)

        # Download PDF from Azure Blob
        input_document_path = self.download_pdf_from_blob(input_document_path, output_dir)
        if not input_document_path:
            return None

        try:
            # Example method calls assuming run_prescribed_extracts_azure exists
            page_dim_json_file_path, _doc_layout_json_file_path, doc_layout = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key)

            merged_content = adi_object.run_prescribed_extracts_azure(
                input_document_path, output_dir, model_name, endpoint, key)

            return merged_content

        except Exception as e:
            logger.exception("Exception Occurred while extracting ADI results: %s", e)
            return None

# Testing
processing_config = {
    "processing_engine": "ADI",
    "model": "Layout",
    "endpoint": "https://npeefrdns.cognitiveservices.azure.com/",
    "key": "dc7c"
}

input_document_path = {
    "storage_type": "blob",
    "container_name": "sample",
    "params": {
        "connection_string": {
            "account_url": "https://<your-account-name>.blob.core.windows.net",
            "container_name": "sample",
            "mi_client_id": "<your-managed-identity-client-id>"
        },
        "storage_location": "PVT Markets/data/LPA/1719852c-91ac-498a-b4ad-508cdab7cbad/1885_private_opportunities_fund_ip_-_2nd_ar_lpa_march_16_2021.pdf"
    }
}

docparser = DocumentParsing()
merged_content = docparser.ADI(processing_config, input_document_path)

print(merged_content)







import os
import json
import re

def get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :param count: The current count of filtered paragraphs.
    :return: Updated list of filtered paragraphs and the count.
    """
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'
    if re.search(pattern, merged_para):
        if merged_para not in quotation_marks_paragraphs:
            filtered_para = str(count) + " " + merged_para
            quotation_marks_paragraphs.append(filtered_para)
            count += 1
    return quotation_marks_paragraphs, count

def extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms):
    """
    Simulate the extraction of definitions based on indexed key terms.
    """
    # This is a placeholder for the actual definition extraction logic.
    return json.dumps({"definitions": quotation_marks_paragraphs})

def load_json(file_path):
    """
    Load JSON data from a file.
    :param file_path: Path to the JSON file.
    :return: Parsed JSON data.
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

def run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir):
    """
    Process a PDF and associated logical chunks file to extract key terms and their definitions.
    :param pdf_path: Path to the input PDF.
    :param logical_chunks_file: Path to the JSON file containing logical chunks.
    :param extracted_keyterms_json_path: Path to the JSON file containing key terms.
    :param output_dir: Directory where the output JSON will be saved.
    :return: JSON output and the path to the saved JSON file.
    """
    # Check if the PDF name matches the logical chunks file name
    pdf_base_name = os.path.splitext(os.path.basename(pdf_path))[0]
    logical_chunks_base_name = os.path.splitext(os.path.basename(logical_chunks_file))[0].replace("logicalchunks", "").strip()

    if pdf_base_name != logical_chunks_base_name:
        raise ValueError(f"Mismatch: PDF base name '{pdf_base_name}' does not match logical chunks base name '{logical_chunks_base_name}'.")

    extracted_definition_json_file_name = pdf_base_name + "_generated_definitions.json"
    extracted_definition_json_path = os.path.join(output_dir, extracted_definition_json_file_name)

    # Load key terms
    with open(extracted_keyterms_json_path, "r") as json_file:
        data = json.load(json_file)

    keyterm_index = 0
    list_indexed_keyterms = []

    search_strings = data["keyTerms"]
    for search_string in search_strings:
        indexed_keyterm = f"{keyterm_index} {search_string}"
        keyterm_index += 1
        list_indexed_keyterms.append(indexed_keyterm)

    # Load logical chunks
    file_data = load_json(logical_chunks_file)

    count = 0
    quotation_marks_paragraphs = []

    for items in file_data["logicalChunks"]:
        for search_string in search_strings:
            # For chunks that need merging of content
            if len(items) >= 2:
                merged_para = ""
                for i in range(len(items)):
                    para = items[i]["content"]
                    merged_para += " " + para
                quotation_marks_paragraphs, count = get_filtered_paras(search_string, merged_para.strip(), quotation_marks_paragraphs, count)
            # For chunks having only one content
            elif len(items) == 1:
                for contents in items:
                    content = contents["content"]
                    quotation_marks_paragraphs, count = get_filtered_paras(search_string, content, quotation_marks_paragraphs, count)

    # Extract definitions based on the filtered paragraphs
    output = extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms)
    json_output = json.loads(output)

    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Save the extracted definitions to a JSON file
    with open(extracted_definition_json_path, "w", encoding="utf-8") as ff:
        json.dump(json_output, ff, ensure_ascii=False, indent=4)

    print(f"Successfully dumped the extracted definitions JSON for the PDF: {pdf_path}")

    return json_output, extracted_definition_json_path

# Testing the function
pdf_path = r"H:\management_fee_extraction\pipeline_test\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA.PDF"
logical_chunks_file = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_logical_chunks.json"
extracted_keyterms_json_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_generated_keyterms.json"
output_dir = r"H://management_fee_extraction//extracts_output//output7//"

json_output, extracted_definition_json_path = run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir)

print(json_output)
print(extracted_definition_json_path)







import os
import json
import re

def load_json(file_path):
    """Utility function to load JSON data from a file."""
    with open(file_path, "r", encoding="utf-8") as file:
        return json.load(file)

def get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :param count: The current count of filtered paragraphs.
    :return: Updated list of filtered paragraphs and the count.
    """
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'
    if re.search(pattern, merged_para):
        indexing_para = merged_para
        if indexing_para not in quotation_marks_paragraphs:
            filtered_para = str(count) + " " + merged_para
            quotation_marks_paragraphs.append(filtered_para)
            count += 1
    return quotation_marks_paragraphs, count

def extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms):
    """
    This is a placeholder for the actual implementation of extracting definitions.
    It would process the filtered paragraphs and indexed keyterms to produce output.
    """
    # Assuming this function returns a JSON-serializable object as output.
    return {"definitions": [{"keyterm": keyterm, "paragraph": para} for keyterm, para in zip(list_indexed_keyterms, quotation_marks_paragraphs)]}

def run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir):
    extracted_definition_json_file_name = os.path.basename(pdf_path)
    extracted_definition_json_file_name = extracted_definition_json_file_name.replace(".pdf", "_generated_definitions.json").replace(".PDF", "_generated_definitions.json")
    extracted_definition_json_path = os.path.join(output_dir, extracted_definition_json_file_name)

    # Load key terms from the JSON file
    data = load_json(extracted_keyterms_json_path)
    search_strings = data["keyTerms"]

    # Index key terms
    list_indexed_keyterms = [str(i) + " " + search_string for i, search_string in enumerate(search_strings)]

    # Load logical chunks
    file_data = load_json(logical_chunks_file)
    
    count = 0
    quotation_marks_paragraphs = []

    for items in file_data["logicalChunks"]:
        for search_string in search_strings:
            if len(items) >= 2:
                merged_para = ""
                for item in items:
                    merged_para += item["content"] + " "
                quotation_marks_paragraphs, count = get_filtered_paras(search_string, merged_para, quotation_marks_paragraphs, count)
            elif len(items) == 1:
                for item in items:
                    content = item["content"]
                    quotation_marks_paragraphs, count = get_filtered_paras(search_string, content, quotation_marks_paragraphs, count)

    # Extract definitions
    output = extract_definitions(quotation_marks_paragraphs, list_indexed_keyterms)

    # Save the extracted definitions to a JSON file
    with open(extracted_definition_json_path, "w", encoding="utf-8") as ff:
        json.dump(output, ff, ensure_ascii=False, indent=4)

    return output, extracted_definition_json_path

# Testing
pdf_path = r"H:\management_fee_extraction\pipeline_test\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA.PDF"
logical_chunks_file = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_logical_chunks.json"
extracted_keyterms_json_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_generated_keyterms.json"
output_dir = r"H:\management_fee_extraction\extracts_output\output7"

json_output, extracted_definition_json_path = run_keyterms_definitions_extract(pdf_path, logical_chunks_file, extracted_keyterms_json_path, output_dir)

print(json_output)
print(extracted_definition_json_path)
import re

def get_filtered_paras(search_string, merged_para, count, quotation_marks_paragraphs):
    """
    Filters paragraphs based on the search string and updates the count.
    :param search_string: The string to search for in the paragraphs.
    :param merged_para: The paragraph to search within.
    :param count: The current count of filtered paragraphs.
    :param quotation_marks_paragraphs: The list of paragraphs that have already been filtered.
    :return: Updated list of filtered paragraphs and the count.
    """
    # Correct the pattern for regex search
    pattern = rf'[^"]*{re.escape(search_string)}[^"]*'

    # Check if the pattern is found in the merged paragraph
    if re.search(pattern, merged_para):
        indexing_para = merged_para

        # Ensure the paragraph is not already in the list
        if indexing_para not in quotation_marks_paragraphs:
            # Correct string concatenation and append the paragraph
            filtered_para = str(count) + " " + merged_para
            print(f"filtered_para: {filtered_para}")

            quotation_marks_paragraphs.append(filtered_para)
            count += 1

    return quotation_marks_paragraphs, count

# Example usage:
search_string = "Management Fee"
merged_para = "This paragraph talks about Management Fee and other details."
count = 1
quotation_marks_paragraphs = []

result, updated_count = get_filtered_paras(search_string, merged_para, count, quotation_marks_paragraphs)

print("Filtered Paragraphs:", result)
print("Updated Count:", updated_count)





import re
import os
import json

class KeywordFilterMethod:

    def extract(self, data) -> list[dict]:
        """
        Extract the data based on keywords
        :param data: Input data as a dictionary
        :return: List of dictionaries containing extracted data
        """
        terms = [
            "Management Fee",
            "Management +Fee",
            "Servicing Fee",
            "Servicing +Fee",
            "Investment Advisory Fee",
            "Investment +Advisory +Fee",
            "Compensation",
            "Remuneration",
            "Expenses",
            "AIFM Fee",
            "AIFM +Fee",
        ]
        patterns = self.compile_search_patterns(terms)
        data_with_terms = self.extract_data_with_terms(data, patterns)
        return data_with_terms

    def compile_search_patterns(self, terms):
        """
        Compile the search pattern using the terms provided
        :param terms: List of search terms
        :return: List of compiled regex patterns
        """
        patterns = [re.compile(re.escape(term)) for term in terms]
        return patterns

    def search_terms_in_content(self, node_content, patterns):
        """
        Search the terms in content provided
        :param node_content: Content to search within
        :param patterns: List of compiled regex patterns
        :return: True if any pattern matches, otherwise False
        """
        for pattern in patterns:
            if pattern.search(node_content):
                return True
        return False

    def extract_data_with_terms(self, data, patterns):
        """
        Extract the data based on keywords
        :param data: Input data as a dictionary
        :param patterns: List of compiled regex patterns
        :return: List of dictionaries containing extracted data
        """
        extracted_data = []
        for item in data['logicalchunks']:
            for node in item:
                if self.search_terms_in_content(node['content'], patterns):
                    extracted_data.append(item)
                    break
        return extracted_data

    def merge_content(self, json_data):
        """
        Merge content from the provided JSON data
        :param json_data: Input JSON data as a dictionary
        :return: Dictionary with merged content
        """
        data = json.loads(json.dumps(json_data))
        combined_strings = []

        for item in data['extractsUsingPrescribedLanguage']:
            combined_string = ' '.join(item['content'] for item in item)
            combined_strings.append(combined_string)

        result = {'extractsUsingPrescribedLanguage': combined_strings}
        return result

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON data to a file
    :param json_file_path: Path to the JSON file
    :param json_data: Data to write to the file
    """
    try:
        with open(json_file_path, "w") as json_file:
            json.dump(json_data, json_file)
    except Exception as e:
        print(f"Error dumping JSON file: {e}")

# File paths
data_file_path = "H:/management_fee_extraction/extracts_output/output5/LEGAL-#148874-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_docprocessing.json"
output_dest_dir = "H:/management_fee_extraction/extracts_output/output5"

with open(data_file_path, "r") as json_file:
    data = json.load(json_file)

obj = KeywordFilterMethod()
paragraphs = obj.extract(data)

print(paragraphs)

paragraphs = {"extractsUsingPrescribedLanguage": paragraphs}

file_name = os.path.splitext(os.path.basename(data_file_path))[0]

filtered_images_json_file_path = os.path.join(output_dest_dir, f"{file_name}_filtered_images.json")
dump_json_file(filtered_images_json_file_path, paragraphs)

merged_content = obj.merge_content(paragraphs)
merged_content_json_file_path = os.path.join(output_dest_dir, f"{file_name}_prescribed_extracts.json")

print(merged_content_json_file_path)
print(merged_content)

dump_json_file(merged_content_json_file_path, merged_content)






import re
import os
import json

class KeywordFilterMethod:

    def extract(self, data) -> list[dict]:
        """
        Extract the data based on keywords
        :param data: JSON data containing logicalChunks
        :return: List of dictionaries containing extracted data
        """
        terms = [
            "Management Fee",
            "Management +Fee",
            "Servicing Fee",
            "Servicing +Fee",
            "Investment Advisory Fee",
            "Investment +Advisory +Fee",
            "Compensation",
            "Remuneration",
            "Expenses",
            "AIFM Fee",
            "AIFM +Fee",
        ]
        patterns = self.compile_search_patterns(terms)
        return self.extract_data_with_terms(data, patterns)

    def compile_search_patterns(self, terms):
        """
        Compile the search pattern using the terms provided
        :param terms: List of keywords
        :return: List of compiled regex patterns
        """
        return [re.compile(re.escape(term)) for term in terms]

    def search_terms_in_content(self, node_content, patterns):
        """
        Search the terms in content provided
        :param node_content: Content of the node
        :param patterns: List of regex patterns
        :return: True if any pattern matches, otherwise False
        """
        for pattern in patterns:
            if pattern.search(node_content):
                return True
        return False

    def extract_data_with_terms(self, data, patterns):
        """
        Extract the data based on keywords
        :param data: JSON data containing logicalChunks
        :param patterns: List of regex patterns
        :return: List of dictionaries containing extracted data
        """
        extracted_data = []
        logical_chunks = data.get('logicalChunks', [])  # Ensure it's a list
        for item in logical_chunks:
            if isinstance(item, list):  # Check if item is a list
                for node in item:
                    if isinstance(node, dict):  # Check if node is a dictionary
                        content = node.get('content', '')  # Safely get content
                        if self.search_terms_in_content(content, patterns):
                            extracted_data.append(item)
                            break
        return extracted_data

    def merge_content(self, json_data):
        """
        Merge the content of the extracted data
        :param json_data: JSON data containing extractsUsingPrescribedLanguage
        :return: Merged content dictionary
        """
        extracts = json_data.get('extractsUsingPrescribedLanguage', [])  # Ensure it's a list
        combined_strings = []
        for item in extracts:
            if isinstance(item, list):  # Check if item is a list
                combined_string = ' '.join(node.get('content', '') for node in item if isinstance(node, dict))
                combined_strings.append(combined_string)
        return {'extractsUsingPrescribedLanguage': combined_strings}

def dump_json_file(json_file_path, json_data):
    """
    Dump the JSON data into a file
    :param json_file_path: Path to save the JSON file
    :param json_data: Data to save
    :return: None
    """
    try:
        with open(json_file_path, "w") as json_file:
            json.dump(json_data, json_file, indent=4)
    except Exception as e:
        print(f"Error writing JSON file {json_file_path}: {e}")

if __name__ == "__main__":
    data_file_path = r"H:\management_fee_extraction\extracts_output\output5\LEGAL-#148074-v3-1824_Private_Equity_Fund_LP_(TJU)_-_AR_LPA_docprocessing.json"
    output_dest_dir = r"H:\management_fee_extraction\extracts_output\output5"

    # Load the JSON data from file
    with open(data_file_path, "r") as json_file:
        data = json.load(json_file)

    # Create an instance of KeywordFilterMethod and process the data
    obj = KeywordFilterMethod()
    paragraphs = obj.extract(data)

    # Prepare file paths for saving the results
    file_name = os.path.splitext(os.path.basename(data_file_path))[0]

    filtered_images_json_file_path = os.path.join(output_dest_dir, f"{file_name}_filtered_images.json")
    merged_content_json_file_path = os.path.join(output_dest_dir, f"{file_name}_prescribed_extracts.json")

    # Dump filtered paragraphs to a JSON file
    dump_json_file(filtered_images_json_file_path, {'extractsUsingPrescribedLanguage': paragraphs})

    # Merge content and dump to JSON file
    merged_content = obj.merge_content({'extractsUsingPrescribedLanguage': paragraphs})
    dump_json_file(merged_content_json_file_path, merged_content)

    # Print results for verification
    print(f"Filtered images JSON saved to: {filtered_images_json_file_path}")
    print(f"Merged content JSON saved to: {merged_content_json_file_path}")
    print("Merged Content:", json.dumps(merged_content, indent=4))

